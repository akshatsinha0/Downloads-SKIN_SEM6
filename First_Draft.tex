% !TeX program = lualatex
\documentclass[11pt,a4paper]{article}

\usepackage{fontspec}
\setmainfont{Times New Roman}
% Ensure input encoding compatibility across engines
\usepackage{polyglossia}
\setdefaultlanguage{english}
\usepackage[margin=0.74in]{geometry}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{pagecolor}
\usepackage{amsmath}
\usepackage{microtype}
\usepackage[hidelinks]{hyperref}
\usepackage{balance}
\usepackage{dblfloatfix}
\AtEndDocument{\balance}
\usepackage{ragged2e}
\usepackage{newunicodechar}
\usepackage{array}
\usepackage{tabularx}
\usepackage{enumitem}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{subcaption}
\usepackage{authblk}
\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup}
\captionsetup[figure]{skip=6pt}
% Avoid conflicts with older packages
\let\Bbbk\relax
\setlength{\textfloatsep}{10pt plus 2pt minus 2pt}
\setlength{\floatsep}{8pt plus 2pt minus 2pt}
\setlength{\intextsep}{8pt plus 2pt minus 2pt}
\setlist[itemize]{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}
\setlist[enumerate]{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}
\setlength{\columnsep}{0.24in}
\newunicodechar{–}{\textendash}   % en dash
\newunicodechar{—}{\textemdash}   % em dash
\newunicodechar{‑}{-}             % non-breaking hyphen (U+2011) -> normal hyphen
\newunicodechar{−}{-}             % math minus (U+2212) -> normal hyphen
% Avoid redefining NBSP as ~; it can break macros and spacing
% \newunicodechar{ }{~}             % NBSP (U+00A0) -> non-breaking space
% \newunicodechar{\u202F}{\,}            % NNBSP -> thin space (disabled)
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{Y}{>{\raggedright\arraybackslash}X}


\definecolor{pagebg}{HTML}{FFFFFA}
\pagecolor{pagebg}

\setstretch{1.2}
\frenchspacing

\begin{document}
% Research-style title and author block
\title{A Multimodal Generative AI System for Skin Lesion Diagnosis and Explanation}
\author[1]{Akshat Sinha (22BCE2218)}
\author[1]{Arnav Sinha (22BCE0830)}
\author[1]{Aman Chauhan (22BCE0476)}
\affil[1]{School of Computer Science and Engineering (SCOPE), Vellore Institute of Technology, Vellore, India}
\affil[ ]{\texttt{akshat.sinha2022@vitstudent.ac.in}\quad|\quad\texttt{arnav.sinha2022@vitstudent.ac.in}\quad|\quad\texttt{aman.chauhan2022@vitstudent.ac.in}}
\date{}

% Spanning title, contact, abstract, and keywords on top of page 1
\makeatletter
\twocolumn[
\begin{@twocolumnfalse}
\maketitle
{\small\noindent Akshat Sinha --- +91\,91428\,12513\quad|\quad Arnav Sinha --- +91\,93540\,39524\quad|\quad Aman Chauhan --- +91\,98374\,08849\par}
\blfootnote{Under the supervision of Dr. Naga Priyadarsini R, Assistant Professor Sr. Grade 1, Department of Analytics, School of Computer Science and Engineering (SCOPE), Vellore Institute of Technology, India.}
\begingroup
\renewcommand\abstractname{Abstract}
\begin{abstract}
Accurate differentiation of benign and malignant skin lesions remains challenging in routine care, where subjective judgment and time constraints can limit the consistent use of dermoscopy alongside patient context (age, sex, lesion site). We present a two-stage multimodal assistant that fuses dermoscopic imagery with structured clinical metadata to deliver calibrated class probabilities and generate concise, clinician-style explanations aimed at transparency and efficient documentation.

In Stage 1 (diagnostic engine), a modern vision backbone encodes dermoscopic images while clinical metadata—rendered as short natural-language statements—is embedded by a medical text encoder. The visual and textual representations are fused and passed to a calibrated softmax classifier to produce class probabilities for common lesion categories. Training uses large public cohorts with cross-dataset evaluation to assess robustness and generalization.

In Stage 2 (generative engine), the predicted class, confidence, and salient cues are transformed into a controlled prompt for a large language model to produce a focused report. The output states the most likely diagnosis, highlights visual and contextual evidence, lists plausible differentials, and, where appropriate, suggests next steps consistent with dermatology note conventions.

We benchmark CNNs and Vision Transformers for imaging, compare general versus clinically pretrained text encoders, and run ablations on fusion and calibration, including single-modality controls. By combining strong discriminative performance with faithful, human-readable rationales, the system is designed to operate as a reliable second opinion and to support earlier melanoma detection within busy workflows.
\end{abstract}
\vspace{4pt}
\noindent\textbf{Keywords—} Multimodal learning; dermoscopy; skin lesion classification; explainable AI; clinical decision support; calibration; uncertainty; large language models.
\par\endgroup
\vspace{8pt}
\end{@twocolumnfalse}
]
\makeatother




% Body line spacing for main text
\setstretch{1.15}
\justifying

\section{INTRODUCTION}

Clinical assessment of pigmented lesions combines dermoscopic patterns with succinct patient context (age, sex, anatomic site). Single-modality systems often miss these complementary signals, and opaque predictions undermine clinician trust and adoption. We introduce a two-stage assistant for dermoscopy: (i) a diagnostic engine that fuses image and metadata embeddings to produce calibrated class probabilities, and (ii) a generative reporter that converts the prediction and context into a concise clinician-style note with justification, differentials, and suggested next steps. In both design and evaluation we emphasize accuracy, calibration, and auditability so the tool functions as a reliable second opinion without displacing clinical judgment. Contributions include a modular training/evaluation protocol, an uncertainty-aware fusion variant, and a controlled prompting template that preserves faithfulness to the discriminative output.

\subsection{Background}

Dermoscopy reveals morphological patterns (e.g., pigment networks, streaks, globules, vascular structures) that benefit from deep visual features, while age, sex, and lesion site shift pre-test probabilities and help disambiguate similar appearances. Multimodal learning unifies these signals via an image encoder for morphology and a text encoder for context, combined through a transparent fusion for classification. Large public cohorts enable robust training, cross-dataset validation, and error analysis by subgroups. Grounded generative models, when constrained by structured outputs and seeded with salient cues, can produce short, reviewable notes that surface the underlying rationale and improve documentation efficiency.

\subsection{Motivations}

Earlier, more consistent triage requires calibrated probabilities paired with clear wording reusable in notes and referrals. A modular two-stage design allows encoders to evolve independently of the reporter, simplifies audit and deployment, and supports incremental updates without altering clinical workflows. By logging decisions and exposing uncertainty, the assistant also facilitates quality improvement and safer threshold tuning.

\subsection{Scope of the Project}

In scope: dermoscopic images and metadata (age, sex, anatomic site); comparison of CNN versus ViT image encoders and general versus clinical text encoders; simple fusion with softmax; discrimination and calibration on internal splits and cross-dataset checks; generation of short clinician-style reports grounded in structured outputs.

Out of scope: histopathology, non-dermoscopic photographs, longitudinal follow-up, and treatment recommendations. Deliverables include trained models, ablations, multimodal versus single-modality evidence, and a compact prompt template for consistent summaries and reproducible inference.


% === Section 2: PROJECT DESCRIPTION AND GOALS ===
\section{PROJECT DESCRIPTION AND GOALS}

At a high level, the system couples a multimodal diagnostic engine with a grounded generative reporter to produce calibrated probabilities and concise explanations for common lesion categories. Our goals are to: (i) quantify the added value of metadata over image-only baselines, (ii) assess robustness across datasets and subgroups, (iii) improve faithfulness and clarity of generated notes, and (iv) package the method for lightweight inference.

\subsection{Literature Review}

Research in dermoscopic analysis has progressed from hand-crafted ABCD/GLASS features to CNNs and ViTs that capture long-range patterns. Incorporating lightweight metadata consistently improves discrimination near decision boundaries. Fusion spans simple concatenation to cross-modal attention and transformers; transparent baselines remain attractive for auditability and deployment in constrained settings. Post-hoc explanations are often unfaithful; constrained prompting grounded in structured outputs can yield concise, consistent prose. Open cohorts (e.g., ISIC) enable standardized benchmarking, ablations on encoders/fusion/calibration, and out-of-distribution evaluation.

\subsection{Gaps Identified}

\begin{itemize}
  \item Faithful explainability: predictions often lack human-readable rationales that clinicians can review when visual and textual signals are fused.
  \item Generalization: limited transfer of models to unseen clinical settings and devices.
  \item Reporting integration: need for clinically relevant narrative outputs beyond a bare diagnosis.
  \item Fusion benchmarking: few systematic comparisons of multimodal fusion methods under audit and resource constraints.
  \item Ethics and trust: bias monitoring and mechanisms to build clinician trust in AI-assisted diagnosis.
  \item Efficiency: maintaining real-time performance for smooth clinical workflows.
  \item Underuse of context: image-only systems neglect readily available priors near decision boundaries.
\end{itemize}

\subsection{Problem Statement}

Given a dermoscopic image and metadata (age, sex, anatomic site), predict a calibrated distribution over lesion classes and generate a faithful, human-readable summary of the decision that cites salient visual and contextual cues. The system should be modular so that backbones, fusion, calibration, and prompting can evolve independently with reproducible training, validation, and reporting.

\subsection{Project Plan}

The work plan is organized into five phases:

\paragraph{Phase 1: Data.} Assemble splits from ISIC 2019/2020; standardize images; normalize metadata; handle imbalance and quality gates.
\paragraph{Phase 2: Baselines/Model.} Train image-only (CNN, ViT), text-only (BERT), and multimodal concatenation; track discrimination and calibration with fixed seeds.
\paragraph{Phase 3: Ablations/Generalization.} Compare encoders, modalities, and calibration; test on ISIC 2018; produce reliability plots and confusion matrices.
\paragraph{Phase 4: Reporting.} Design a compact, deterministic prompt with guardrails; validate clarity, faithfulness, and completeness via a small rubric.
\paragraph{Phase 5: Packaging.} Provide a lightweight inference utility (image+metadata$\rightarrow$probs+report), experiment tables, and labeled limitations.

\subsection{Activity Chart and Work Breakdown Structure}
This activity view follows the five phases already defined in the plan and maps them to concrete tasks, milestones, and deliverables.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{WBS.png}
  \caption{Work Breakdown Structure (WBS).}
  \label{fig:wbs}
\end{figure*}
% ensure float placement doesn't block later figures in two-column mode
% \addtolength{\textfloatsep}{1pt}

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{gantt.png}
  \caption{Activity chart (Gantt) aligned to Phases 1--5.}
  \label{fig:activity-gantt}
\end{figure*}


% === Section 3: REQUIREMENT ANALYSIS  ===
\section{METHODOLOGY}

This section details datasets and governance, preprocessing, model components, training and evaluation protocol, and a proposed algorithmic improvement that augments fusion, calibration, and explanation quality. We favor simple, auditable, and reproducible choices; when added complexity is introduced (e.g., gated fusion), we provide ablations and clear fallback baselines. Subsections are arranged for replication and traceability to experiments and deliverables.

\subsection{Datasets and Governance}

\begin{itemize}
  \item Create patient-level stratified Train/Val/Test splits on ISIC 2019/2020; reserve ISIC 2018 as an external out-of-distribution set.
  \item Use de-identified metadata (age, sex, anatomic site); represent missing fields explicitly (e.g., ``site: unknown'').
  \item Document licenses, inclusion/exclusion criteria, transforms, and class definitions; version all datasets and configs.
\end{itemize}

\subsection{Task and Outputs}

\begin{itemize}
  \item Input: one dermoscopic image + {age, sex, anatomic site}.
  \item Diagnostic output: class probabilities with per-class confidence and a calibrated overall score.
  \item Reporter output: a short note stating the diagnosis, justification (visual+context), differentials, and suggested next steps.
  \item Uncertainty/Audit: below-threshold confidence triggers deferral; log model/version/seed, preprocessing hashes, and thresholds.
\end{itemize}

\subsection{Data and Preprocessing}

\begin{itemize}
  \item Imaging: square crop/pad; resize (e.g., 448--512); per-channel normalize; light color-preserving augments; optional hair artifact suppression.
  \item Metadata: standardize categories; bucketize age if useful; encode as short sentences (e.g., ``Male, 62 years, upper back'').
  \item Imbalance/Quality: use stratified sampling and/or class weights; exclude corrupted images; represent missing metadata explicitly.
\end{itemize}

\subsection{Models and Baselines}

\begin{itemize}
  \item Image encoders: one strong CNN and one ViT-family model, fine-tuned from public weights.
  \item Text encoders: a compact BERT and a clinically pretrained variant for metadata sentences.
  \item Fusion/Calibration: concatenation+linear softmax as the reference; temperature scaling and/or Platt/binning for calibration.
  \item Optimization: AdamW, cosine decay, early stopping on validation AUROC; report random-seed variability.
  \item Practicality: report single-image CPU/GPU latency (mean, p95) and memory footprint.
\end{itemize}

\subsection{Novel Algorithmic Improvement: Uncertainty-Guided Cross-Modal Gated Fusion with Prototype Alignment}

We introduce UG-CMGF, an uncertainty-aware gating mechanism that balances image and metadata features per case and aligns the joint embedding to class prototypes. A selection head defers low-confidence cases to improve safety. The design preserves the simple concatenation baseline as a fallback while improving robustness and providing grounded signals for the report. Gates are robust to missing metadata and are regularized to remain complementary. See Appendix~\ref{app:ugcmgf} for equations, loss terms, and inference flow.

\subsection{Evaluation Protocol}

\begin{itemize}
  \item Metrics: AUROC/AUPRC/Accuracy/F1; ECE, Brier score, and reliability plots; per-class support and confusion matrices.
  \item Generalization: train/validate on ISIC 2019/2020; evaluate on ISIC 2018; sensitivity analyses by site, sex, and device/source.
  \item Significance: bootstrap CIs for all metrics; DeLong or paired bootstrap for AUROC differences; report effect sizes.
  \item Safety/Deferral: track deferral rates and error types; require manual review for deferred/low-confidence cases.
\end{itemize}

\subsection{Implementation Details}

\begin{itemize}
  \item Reproducibility: fixed seeds, deterministic loaders where feasible, exact environment manifests, stored splits.
  \item Packaging: API takes image+metadata$\rightarrow$probabilities+report; CPU/GPU modes; configurable thresholds.
  \item Security/Privacy: remove PII from prompts/logs; hash inputs; restrict logging to essential metadata.
  \item Monitoring: log latency, confidence, model version; support rollbacks, threshold tuning, and structured error reporting.
\end{itemize}

\subsection{Risks, Ethics, and Mitigations}

\begin{itemize}
  \item Overconfidence: use temperature scaling and abstention; display calibrated confidence.
  \item Dataset bias: monitor subgroup metrics; consider re-weighting or thresholds if disparities appear.
  \item Scope/Privacy: restrict generation to diagnostic justification/differentials; exclude PII from prompts and logs.
  \item Reporting risks: mitigate hallucinations via grounded prompts and guardrails; avoid speculative recommendations.
\end{itemize}

\subsection{Deliverables}

\begin{itemize}
  \item Trained baselines and UG-CMGF with configs and weights.
  \item Evaluation report (discrimination, calibration, ablations, OOD).
  \item Prompt templates and a minimal inference package producing calibrated probabilities and concise reports with deferral.
\end{itemize}

% === Section 4: SYSTEM DESIGN  (continued from page 11) ===
\section{SYSTEM DESIGN }

\subsection{Architecture Overview}
The system comprises two stages: a multimodal diagnostic engine that fuses image and metadata features to yield calibrated class probabilities, and a generative reporter that converts structured outputs into a concise clinician-style summary under scope and safety guardrails. The two stages are decoupled to allow independent iteration and testing; interfaces are explicit and versioned.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.9\textwidth]{SYSTEMDESIGN.png}
  \caption{Two-stage system: image/text encoders, gated fusion with prototypes, calibrated softmax, selection head, and controlled prompting.}
  \label{fig:system-design}
\end{figure*}

\subsection{Components}
\paragraph{Image encoder.} A CNN or ViT backbone extracts morphology-sensitive features. After global pooling and a projection layer, we obtain $z_{\text{img}}$ with fixed dimensionality for fusion.

\paragraph{Metadata encoder.} A compact BERT-class model embeds short, templated sentences (e.g., ``Male, 62 years, upper back'') to produce $z_{\text{text}}$.

\paragraph{Fusion and classifier.} The reference uses concatenation with a linear softmax head. UG-CMGF augments this with uncertainty-gated fusion and class prototypes to stabilize decision boundaries.

\paragraph{Calibration and selection.} We apply temperature scaling on validation splits. A selection head supports conservative deferral when confidence is low or conflicts are detected.

\paragraph{Generative reporter.} A structured prompt composed from class, confidence, salient cues, and (optionally) prototype neighbors yields a focused note aligned with dermatology documentation.

\subsection{Data Flow}
\begin{enumerate}
  \item Validate and normalize image+metadata; record preprocessing hashes.
  \item Extract $z_{\text{img}}$ and $z_{\text{text}}$ with frozen/finetuned encoders as configured.
  \item Fuse (concatenation or UG-CMGF) and classify; apply learned calibration parameters.
  \item If above thresholds, generate the report; otherwise, return a defer message with a probability summary and guidance.
\end{enumerate}



\subsection{Prompting Template (Report Skeleton)}
\begin{itemize}
  \item \textbf{Diagnosis}: <top class> (confidence: <value>).
  \item \textbf{Justification}: salient morphology and context summarized from image cues and metadata.
  \item \textbf{Differentials}: 2--3 plausible alternatives with brief rationale.
  \item \textbf{Next steps}: dermoscopy follow-up or escalation guidance consistent with scope.
  \item \textbf{Note}: this summary supports---not replaces---clinical judgment.
\end{itemize}

\subsection{Deployment Considerations}
\begin{itemize}
  \item Stateless inference service exposing a simple API (image + metadata $\rightarrow$ probabilities + report).
  \item CPU and GPU targets; configurable thresholds for deferral and report length.
  \item Logging for inputs (hashed), outputs, latency, confidence, and model version for audit.
\end{itemize}

\subsection{Assumptions and Limitations}
\begin{itemize}
  \item Scope limited to dermoscopy and the specified metadata fields; no treatment recommendations.
  \item Reports remain decision support and require clinician review, especially on deferred or low-confidence cases.
\end{itemize}


% === RESULTS AND ANALYSIS ===
\section{RESULTS AND ANALYSIS}
We report discrimination, calibration, generalization, and reporting quality under fixed seeds and matched preprocessing. Confidence intervals come from bootstrap resampling; significance testing uses paired bootstraps (and DeLong for AUROC where applicable). Error analysis examines failure modes by class, site, and sex.

\subsection{Experimental Setup}
\begin{itemize}
  \item \textit{Datasets:} ISIC 2019/2020 for development; ISIC 2018 for out-of-distribution tests.
  \item \textit{Splits:} patient-level stratification; class-balancing as noted; transforms as in Methodology.
  \item \textit{Models:} image encoders (CNN, ViT); text encoders (general vs clinical); fusion (concat; UG-CMGF).
  \item \textit{Training:} identical schedules unless otherwise stated; temperature scaling on validation.
\end{itemize}

\subsection{Baselines}
We compare image-only, text-only, and multimodal concatenation baselines. Report AUROC, AUPRC, accuracy, F1, and per-class metrics with support. Include confusion matrices and class-wise breakdowns for interpretability.

\subsection{Ablations}
Ablations include: (i) CNN vs ViT, (ii) general vs clinical text encoder, (iii) concatenation vs UG-CMGF, (iv) prototype loss on/off, and (v) uncertainty-free vs uncertainty-guided gates. Present reliability diagrams and calibration metrics alongside discrimination for each variant.

\subsection{Calibration, Deferral, and Robustness}
We analyze calibration via ECE and Brier score and show reliability plots. We report deferral rates from the selection head, error types on deferred vs non-deferred cases, and sensitivity of decisions to threshold changes. OOD performance on ISIC 2018 is summarized separately.

\subsection{Qualitative Examples}
We include paired dermoscopic images with generated notes. Each example highlights the predicted class, confidence, key visual cues (e.g., pigment network, streaks), context use (age, site), plausible differentials, and note brevity/clarity. All examples exclude PII.

\subsection{Efficiency and Deployment Metrics}
We report single-image latency on CPU and GPU (mean and p95), memory usage, and throughput estimates for batch sizes of interest. We also summarize logging footprint, thresholds used for deferral, and any fallback behavior under missing metadata.

% === REFERENCES SECTION ===
\section{REFERENCES}

% Your main references go here...
% Example:
% [1] Author Name, Title, Journal/Conference, Year.

\subsection*{Datasets}
\addcontentsline{toc}{subsection}{Datasets}

\begin{itemize}
  \item \textbf{ISIC 2020:} Contains over 33,000 images and metadata. Focuses on melanoma detection.  
  \url{https://challenge2020.isic-archive.com/}

  \item \textbf{ISIC 2019:} Contains over 25,000 images with 8 diagnostic categories.  
  \url{https://challenge2019.isic-archive.com/}

  \item \textbf{ISIC 2018:} Contains 10,000 images for lesion classification into 7 categories.  
  \url{https://challenge2018.isic-archive.com/}

  \item \textbf{Kaggle Resources:}\\ 
  \url{https://www.kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000/code}\\
  \url{https://www.kaggle.com/code/sujitmishra64/melanoma-detection}\\
  \url{https://www.kaggle.com/datasets/fanconic/skin-cancer-malignant-vs-benign/code}

  \item \textbf{ISIC Archive Main Page:} \url{https://www.isic-archive.com/}

  \item \textbf{NIH Open-i Medical Image Archive:} \url{https://openi.nlm.nih.gov/}
\end{itemize}

{\small
\begin{thebibliography}{99}

\bibitem{chatterjee2024}
Chatterjee, S., Fruhling, A., Kotiadis, K., \& Gartner, D. (2024). \emph{Towards new frontiers of healthcare systems research using artificial intelligence and generative AI}. Health Systems, 13(4), 263--273. DOI: 10.1080/20476965.2024.2402128

\bibitem{reddy2024}
Reddy, S. (2024). Generative AI in healthcare: an implementation science informed translational path on application, integration and governance. Implementation Science, 19:27. https://doi.org/10.1186/s13012-024-01357-9

\bibitem{saeed2023}
Saeed, M., Naseer, A., Masood, H., Rehman, S. U., \& Gruhn, V. (2023). \emph{The Power of Generative AI to Augment for Enhanced Skin Cancer Classification: A Deep Learning Approach}. IEEE Access. DOI: 10.1109/ACCESS.2023.3332628

\bibitem{lasalvia2022}
La Salvia, M., Torti, E., Leon, R., Fabelo, H., Ortega, S., Martinez-Vega, B., Callico, G. M., \& Leporati, F. (2022). \emph{Deep Convolutional Generative Adversarial Networks to Enhance Artificial Intelligence in Healthcare: A Skin Cancer Application}. \textit{Sensors}, 22(16), Article 6145. https://doi.org/10.3390/s22166145


\bibitem{jutte2024}
Jütte, L., González-Villà, S., Quintana, J., Steven, M., Garcia, R., \& Roth, B. (2024). \emph{Integrating generative AI with ABCDE rule analysis for enhanced skin cancer diagnosis, dermatologist training and patient education}. Frontiers in Medicine, 11, Article 1445318. doi:10.3389/fmed.2024.1445318


\bibitem{tsai2024}
Tsai, A.-C., Huang, P.-H., Wu, Z.-C., & Wang, J.-F. (2024). \emph{Advanced Pigmented Facial Skin Analysis Using Conditional Generative Adversarial Networks}. 12, 46646–46656. doi:10.1109/ACCESS.2024.3381535

\bibitem{thoviti2024}
Thoviti, S. H., Varma, B. K., Sai, S. N., \& Prasanna, B. L. (2024). \emph{Generative AI Empowered Skin Cancer Diagnosis: Advancing Classification Through Deep Learning}. In 2024 International Conference on IoT Based Control Networks and Intelligent Systems (ICICNIS) (pp. ⁠—). IEEE. DOI:10.1109/ICICNIS64247.2024.10823133

\bibitem{reddy2025}
Reddy, N. N., \& Agarwal, P. (2025). \emph{Diagnosis and Classification of Skin Cancer Using Generative Artificial Intelligence (Gen AI)}. In Generative Artificial Intelligence for Biomedical and Smart Health Informatics (pp. 591–605). Wiley. DOI:10.1002/9781394280735.ch28

\bibitem{garciaespinosa2025}
Garcia-Espinosa, E., Ruiz-Castilla, J. S., \& Garcia-Lamont, F. (2025). \emph{Generative AI and Transformers in Advanced Skin Lesion Classification applied on a mobile device}. International Journal of Combinatorial Optimization Problems and Informatics, 16(2), 158–175. https://doi.org/10.61467/2007.1558.2025.v16i2.1078

\bibitem{amgothu2025}
Amgothu, S., Lokesh, A., Kumar, S. S., Devipriyanka, S., \& Chandu, R. (2025). \emph{Enhanced Skin Lesion Analysis using Generative AI for Cancer Diagnosis}. In 2025 International Conference on Sensors and Related Networks (SENNET) – Special Focus on Digital Healthcare (SENNET 64220), Bengaluru, India, July 24–27, 2025. IEEE. DOI:10.1109/SENNET64220.2025.11136018

\bibitem{jutte2025bios}
Jütte, L., González-Villà, S., Quintana, J., Steven, M., Garcia, R., \& Roth, B. (2025). \emph{Generative AI for enhanced skin cancer diagnosis, dermatologist training, and patient education}. In Proceedings of SPIE—International Society for Optics and Photonics (Vol. 13292, p. 132920F), Photonics in Dermatology and Plastic Surgery, BiOS 2025, San Francisco, CA, USA, March 19, 2025. https://doi.org/10.1117/12.3042664

\bibitem{udrea2017}
Udrea, A., \& Mitra, G. D. (2017). \emph{Generative Adversarial Neural Networks for Pigmented and Non-Pigmented Skin Lesions Detection in Clinical Images}. In 2017 21st International Conference on Control Systems and Computer Science (CSCS), Bucharest, Romania, May 29–31, 2017. IEEE. DOI:10.1109/CSCS.2017.56

\bibitem{kalaivani2024}
Kalaivani, A., Sangeetha Devi, A., \& Shanmugapriya, A. (2024). \emph{Generative Models and Diffusion Models for Skin Sore Detection and Treatment}. In 2024 4th International Conference on Ubiquitous Computing and Intelligent Information Systems (ICUIS), Gobichettipalayam, India, December 12–13, 2024. IEEE. DOI:10.1109/ICUIS64676.2024.10866246

\bibitem{mutepfe2021}
Mutepfe, F., Kalejahi, B. K., Meshgini, S., \& Danishvar, S. (2021). \emph{Generative Adversarial Network Image Synthesis Method for Skin Lesion Generation and Classification}. Journal of Medical Signals & Sensors, 11(4), 237–252. doi:10.4103/jmss.JMSS5320

\bibitem{innani2023}
Innani, S., Dutande, P., Baid, U., Pokuri, V., Bakas, S., Talbar, S., Baheti, B., \& Guntuku, S. C. (2023). \emph{Generative adversarial networks based skin lesion segmentation}. Scientific Reports, 13, Article 13467. doi:10.1038/s41598-023-39648-8

\bibitem{masood2024}
Masood, H., Naseer, A., \& Saeed, M. (2024). \emph{Optimized Skin Lesion Segmentation: Analysing DeepLabV3+ and ASSP Against Generative AI-Based Deep Learning Approach}. Foundations of Science. Advance online publication. https://doi.org/10.1007/s10699-024-09957-w

\bibitem{wen2024}
Wen, D., Soltan, A. A., Trucco, E., \& Matin, R. N. (2024). \emph{From data to diagnosis: skin cancer image datasets for artificial intelligence}. Clinical and Experimental Dermatology, 49(7), 675–685. doi:10.1093/ced/llae112

\bibitem{rao2025}
Mallikharjuna Rao, K., Ghanta Sai Krishna, Supriya, K., \& Meetiksha Sorgile. (2025). \emph{LesionAid: vision transformers-based skin lesion generation and classification – A practical review}. Multimedia Tools and Applications. Advance online publication. doi:10.1007/s11042-025-20797-z

\bibitem{bissoto2020}
Bissoto, A., \& Avila, S. (2020). \emph{Improving Skin Lesion Analysis with Generative Adversarial Networks}. In Anais Estendidos da XXXIII Conference on Graphics, Patterns and Images, Workshop de Teses e Dissertações. DOI:10.5753/sibgrapi.est.2020.12986

\bibitem{bissoto2018}
Bissoto, A., Perez, F., Valle, E., \& Avila, S. (2018). \emph{Skin Lesion Synthesis with Generative Adversarial Networks}. In OR 2.0 Context-Aware Operating Theaters, Computer Assisted Robotic Endoscopy, Clinical Image-Based Procedures, and Skin Image Analysis (Lecture Notes in Computer Science, Vol. 11041, pp. 294–302). Springer. https://doi.org/10.1007/978-3-030-01201-432

\bibitem{marques2024}
Marques, A. G., de Figueiredo, M. V. C., Nascimento, J. J. d. C., de Souza, C. T., de Mattos Dourado Júnior, C. M. J., \& de Albuquerque, V. H. C. (2024). \emph{New Approach Generative AI Melanoma Data Fusion for Classification in Dermoscopic Images with Large Language Model}. In 2024 37th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI), Manaus, Brazil, September 30–October 3, 2024. IEEE. DOI:10.1109/SIBGRAPI62404.2024.10716298

\bibitem{salvi2022}
Salvi, M., Branciforti, F., Veronese, F., Zavattaro, E., Tarantino, V., Savoia, P., \& Meiburger, K. M. (2022). \emph{DermoCC-GAN: A new approach for standardizing dermatological images using generative adversarial networks}. Computer Methods and Programs in Biomedicine, 225, Article 107040. doi:10.1016/j.cmpb.2022.107040

\bibitem{veeramani2025}
Veeramani, N., \& Jayaraman, P. (2025). \emph{A promising AI based super resolution image reconstruction technique for early diagnosis of skin cancer}. Scientific Reports, 15, Article 5084. doi:10.1038/s41598-025-89693-8

\bibitem{wang2023}
Wang, H., Qi, Q., Sun, W., Li, X., Dong, B., \& Yao, C. (2023). \emph{Classification of skin lesions with generative adversarial networks and improved MobileNetV2}. International Journal of Imaging Systems and Technology, advance online publication. https://doi.org/10.1002/ima.22880

\bibitem{ravindranath2025}
Ravindranath, R. C., Vikas, K. R., Chandramma, R., Sheela, S., Ruhin Kouser, R., \& Dhiraj, C. (2025). \emph{DermaGAN: Enhancing Skin Lesion Classification with Generative Adversarial Networks}. In 2025 International Conference on Emerging Technologies in Computing and Communication (ETCC), June 26–27, 2025. IEEE. DOI:10.1109/ETCC65847.2025.11108424


\bibitem{alrasheed2022}
Al-Rasheed, A., Ksibi, A., Ayadi, M., Alzahrani, A. I. A., Zakariah, M., & Ali Hakami, N. (2022). \emph{An Ensemble of Transfer Learning Models for the Prediction of Skin Lesions with Conditional Generative Adversarial Networks}. Diagnostics, 12(12), Article 3145. doi:10.3390/diagnostics12123145

\bibitem{abbasi2024deep}
S. Abbasi, M. B. Farooq, T. Mukherjee, J. Churm, O. Pournik, G. Epiphaniou, and T. N. Arvanitis, 
``Deep learning-based synthetic skin lesion image classification,'' 
in \textit{Proc. 34th Medical Informatics Europe Conf. (MIE)}, 
pp. 1145--1150, IOS Press, 2024.

\bibitem{medi2021skinaid}
P. R. Medi, P. Nemani, V. R. Pitta, V. Udutalapally, D. Das, and S. P. Mohanty, 
``Skinaid: A GAN-based automatic skin lesion monitoring method for IoMT frameworks,'' 
in \textit{Proc. 2021 19th OITS Int. Conf. Inf. Technol. (OCIT)}, 
pp. 200--205, IEEE, 2021.

\bibitem{farooq2024dermt2im}
M. A. Farooq, Y. Wang, M. Schukat, M. A. Little, and P. Corcoran, 
``Derm-T2IM: Harnessing synthetic skin lesion data via stable diffusion models for enhanced skin disease classification using ViT and CNN,'' 
in \textit{Proc. 2024 46th Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. (EMBC)}, 
pp. 1--5, IEEE, 2024.

\bibitem{rao2025synthetic}
A. S. Rao, J. Kim, A. Mu, C. C. Young, E. Kalmowitz, M. Senter-Zapata, D. C. Whitehead, L. Garibyan, A. B. Landman, and M. D. Succi, 
``Synthetic medical education in dermatology leveraging generative artificial intelligence,'' 
\textit{npj Digit. Med.}, vol. 8, no. 1, p. 247, 2025.

\bibitem{burlina2020ai}
P. M. Burlina, W. Paul, P. A. Mathew, N. J. Joshi, A. W. Rebman, and J. N. Aucott, 
``AI progress in skin lesion analysis,'' 
\textit{arXiv preprint arXiv:2009.13323}, 2020.




\end{thebibliography}
}

\clearpage
\appendix
\section{UG-CMGF: Method Details}\label{app:ugcmgf}
\paragraph{Design overview.}
We propose \textbf{UG-CMGF}, an uncertainty-aware fusion mechanism that learns to gate the contributions of image and metadata features on a per-sample basis, while aligning the joint embedding to class prototypes for stability and interpretability.
\begin{itemize}
  \item \textit{Uncertainty heads}: attach lightweight evidential heads to both image and text encoders to estimate per-sample uncertainty from intermediate features.
  \item \textit{Gated fusion}: compute gates $g_{\text{img}}$ and $g_{\text{text}}$ from uncertainty scores using a small MLP with sigmoid outputs and a soft penalty encouraging $g_{\text{img}} + g_{\text{text}} \approx 1$. Form the fused embedding:
  \[
    z = g_{\text{img}} \cdot z_{\text{img}} \;+\; g_{\text{text}} \cdot z_{\text{text}}.
  \]
  \item \textit{Prototype alignment}: maintain class prototypes $\{\mu_c\}$ in the joint space and add a prototypical contrastive loss that pulls samples toward the correct prototype and pushes away from others.
  \item \textit{Selective prediction}: a selection head $s(z)$ estimates whether to auto-report or defer; low $s(z)$ triggers a ``review required'' path and conservative prompting.
  \item \textit{Grounded explanation}: expose top prototypes and gate values to the reporting prompt so rationales emphasize morphology when $g_{\text{img}}$ is high and contextual priors when $g_{\text{text}}$ dominates.
\end{itemize}
\paragraph{Training objective.}
\[
  \mathcal{L} = \mathcal{L}{\text{cls}} \;+\; \lambda_1 \mathcal{L}{\text{proto}} \;+\; \lambda_2 \mathcal{L}{\text{gate}} \;+\; \lambda_3 \mathcal{L}{\text{sel}} \;+\; \lambda_4 \mathcal{L}_{\text{cal}},
\]
where $\mathcal{L}{\text{cls}}$ is cross-entropy, $\mathcal{L}{\text{proto}}$ is the prototypical contrastive term, $\mathcal{L}{\text{gate}}$ regularizes complementary gates and robustness to missing metadata, $\mathcal{L}{\text{sel}}$ trains the selection head using confident-correct targets, and $\mathcal{L}_{\text{cal}}$ captures calibration (or a temperature-scaling proxy).
\paragraph{Inference flow.}
Encode image and metadata, estimate uncertainty, compute gates, form $z$, and output probabilities. If $s(z)$ is below threshold or the maximum probability is low, return a defer message. Otherwise, compose a structured prompt with class, confidence, salient visual tokens, metadata cues, gate values, and nearest prototypes to generate the concise report.
\paragraph{Expected benefits.}
UG-CMGF down-weights noisy metadata when it conflicts with strong visual evidence and elevates contextual priors when images are ambiguous. Prototype alignment stabilizes boundaries and supports semantically grounded justifications. The selection head provides principled abstention for safer deployment.

\subsection{Ablation Protocols}\label{app:ablations}
Compare: (i) concatenation baseline vs UG-CMGF, (ii) with/without prototype loss, (iii) with/without selection head, (iv) uncertainty-free gates vs uncertainty-guided gates, and (v) image-only and text-only controls. Report discrimination, calibration, and deferral-quality metrics.


\end{document}