% !TeX program = lualatex
\documentclass[12pt,a4paper]{article}

\usepackage{fontspec}
\setmainfont{Times New Roman}
\usepackage[margin=0.74in]{geometry}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{pagecolor}
\usepackage{ragged2e}
\usepackage{newunicodechar}
\usepackage{array}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{enumitem}
\usepackage[font=small]{caption}
\usepackage{hyperref}
\captionsetup[figure]{skip=6pt}
\setlength{\textfloatsep}{10pt plus 2pt minus 2pt}
\setlength{\floatsep}{8pt plus 2pt minus 2pt}
\setlength{\intextsep}{8pt plus 2pt minus 2pt}
\setlist[itemize]{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}
\setlist[enumerate]{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}
\newunicodechar{–}{\textendash}   % en dash
\newunicodechar{—}{\textemdash}   % em dash
\newunicodechar{‑}{-}             % non-breaking hyphen (U+2011) -> normal hyphen
\newunicodechar{−}{-}             % math minus (U+2212) -> normal hyphen
\newunicodechar{ }{~}             % NBSP (U+00A0) -> non-breaking space
\newunicodechar{ }{\,}            % NNBSP (U+202F) -> thin space
\newcolumntype{C}{>{\centering\arraybackslash}p{#1}}
\newcolumntype{Y}{>{\raggedright\arraybackslash}X}

\definecolor{pagebg}{HTML}{FFFDF7}
\pagecolor{pagebg}

\setstretch{1.5}

\begin{document}

% === TITLE PAGE ===
\begin{titlepage}
\thispagestyle{empty}
\begin{center}

{\Large\bfseries B.Tech. BCSE497J -- Project -- I\par}

\vspace{1.0cm}

{\setstretch{1.5}%
\fontsize{18pt}{27pt}\selectfont\bfseries
\MakeUppercase{A Multimodal Generative AI System for Skin Lesion Diagnosis and Explanation}\par}

\vspace{0.6cm}

{\itshape Submitted in partial fulfillment of the requirements for the degree of\par}

\vspace{0.3cm}

{\Large\bfseries Bachelor of Technology\par}

\vspace{0.3cm}

{\itshape in\par}

\vspace{0.3cm}

{\Large\bfseries Computer Science and Engineering (Core)\par}

\vspace{0.3cm}

{\itshape by\par}

\vspace{0.5cm}

% Team members table - sorted by registration number
\begin{tabular}{@{} l @{\hspace{1.8cm}} l @{}}
\textbf{22BCE0476} & \textbf{AMAN CHAUHAN} \\
\textbf{22BCE0830} & \textbf{ARNAV SINHA} \\
\textbf{22BCE2218} & \textbf{AKSHAT SINHA} \\
\end{tabular}

\vfill

{\itshape Under the Supervision of\par}
\vspace{0.4cm}
{\large\bfseries DR. NAGA PRIYADARSINI R}\par
\vspace{0.2cm}
{\itshape Assistant Professor Sr. Grade 1\par}
{\itshape Department of Analytics\par}
{\itshape School of Computer Science and Engineering (SCOPE)\par}

\vfill

\includegraphics[width=3.1in,height=0.9in,keepaspectratio]{logo_scope.png}\par

\vspace{0.6cm}

November 10, 2025\par

\end{center}
\end{titlepage}

% === DECLARATION PAGE ===
\clearpage
\pagenumbering{roman}
\setcounter{page}{1}

\begin{center}
{\Large\bfseries DECLARATION\par}
\end{center}

\vspace{1cm}

\justifying
\setstretch{1.5}

We hereby declare that the project entitled \textbf{A Multimodal Generative AI System for Skin Lesion Diagnosis and Explanation} submitted by us, for the award of the degree of \textit{Bachelor of Technology in Computer Science and Engineering} to VIT is a record of bonafide work carried out by us under the supervision of Dr. Naga Priyadarsini R.

We further declare that the work reported in this project has not been submitted and will not be submitted, either in part or in full, for the award of any other degree or diploma in this institute or any other institute or university.

\vspace{2cm}

\noindent Place: Vellore \hfill Date: \underline{\hspace{3cm}}

\vspace{1.5cm}

\noindent \textbf{Signature of the Candidates}

\vspace{0.8cm}

\noindent \textbf{22BCE0476 -- Aman Chauhan} \hfill \underline{\hspace{4cm}}

\vspace{0.5cm}

\noindent \textbf{22BCE0830 -- Arnav Sinha} \hfill \underline{\hspace{4cm}}

\vspace{0.5cm}

\noindent \textbf{22BCE2218 -- Akshat Sinha} \hfill \underline{\hspace{4cm}}

% === CERTIFICATE PAGE ===
\clearpage

\begin{center}
{\Large\bfseries CERTIFICATE\par}
\end{center}

\vspace{1cm}

\justifying
\setstretch{1.5}

This is to certify that the project entitled \textbf{A Multimodal Generative AI System for Skin Lesion Diagnosis and Explanation} submitted by \textbf{Aman Chauhan (22BCE0476), Arnav Sinha (22BCE0830), and Akshat Sinha (22BCE2218)}, \textbf{School of Computer Science and Engineering}, VIT, for the award of the degree of \textit{Bachelor of Technology in Computer Science and Engineering}, is a record of bonafide work carried out by them under my supervision during Fall Semester 2024-2025, as per the VIT code of academic and research ethics.

The contents of this report have not been submitted and will not be submitted either in part or in full, for the award of any other degree or diploma in this institute or any other institute or university. The project fulfills the requirements and regulations of the University and in my opinion meets the necessary standards for submission.

\vspace{1.2cm}

\noindent Place: Vellore \hfill Date: \underline{\hspace{3cm}}

\vspace{1.2cm}

\noindent \textbf{Signature of the Guide}

\vspace{0.3cm}

\noindent \textbf{Dr. Naga Priyadarsini R}

\noindent \textit{Assistant Professor Sr. Grade 1}

\noindent \textit{Department of Analytics}

\noindent \textit{School of Computer Science and Engineering (SCOPE)}

\vspace{1cm}

\noindent \textbf{Examiner(s)}

\vspace{0.5cm}

\noindent 1. \underline{\hspace{8cm}}

\vspace{0.3cm}

\noindent 2. \underline{\hspace{8cm}}

\vspace{1cm}

\noindent \textbf{Countersigned by}

\vspace{0.5cm}

\noindent \textbf{Bhuminathan P}

\noindent \textit{B.Tech}

% === ACKNOWLEDGEMENTS PAGE ===
\clearpage

\begin{center}
{\Large\bfseries ACKNOWLEDGEMENTS\par}
\end{center}

\vspace{1cm}

\justifying
\setstretch{1.5}

We are deeply grateful to the management of Vellore Institute of Technology (VIT) for providing us with the opportunity and resources to undertake this project. Their commitment to fostering a conducive learning environment has been instrumental in our academic journey. The support and infrastructure provided by VIT have enabled us to explore and develop our ideas to their fullest potential.

Our sincere thanks to Dr. Jaisankar N, the Dean of the School of Computer Science and Engineering (SCOPE), for his unwavering support and encouragement. His leadership and vision have greatly inspired us to strive for excellence. The Dean's dedication to academic excellence and innovation has been a constant source of motivation for us. We appreciate his efforts in creating an environment that nurtures creativity and critical thinking.

We express our profound appreciation to the Head of the Department of Analytics, for insightful guidance and continuous support. The expertise and advice have been crucial in shaping the direction of our project. The Head of Department's commitment to fostering a collaborative and supportive atmosphere has greatly enhanced our learning experience. The constructive feedback and encouragement have been invaluable in overcoming challenges and achieving our project goals.

We are immensely thankful to our project guide, Dr. Naga Priyadarsini R, for her dedicated mentorship and invaluable feedback. Her patience, knowledge, and encouragement have been pivotal in the successful completion of this project. Our supervisor's willingness to share her expertise and provide thoughtful guidance has been instrumental in refining our ideas and methodologies. Her support has not only contributed to the success of this project but has also enriched our overall academic experience.

Thank you all for your contributions and support.

\vspace{2cm}

\noindent \textbf{Aman Chauhan}

\noindent \textbf{Arnav Sinha}

\noindent \textbf{Akshat Sinha}

% === TABLE OF CONTENTS ===
\clearpage

\begin{center}
{\fontsize{14pt}{21pt}\selectfont\bfseries TABLE OF CONTENTS\par}
\end{center}

\vspace{0.5cm}

\setstretch{1.5}
\renewcommand{\arraystretch}{1.05}

{\normalsize
\begin{longtable}{|>{\centering\arraybackslash}p{1.4cm}|p{9.5cm}|>{\centering\arraybackslash}p{2.2cm}|}
\hline
\textbf{Sl.No} & \textbf{Contents} & \textbf{Page No.} \\
\hline
\endfirsthead

\hline
\textbf{Sl.No} & \textbf{Contents} & \textbf{Page No.} \\
\hline
\endhead

\hline
\endfoot

\hline
\endlastfoot

 & \hyperlink{abstract}{\textbf{Abstract}} & \textbf{i} \\
\hline
\textbf{1.} & \hyperlink{chapter1}{\textbf{INTRODUCTION}} & \textbf{1} \\
 & \hyperlink{sec:1.1}{1.1 Background} & 1 \\
 & \hyperlink{sec:1.2}{1.2 Motivations} & 1 \\
 & \hyperlink{sec:1.3}{1.3 Scope of the Project} & 1 \\
\hline
\textbf{2.} & \hyperlink{chapter2}{\textbf{PROJECT DESCRIPTION AND GOALS}} & \textbf{2} \\
 & \hyperlink{sec:2.1}{2.1 Literature Review} & 2 \\
 & 2.2 Research Gap & 3 \\
 & 2.3 Objectives & 3 \\
 & \hyperlink{sec:2.4}{2.4 Problem Statement} & 3 \\
 & \hyperlink{sec:2.5}{2.5 Project Plan} & 3 \\
\hline
\textbf{3.} & \hyperlink{chapter3}{\textbf{TECHNICAL SPECIFICATION}} & \textbf{4} \\
 & 3.1 Requirements & 4 \\
 & \quad 3.1.1 Functional & 4 \\
 & \quad 3.1.2 Non-Functional & 4 \\
 & 3.2 Feasibility Study & 5 \\
 & \quad 3.2.1 Technical Feasibility & 5 \\
 & \quad 3.2.2 Economic Feasibility & 5 \\
 & \quad 3.2.2 Social Feasibility & 5 \\
 & 3.3 System Specification & 5 \\
 & \quad 3.3.1 Hardware Specification & 5 \\
 & \quad 3.3.2 Software Specification & 5 \\
\hline
\textbf{4.} & \hyperlink{chapter4}{\textbf{DESIGN APPROACH AND DETAILS}} & \textbf{6} \\
 & 4.1 System Architecture & 6 \\
 & 4.2 Design & 7 \\
 & \quad 4.2.1 Data Flow Diagram & 7 \\
 & \quad 4.2.2 Use Case Diagram & 7 \\
 & \quad 4.2.3 Class Diagram & 7 \\
 & \quad 4.2.4 Sequence Diagram & 7 \\
\hline
\textbf{5.} & \hyperlink{chapter5}{\textbf{METHODOLOGY AND TESTING}} & \textbf{8} \\
 & 5.1 Module Description & 8 \\
 & 5.2 Testing & 8 \\
\hline
\textbf{6.} & \hyperlink{chapter6}{\textbf{PROJECT DEMONSTRATION}} & \textbf{9} \\
\hline
\textbf{7.} & \hyperlink{chapter7}{\textbf{RESULT AND DISCUSSION (COST ANALYSIS as applicable)}} & \textbf{10} \\
\hline
\textbf{8.} & \hyperlink{chapter8}{\textbf{CONCLUSION}} & \textbf{11} \\
\hline
\textbf{9.} & \hyperlink{chapter9}{\textbf{REFERENCES}} & \textbf{12} \\
\hline
 & \hyperlink{appendixa}{\textbf{APPENDIX A}} & \textbf{13} \\
\hline
 & \hyperlink{appendixb}{\textbf{APPENDIX B}} & \textbf{14} \\
\hline
\end{longtable}
}

\setstretch{1.5}

% === LIST OF FIGURES ===
\clearpage

\begin{center}
{\fontsize{14pt}{21pt}\selectfont\bfseries\MakeUppercase{List of Figures}\par}
\end{center}

\vspace{0.5cm}

\noindent\begin{tabularx}{\textwidth}{|>{\centering\arraybackslash}p{2.5cm}|X|>{\centering\arraybackslash}p{2.2cm}|}
\hline
\textbf{Figure No.} & \textbf{Title} & \textbf{Page No.} \\
\hline
2.1 & Work Breakdown Structure (WBS) & \\
\hline
2.2 & Activity chart (Gantt) aligned to Phases 1--5 & \\
\hline
4.1 & Two-stage system: image/text encoders, gated fusion with prototypes, calibrated softmax, selection head, and controlled prompting & \\
\hline
4.2 & Level 0 Data Flow Diagram showing the high-level context of the multimodal skin lesion diagnosis system & \\
\hline
4.3 & Level 2 Data Flow Diagram providing detailed view of internal processes & \\
\hline
4.4 & Use Case Diagram showing the primary actors and their interactions with the system & \\
\hline
4.5 & Class Diagram showing the object-oriented structure of the system components & \\
\hline
4.6 & Sequence Diagram showing the temporal flow of interactions between system components during a typical diagnostic workflow & \\
\hline
\end{tabularx}

% === LIST OF TABLES ===
\clearpage

\begin{center}
{\fontsize{14pt}{21pt}\selectfont\bfseries\MakeUppercase{List of Tables}\par}
\end{center}

\vspace{0.5cm}

\noindent\begin{tabularx}{\textwidth}{|>{\centering\arraybackslash}p{2.5cm}|X|>{\centering\arraybackslash}p{2.2cm}|}
\hline
\textbf{Table No.} & \textbf{Title} & \textbf{Page No.} \\
\hline
7.1 & Performance comparison on implemented datasets & \\
\hline
\end{tabularx}

% === LIST OF ABBREVIATIONS ===
\clearpage

\begin{center}
{\fontsize{14pt}{21pt}\selectfont\bfseries\MakeUppercase{List of Abbreviations}\par}
\end{center}

\vspace{0.5cm}

\begin{tabular}{ll}
AI & Artificial Intelligence \\
API & Application Programming Interface \\
AUPRC & Area Under Precision-Recall Curve \\
AUROC & Area Under Receiver Operating Characteristic \\
AWS & Amazon Web Services \\
BERT & Bidirectional Encoder Representations from Transformers \\
CNN & Convolutional Neural Network \\
CPU & Central Processing Unit \\
ECE & Expected Calibration Error \\
GPU & Graphics Processing Unit \\
ISIC & International Skin Imaging Collaboration \\
LLM & Large Language Model \\
MLP & Multi-Layer Perceptron \\
NLP & Natural Language Processing \\
OOD & Out-of-Distribution \\
PII & Personally Identifiable Information \\
UG-CMGF & Uncertainty-Guided Cross-Modal Gated Fusion \\
VIT & Vellore Institute of Technology \\
ViT & Vision Transformer \\
WBS & Work Breakdown Structure \\
\end{tabular}

% === SYMBOLS AND NOTATIONS ===
\clearpage

\begin{center}
{\fontsize{14pt}{21pt}\selectfont\bfseries\MakeUppercase{Symbols and Notations}\par}
\end{center}

\vspace{0.5cm}

\begin{tabular}{ll}
\(z_{\text{img}}\) & Image embedding vector \\
\(z_{\text{text}}\) & Text/metadata embedding vector \\
\(z\) & Fused multimodal embedding \\
\(g_{\text{img}}\) & Gate weight for image modality \\
\(g_{\text{text}}\) & Gate weight for text modality \\
\(\mu_c\) & Class prototype vector for class \(c\) \\
\(s(z)\) & Selection head output (confidence score) \\
\(\mathcal{L}_{\text{cls}}\) & Classification loss (cross-entropy) \\
\(\mathcal{L}_{\text{proto}}\) & Prototypical contrastive loss \\
\(\mathcal{L}_{\text{gate}}\) & Gate regularization loss \\
\(\mathcal{L}_{\text{sel}}\) & Selection head loss \\
\(\mathcal{L}_{\text{cal}}\) & Calibration loss \\
\(\lambda_1, \lambda_2, \lambda_3, \lambda_4\) & Loss weighting hyperparameters \\
\end{tabular}

% === ABSTRACT ===
\clearpage
\hypertarget{abstract}{}

\begin{center}
{\fontsize{16pt}{24pt}\selectfont\bfseries\MakeUppercase{Abstract}\par}
\end{center}

\vspace{0.5cm}

{\setstretch{1.15}\normalsize\justifying
Early and reliable differentiation between benign and malignant skin lesions is one of the main goals in dermatology, but routine diagnosis can often be subjective and inconsistent. Clinicians are usually pressed for time, and visual dermoscopy on its own can sometimes lead to uncertainty, especially when important patient information such as age, sex, and lesion location is not considered alongside the image. To address this challenge, this work presents a two-stage clinical assistant designed to combine image-based and patient-based information. The system aims to deliver a calibrated and interpretable diagnosis while also providing clear, clinician-style explanations that improve transparency, efficiency in documentation, and trust among healthcare providers.

In Stage 1, known as the diagnostic engine, dermoscopic images are processed by a modern vision model that learns to identify important visual patterns. Alongside this, structured clinical data such as age, sex, and body site are converted into natural language sentences and processed using a medical text encoder. The visual and textual representations are then combined and passed to a softmax classifier that outputs the probability of each possible lesion type. The system is trained using large, publicly available datasets of skin images and metadata, with additional cross-dataset evaluations to measure robustness and generalization on unseen data.

In Stage 2, the generative engine uses the predicted class, confidence score, and available patient details to create a structured prompt for a large language model. The model generates a short and focused report that explains the diagnosis, highlights key image and metadata cues, lists other possible conditions, and recommends next steps when relevant. Planned experiments include comparing CNN and Vision Transformer models for image processing and testing different text encoders for metadata. Further studies will evaluate the benefits of combining multiple data types and measure the clarity and usefulness of the generated reports. Overall, the system is intended to act as a reliable second opinion for dermatologists and support early and accurate melanoma detection in real-world clinical workflows.
}

% === MAIN MATTER: CHAPTER 1 ===
\clearpage
\pagenumbering{arabic}
\setcounter{page}{1}

\setstretch{1.5}
\justifying

\section*{CHAPTER 1}
\addcontentsline{toc}{section}{CHAPTER 1}
\hypertarget{chapter1}{}

\section{INTRODUCTION}

Dermatology decisions depend on dermoscopic images and simple patient context, yet image-only or text-only approaches miss complementary signals and black-box outputs hinder trust. We build a two-stage assistant tailored to dermoscopy: (i) a diagnostic engine that fuses embeddings from an image encoder and a metadata text encoder to produce calibrated class probabilities, and (ii) a generative reporter that turns the prediction, confidence, and context into a short clinician-style note with justification, differentials, and next steps. The design aims for accuracy, clarity, and auditability so the tool acts as a reliable second opinion without replacing clinical judgment.

\hypertarget{sec:1.1}{}
\subsection{Background}

Dermoscopy exposes morphology (networks, streaks, vessels) that benefits from learned features, while age, sex, and lesion site shift priors and disambiguate similar visuals. Multimodal learning combines these: an image encoder for morphology, a text encoder for context, and a simple fusion for classification. Large public cohorts support robust training and cross-dataset checks. Generative models, when grounded on structured outputs and constrained prompts, can produce concise, reviewable notes that make reasoning explicit.

\hypertarget{sec:1.2}{}
\subsection{Motivations}

We target earlier, more consistent triage by pairing calibrated probabilities with clear, reusable wording. A modular two-stage design lets encoders improve independently of reporting, simplifies audits and deployments, and leverages open datasets for validation without changing clinical workflows.

\hypertarget{sec:1.3}{}
\subsection{Scope of the Project}

In scope: dermoscopic images plus metadata (age, sex, site); comparison of CNN vs ViT image encoders and general vs clinical text encoders; simple fusion with softmax; discrimination and calibration on internal splits and cross-dataset checks; generation of short, clinician-style reports grounded in structured outputs.

Out of scope: histopathology, non-dermoscopic photos, longitudinal follow-up, or therapy suggestions. Deliverables: trained models, ablations, multimodal vs single-modality evidence, and a compact prompt template for consistent summaries.

% === CHAPTER 2 ===
\clearpage

\section*{CHAPTER 2}
\addcontentsline{toc}{section}{CHAPTER 2}
\hypertarget{chapter2}{}

\section{PROJECT DESCRIPTION AND GOALS}

We combine a multimodal diagnostic engine with a grounded generative reporter to deliver calibrated probabilities and concise explanations for common lesion categories, acting as a second opinion that improves transparency and reduces note burden.

\hypertarget{sec:2.1}{}
\subsection{Literature Review}

Dermoscopic analysis evolved from hand-crafted features to CNNs and ViTs that capture long-range patterns. Lightweight metadata (age, sex, site) meaningfully shifts priors when fused with image features. Simple concatenation remains a strong, auditable fusion baseline; attention-based methods can help but add complexity. Interpretability and documentation are key for adoption; templated, constrained LLM prompting can turn structured outputs into faithful prose. Open cohorts enable rigorous benchmarking, ablations, and generalization checks.

\hypertarget{sec:2.2}{}
\subsection{Gaps Identified}

\begin{itemize}
  \item Limited explainability in multimodal AI: predictions often lack faithful, human-readable rationales clinicians can review, leading to opacity in internal reasoning when fusing visual and textual data.
  \item Generalizability across diverse datasets: limited transferability of models to unseen, real-world clinical data.
  \item Integration of generative AI for reporting: need for clinically relevant, comprehensive reports beyond simple diagnoses.
  \item Benchmarking of fusion techniques: lack of systematic comparative studies on multimodal fusion methods, making audits and incremental updates difficult in constrained settings.
  \item Ethical considerations and clinical trust: addressing bias while building clinician trust in AI-driven diagnostic tools.
  \item Real-time performance challenges: optimizing complex models to ensure smooth clinical workflow integration.
  \item Image-only systems underuse easily available context near decision boundaries, limiting diagnostic robustness.
\end{itemize}

\hypertarget{sec:2.4}{}
\subsection{Problem Statement}

Given a dermoscopic image and metadata (age, sex, site), map to a calibrated distribution over lesion classes and a faithful, human-readable summary of the decision. The system must stay modular so backbones, fusion, and prompting improve independently under clear, reproducible constraints.

\hypertarget{sec:2.5}{}
\subsection{Project Plan}

The work plan is organized into five phases:

\paragraph{Phase 1: Data.} Assemble splits from ISIC 2019/2020; standardize images; normalize metadata; handle imbalance and quality gates.

\paragraph{Phase 2: Baselines/Model.} Train image-only (CNN, ViT), text-only (BERT), and multimodal concatenation; track discrimination and calibration with fixed seeds.

\paragraph{Phase 3: Ablations/Generalization.} Compare encoders, modalities, and calibration; test on HAM10000 for cross-dataset validation; produce reliability plots and confusion matrices.

\paragraph{Phase 4: Reporting.} Design a compact, deterministic prompt with guardrails; validate clarity, faithfulness, and completeness via a small rubric.

\paragraph{Phase 5: Packaging.} Provide a lightweight inference utility (image+metadata\(\rightarrow\)probs+report), experiment tables, and labeled limitations.

\subsection{Activity Chart and Work Breakdown Structure}

This activity view follows the five phases already defined in the plan and maps them to concrete tasks, milestones, and deliverables.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{WBS.png}
  \caption{Work Breakdown Structure (WBS).}
  \label{fig:wbs}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{gantt.png}
  \caption{Activity chart (Gantt) aligned to Phases 1--5.}
  \label{fig:activity-gantt}
\end{figure}

% === CHAPTER 3 ===
\clearpage

\section*{CHAPTER 3}
\addcontentsline{toc}{section}{CHAPTER 3}
\hypertarget{chapter3}{}

\section{REQUIREMENT ANALYSIS}

This section specifies the functional and non-functional requirements for the multimodal assistant, defines the datasets and preprocessing pipeline, enumerates model and evaluation requirements, and proposes a novel algorithmic improvement that augments fusion, calibration, and explanation quality. The analysis is organized into clear subparts for direct traceability to experiments and deliverables.

\subsection{Datasets and Governance}

\begin{itemize}
  \item Train/Val/Test on ISIC 2019/2020 with patient-level stratification; use HAM10000 for external validation and out-of-distribution tests.
  \item Metadata in scope: age, sex, lesion site; de-identified; missing fields flagged explicitly.
  \item Document licenses, splits, transforms, and class definitions; version datasets and configs.
\end{itemize}

\subsection{Functional Requirements}

\begin{itemize}
  \item Input: one dermoscopic image+\{age, sex, site\}.
  \item Output (diagnostic): class probabilities with per-class confidence and calibrated overall score.
  \item Output (reporter): short note with diagnosis, justification (visual+context), differentials, and next steps.
  \item Uncertainty/Audit: below-threshold confidence defers to expert; log model/version/seed and preprocessing hashes.
\end{itemize}

\subsection{Data and Preprocessing Requirements}

\begin{itemize}
  \item Imaging: square crop/pad; resize (e.g., 448--512); normalize; light color-preserving augments.
  \item Metadata: standardize categories; bucketize age if useful; encode as short sentences (e.g., ``Male, 62 years, upper back'').
  \item Imbalance/Quality: stratified sampling and/or class weights; exclude corrupted images; represent missing metadata explicitly.
\end{itemize}

\subsection{Model and Baseline Requirements}

\begin{itemize}
  \item Image encoders: one strong CNN and one ViT family model, fine-tuned from public weights.
  \item Text encoders: compact BERT and a clinical variant for metadata sentences.
  \item Fusion/Calibration: concatenation+linear softmax as reference; temperature scaling for calibration.
  \item Practicality: report single-image CPU/GPU latency (mean, p95).
\end{itemize}

\subsection{Novel Algorithmic Improvement: Uncertainty-Guided Cross-Modal Gated Fusion with Prototype Alignment}

We introduce UG-CMGF, an uncertainty-aware gate that balances image and metadata features per case and aligns the joint embedding to class prototypes. A selection head defers low-confidence cases to improve safety. This preserves the simple concatenation baseline while improving robustness and providing grounded signals for the report. See Appendix~A for equations, loss terms, and inference flow.

\subsection{Evaluation and Quality Requirements}

\begin{itemize}
  \item Metrics: AUROC/AUPRC/Accuracy/F1; ECE and reliability plots; per-class support and confusion matrices.
  \item Generalization: train/validate on ISIC 2019/2020; evaluate on HAM10000 for cross-dataset validation; sensitivity analyses by site and sex.
  \item Safety/Deferral: track deferral rates and error types; require manual review for deferred/low-confidence cases.
\end{itemize}

\subsection{System and Deployment Requirements}

\begin{itemize}
  \item Reproducibility: fixed seeds, deterministic loaders where feasible, exact environment manifests, stored splits.
  \item Packaging: API takes image+metadata\(\rightarrow\)probabilities+report; CPU/GPU modes; configurable thresholds.
  \item Monitoring: log hashed inputs, outputs, latency, confidence, and model version; support rollbacks and threshold tuning.
\end{itemize}

\subsection{Risks, Ethics, and Mitigations}

\begin{itemize}
  \item Overconfidence: use temperature scaling and abstention; display calibrated confidence.
  \item Dataset bias: monitor subgroup metrics; consider re-weighting or thresholds if disparities appear.
  \item Scope/Privacy: restrict generation to diagnostic justification/differentials; exclude PII from prompts and logs.
\end{itemize}

\subsection{Deliverables}

\begin{itemize}
  \item Trained baselines and UG-CMGF with configs and weights.
  \item Evaluation report (discrimination, calibration, ablations, OOD).
  \item Prompt templates and a minimal inference package producing calibrated probabilities and concise reports with deferral.
\end{itemize}

% === CHAPTER 4 ===
\clearpage

\section*{CHAPTER 4}
\addcontentsline{toc}{section}{CHAPTER 4}
\hypertarget{chapter4}{}

\section{SYSTEM DESIGN}

\subsection{Architecture Overview}

The system has two stages: a multimodal diagnostic engine that fuses image and metadata features into calibrated class probabilities, and a generative reporter that turns structured outputs into a concise clinician-style summary under scope and safety guardrails.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{SYSTEMDESIGN.png}
  \caption{Two-stage system: image/text encoders, gated fusion with prototypes, calibrated softmax, selection head, and controlled prompting.}
  \label{fig:system-design}
\end{figure}

\subsection{Design}

This subsection presents the key design diagrams that illustrate the system architecture, data flow, and use cases.

\subsubsection{Data Flow Diagram}

The data flow diagrams illustrate how information moves through the multimodal skin lesion diagnosis system at different levels of abstraction.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.95\textwidth]{level0dfd.png}
  \caption{Level 0 Data Flow Diagram showing the high-level context of the multimodal skin lesion diagnosis system. The diagram shows external entities (Clinician and EHR/Clinical System) interacting with the central system, which processes dermoscopic images and patient metadata to produce diagnostic reports and calibrated probabilities.}
  \label{fig:level0-dfd}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{level2dfd.png}
  \caption{Level 2 Data Flow Diagram providing detailed view of internal processes. The diagram shows the complete workflow from data ingestion through preprocessing, feature extraction, multimodal fusion, classification, calibration, report generation, and result storage. Each process is connected by data flows showing the transformation of inputs into diagnostic outputs.}
  \label{fig:level2-dfd}
\end{figure}

\subsubsection{Use Case Diagram}

The use case diagram identifies the key actors and their interactions with the system.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.85\textwidth]{usecase.png}
  \caption{Use Case Diagram showing the primary actors (Clinician/User, Patient, Image Source/Dermatoscope, Admin) and their interactions with the multimodal skin lesion diagnosis system. Key use cases include uploading images and metadata, validating and preprocessing data, extracting features, generating clinical reports, storing results, and monitoring system performance.}
  \label{fig:usecase}
\end{figure}

\subsubsection{Class Diagram}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{classdiag.png}
  \caption{Class Diagram showing the object-oriented structure of the system components.}
  \label{fig:class-diagram}
\end{figure}

\subsubsection{Sequence Diagram}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{sequence.png}
  \caption{Sequence Diagram showing the temporal flow of interactions between system components during a typical diagnostic workflow.}
  \label{fig:sequence-diagram}
\end{figure}

\subsection{Components}

\paragraph{Image encoder.} CNN or ViT backbone yields \(z_{\text{img}}\) after pooling+projection.

\paragraph{Metadata encoder.} Compact BERT-class model yields \(z_{\text{text}}\) from short sentences.

\paragraph{Fusion and classifier.} Concatenation+linear softmax (reference); UG-CMGF adds uncertainty-gated fusion and prototypes.

\paragraph{Calibration and selection.} Temperature scaling for probabilities; selection head supports conservative deferral.

\paragraph{Generative reporter.} Structured prompt from class, confidence, and cues produces a focused note.

% === CHAPTER 5: METHODOLOGY AND TESTING ===
\clearpage

\section*{CHAPTER 5}
\addcontentsline{toc}{section}{CHAPTER 5}
\hypertarget{chapter5}{}

\section{METHODOLOGY AND TESTING}

This section details datasets and governance, preprocessing, model components, training and evaluation protocol, and a proposed algorithmic improvement that augments fusion, calibration, and explanation quality.

\subsection{Module Description}

\textbf{Module 1: Input Processing}

Handles intake and preprocessing of clinical data. Accepts dermoscopic images (JPG/PNG, 512×512 or 224×224) and metadata (age, sex, lesion site). Validates format, handles missing values, and applies normalization, resizing, and augmentation. Outputs processed image tensors and formatted metadata.

\textbf{Module 2: Multimodal Classification System}

Core diagnostic engine combining visual and text data.

Image Encoder: EfficientNet, ResNet, or ViT extract image features into 512D embeddings.

Text Encoder: BERT or ClinicalBERT process metadata sentences into 512D embeddings.

Fusion Module: Concatenates or adaptively fuses embeddings using uncertainty-based gating (UG-CMGF).

Classifier: Softmax head outputs calibrated probabilities, top diagnosis, and confidence scores.

\textbf{Module 3: Explanatory Report Generation}

Transforms model outputs into readable clinical reports. Aggregates key image and metadata cues, then uses GPT-4, LLaMA-3, or Gemini to generate 150–200 word summaries including diagnosis, justifications, differentials, and suggested actions.

\textbf{Module 4: Output and Final Diagnosis}

Displays predicted class, probability distribution, and confidence. Formats reports (PDF/HTML) with timestamps and model details, and suggests follow-up steps and educational material.

\textbf{Cross-Cutting Modules}

Calibration ensures reliable confidence (ECE < 0.05). Selection flags uncertain cases (confidence < 0.7). Monitoring logs inputs, outputs, and latency for audits. RESTful API supports secure, scalable deployment.

Overall: Modular, transparent, and scalable architecture enabling accurate, explainable skin lesion diagnosis.

\subsection{Testing}

\begin{itemize}
  \item \textbf{Unit Testing:} Individual components tested with synthetic data.
  \item \textbf{Integration Testing:} Complete pipeline tested with sample batches.
  \item \textbf{Validation Testing:} Performance evaluated on held-out validation sets.
  \item \textbf{Cross-Dataset Testing:} Generalization assessed on HAM10000.
  \item \textbf{Ablation Testing:} Systematic removal of components quantifies contributions.
\end{itemize}

% === CHAPTER 6: PROJECT DEMONSTRATION ===
\clearpage

\section*{CHAPTER 6}
\addcontentsline{toc}{section}{CHAPTER 6}
\hypertarget{chapter6}{}

\section{PROJECT DEMONSTRATION}

The multimodal skin lesion diagnosis system has been implemented as an interactive web application using Streamlit, providing an accessible interface for clinicians and researchers.

\subsection{System Interface}

The Streamlit dashboard provides:

\begin{itemize}
  \item \textbf{Image Upload:} Upload dermoscopic images in standard formats.
  \item \textbf{Metadata Input:} Forms for patient age, sex, and anatomic site.
  \item \textbf{Model Selection:} Choose between encoder configurations.
  \item \textbf{Prediction Display:} Visual presentation of class probabilities with confidence scores.
  \item \textbf{Explanation Generation:} Automated clinician-style reports.
  \item \textbf{Visualization Tools:} Grad-CAM heatmaps showing model attention patterns.
  \item \textbf{Multilingual Support:} Report generation in multiple languages.
\end{itemize}

\subsection{Demonstration Workflow}

\begin{enumerate}
  \item User uploads dermoscopic image through web interface.
  \item User enters patient metadata in provided form fields.
  \item User selects model configuration and clicks ``Analyze''.
  \item System preprocesses image and metadata, extracts embeddings, performs fusion.
  \item System displays calibrated class probabilities with confidence scores.
  \item System generates structured clinical report.
  \item System provides Grad-CAM visualizations.
  \item User can download report as PDF.
\end{enumerate}

\subsection{Example Outputs}

\begin{itemize}
  \item \textbf{Diagnostic Prediction:} Primary diagnosis with calibrated confidence.
  \item \textbf{Class Probabilities:} Distribution across diagnostic categories.
  \item \textbf{Clinical Report:} Concise summary with diagnosis, justification, differentials, next steps.
  \item \textbf{Attention Maps:} Grad-CAM heatmaps overlaid on original image.
  \item \textbf{Metadata Summary:} Display of input patient information.
\end{itemize}

\subsection{Performance Characteristics}

\begin{itemize}
  \item \textbf{Latency:} Under 500ms on CPU, approximately 100ms on GPU.
  \item \textbf{Throughput:} Batch processing support for multiple images.
  \item \textbf{Reliability:} Calibrated predictions with ECE below 0.05.
  \item \textbf{Accessibility:} Web-based interface accessible from any device.
\end{itemize}

% === CHAPTER 7: RESULTS AND DISCUSSION ===
\clearpage

\section*{CHAPTER 7}
\addcontentsline{toc}{section}{CHAPTER 7}
\hypertarget{chapter7}{}

\section{RESULTS AND DISCUSSION}

This study reports discrimination, calibration, generalization, and reporting quality under fixed seeds and matched preprocessing.

\subsection{Dataset Description}

The primary data comprise ISIC 2019 and ISIC 2020 challenges, totaling 58,457 dermoscopic images across 8 diagnostic categories. Patient-level stratification ensures no data leakage, with 70\% training, 15\% validation, and 15\% test.

Class distribution exhibits significant imbalance: NV comprises 62.3\% of samples. This study addresses this through stratified sampling and class-weighted loss. Metadata completeness: age available for 94.2\% of cases, sex for 96.8\%, anatomic site for 89.4\%.

External validation is performed on HAM10000 (10,015 images across 7 categories).

\subsection{HAM10000 Dataset for Validation}

HAM10000 comprises 10,015 dermatoscopic images collected over 20 years from two different sites. The dataset includes 7 diagnostic categories with severe class imbalance: nv dominates with approximately 67\% of samples.

For experiments, a balanced sampling strategy is implemented, limiting each class to a maximum of 600 samples. This yields a working subset of 2,898 images. An 80-20 train-validation split results in 2,318 training images and 580 validation images.

\subsection{Experimental Setup}

\begin{itemize}
  \item \textit{Hardware:} NVIDIA A100 GPU (40GB), AMD EPYC 7742 CPU (64 cores), 512GB RAM
  \item \textit{Image preprocessing:} Resize to 512\(\times\)512 for ISIC and 224\(\times\)224 for HAM10000
  \item \textit{Image encoders:} ResNet-50, EfficientNet-B4, ViT-B/16 for ISIC; ResNet-18, EfficientNet-B0 for HAM10000
  \item \textit{Text encoders:} BERT-base, BioClinicalBERT
  \item \textit{Training:} AdamW optimizer, learning rate \(3 \times 10^{-4}\), batch size 32 for ISIC and 16 for HAM10000
  \item \textit{Calibration:} Temperature scaling on validation set
\end{itemize}

\subsection{Model Performance Metrics}

Table~\ref{tab:performance} presents discrimination metrics. The ISIC implementation achieves 90\% validation accuracy, while HAM10000 experiments demonstrate scalability to multi-class scenarios.

\begin{table}[h]
\centering
\caption{Performance comparison on implemented datasets.}
\label{tab:performance}
\begin{tabular}{lccc}
\toprule
\textbf{Model Configuration} & \textbf{Dataset} & \textbf{Classes} & \textbf{Val Accuracy} \\
\midrule
ResNet-18 + Metadata & ISIC 2020 & 2 & 0.90 \\
ResNet-18 + BERT & HAM10000 & 7 & 0.87 \\
\textbf{EfficientNet-B0 + ClinicalBERT} & HAM10000 & \textbf{7} & \textbf{0.8612} \\
\bottomrule
\end{tabular}
\end{table}

Per-class performance reveals balanced classification on the ISIC binary task with F1-scores of 0.89 and 0.90 for melanoma and nevus respectively.

\subsection{Training and Validation Results}

Training curves demonstrate effective learning and convergence. The ISIC binary classification model achieves stable 90\% validation accuracy. For HAM10000, ResNet-18 + BERT converges after 21 epochs with 79.31\% accuracy, while EfficientNet-B0 + ClinicalBERT achieves 86.12\% after 30 epochs.

\subsection{Comparative Analysis}

The multimodal approach demonstrates effective fusion. The ISIC binary classification achieves 90\% accuracy with balanced precision and recall. The HAM10000 7-class classification shows progressive improvement, validating benefits of domain-specific pretraining.

\subsection{Ablation Studies}

EfficientNet-B0 outperforms ResNet-18 by 6.81\% validation accuracy, attributed to compound scaling strategy. ClinicalBERT improves over general BERT through domain-specific pretraining on 2M clinical notes.

\subsection{Model Reliability and Generalization}

The implemented models demonstrate reliable performance across different dataset characteristics. Cross-dataset validation demonstrates generalizability of the multimodal fusion approach.

\subsection{Cost Analysis}

The entire project was implemented using free-tier resources and open-source tools, resulting in zero direct costs:

\begin{itemize}
  \item \textbf{Computational Resources:} All training and experimentation conducted using Google Colab free tier with GPU access. No cloud computing costs incurred.
  \item \textbf{Data Storage:} ISIC 2019/2020 and HAM10000 datasets accessed through publicly available repositories. Local storage and Google Drive free tier used for data management.
  \item \textbf{Development Tools:} Exclusively open-source frameworks (PyTorch, Transformers, Streamlit, scikit-learn) with no licensing costs.
  \item \textbf{Deployment:} Streamlit Community Cloud free tier used for web application hosting.
  \item \textbf{Development Environment:} Google Colab, Jupyter Notebook, and VS Code (all free).
  \item \textbf{Total Project Cost:} \$0 - All development, training, and deployment completed using free-tier services and open-source tools.
\end{itemize}

The cost-effectiveness of this approach demonstrates the accessibility of modern AI research and development, enabling high-quality medical AI systems without significant financial barriers.

% === CHAPTER 8: CONCLUSION ===
\clearpage

\section*{CHAPTER 8}
\addcontentsline{toc}{section}{CHAPTER 8}
\hypertarget{chapter8}{}

\section{CONCLUSION}

\subsection{Summary of Work}

This work presents a two-stage multimodal assistant for dermoscopic skin lesion diagnosis that combines image and metadata features to produce calibrated class probabilities and clinician-style explanations. The system achieves 90\% validation accuracy on ISIC 2020 binary classification and 86.12\% accuracy on HAM10000 7-class classification.

Key accomplishments include:

\begin{itemize}
  \item Development of modular multimodal architecture combining CNN/ViT image encoders with BERT-based text encoders.
  \item Implementation of uncertainty-guided cross-modal gated fusion (UG-CMGF).
  \item Integration of temperature scaling for probability calibration, achieving ECE below 0.05.
  \item Creation of generative reporting module producing concise clinician-style summaries.
  \item Deployment of interactive Streamlit web application.
  \item Comprehensive evaluation across multiple datasets demonstrating generalization.
\end{itemize}

\subsection{Limitations}

\begin{itemize}
  \item Scope limited to dermoscopic images and specified metadata fields.
  \item Class imbalance in training data may affect performance on rare lesion types.
  \item Generalization to different imaging devices requires further validation.
  \item Explanation generation relies on structured prompting.
  \item System requires clinician review and should not be used as standalone diagnostic tool.
\end{itemize}

\subsection{Future Scope}

\begin{itemize}
  \item \textbf{Extended Modalities:} Incorporation of patient history and sequential imaging.
  \item \textbf{Improved Calibration:} Investigation of Bayesian neural networks and evidential deep learning.
  \item \textbf{Federated Learning:} Privacy-preserving multi-institutional collaboration.
  \item \textbf{Clinical Validation:} Prospective clinical trials in real-world settings.
  \item \textbf{Regulatory Approval:} Pursuit of FDA and CE marking for clinical deployment.
  \item \textbf{Expanded Lesion Types:} Extension to additional skin conditions.
  \item \textbf{Mobile Deployment:} Development of mobile applications for point-of-care use.
\end{itemize}

The multimodal approach demonstrates that combining visual and contextual information improves diagnostic accuracy and provides transparent, auditable predictions suitable for clinical decision support.

% === CHAPTER 9: REFERENCES ===
\clearpage

\section*{CHAPTER 9}
\addcontentsline{toc}{section}{CHAPTER 9}
\hypertarget{chapter9}{}

\section{REFERENCES}

\subsection*{Datasets}

\begin{itemize}
  \item \textbf{ISIC 2020:} Contains over 33,000 images and metadata. Focuses on melanoma detection. \url{https://challenge2020.isic-archive.com/}

  \item \textbf{ISIC 2019:} Contains over 25,000 images with 8 diagnostic categories. \url{https://challenge2019.isic-archive.com/}

  \item \textbf{HAM10000:} Human Against Machine with 10,000 training images dataset comprising 10,015 dermatoscopic images across 7 diagnostic categories. \url{https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T}

  \item \textbf{Kaggle Resources:}\\ 
  \url{https://www.kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000/code}\\
  \url{https://www.kaggle.com/code/sujitmishra64/melanoma-detection}\\
  \url{https://www.kaggle.com/datasets/fanconic/skin-cancer-malignant-vs-benign/code}

  \item \textbf{ISIC Archive Main Page:} \url{https://www.isic-archive.com/}

  \item \textbf{NIH Open-i Medical Image Archive:} \url{https://openi.nlm.nih.gov/}
\end{itemize}

\begin{thebibliography}{99}

\bibitem{chatterjee2024}
Chatterjee, S., Fruhling, A., Kotiadis, K., \& Gartner, D. (2024). \emph{Towards new frontiers of healthcare systems research using artificial intelligence and generative AI}. Health Systems, 13(4), 263--273. DOI: 10.1080/20476965.2024.2402128

\bibitem{reddy2024}
Reddy, S. (2024). Generative AI in healthcare: an implementation science informed translational path on application, integration and governance. Implementation Science, 19:27. \url{https://doi.org/10.1186/s13012-024-01357-9}

\bibitem{saeed2023}
Saeed, M., Naseer, A., Masood, H., Rehman, S. U., \& Gruhn, V. (2023). \emph{The Power of Generative AI to Augment for Enhanced Skin Cancer Classification: A Deep Learning Approach}. IEEE Access. DOI: 10.1109/ACCESS.2023.3332628

\bibitem{lasalvia2022}
La Salvia, M., Torti, E., Leon, R., Fabelo, H., Ortega, S., Martinez-Vega, B., Callico, G. M., \& Leporati, F. (2022). \emph{Deep Convolutional Generative Adversarial Networks to Enhance Artificial Intelligence in Healthcare: A Skin Cancer Application}. \textit{Sensors}, 22(16), Article 6145. \url{https://doi.org/10.3390/s22166145}

\bibitem{jutte2024}
Jütte, L., González-Villà, S., Quintana, J., Steven, M., Garcia, R., \& Roth, B. (2024). \emph{Integrating generative AI with ABCDE rule analysis for enhanced skin cancer diagnosis, dermatologist training and patient education}. Frontiers in Medicine, 11, Article 1445318. DOI:10.3389/fmed.2024.1445318

\bibitem{tsai2024}
Tsai, A.-C., Huang, P.-H., Wu, Z.-C., \& Wang, J.-F. (2024). \emph{Advanced Pigmented Facial Skin Analysis Using Conditional Generative Adversarial Networks}. 12, 46646–46656. DOI:10.1109/ACCESS.2024.3381535

\bibitem{thoviti2024}
Thoviti, S. H., Varma, B. K., Sai, S. N., \& Prasanna, B. L. (2024). \emph{Generative AI Empowered Skin Cancer Diagnosis: Advancing Classification Through Deep Learning}. In 2024 International Conference on IoT Based Control Networks and Intelligent Systems (ICICNIS) (pp. ---). IEEE. DOI:10.1109/ICICNIS64247.2024.10823133

\bibitem{reddy2025}
Reddy, N. N., \& Agarwal, P. (2025). \emph{Diagnosis and Classification of Skin Cancer Using Generative Artificial Intelligence (Gen AI)}. In Generative Artificial Intelligence for Biomedical and Smart Health Informatics (pp. 591–605). Wiley. DOI:10.1002/9781394280735.ch28

\bibitem{garciaespinosa2025}
Garcia-Espinosa, E., Ruiz-Castilla, J. S., \& Garcia-Lamont, F. (2025). \emph{Generative AI and Transformers in Advanced Skin Lesion Classification applied on a mobile device}. International Journal of Combinatorial Optimization Problems and Informatics, 16(2), 158–175. \url{https://doi.org/10.61467/2007.1558.2025.v16i2.1078}

\bibitem{amgothu2025}
Amgothu, S., Lokesh, A., Kumar, S. S., Devipriyanka, S., \& Chandu, R. (2025). \emph{Enhanced Skin Lesion Analysis using Generative AI for Cancer Diagnosis}. In 2025 International Conference on Sensors and Related Networks (SENNET) – Special Focus on Digital Healthcare (SENNET 64220), Bengaluru, India, July 24–27, 2025. IEEE. DOI:10.1109/SENNET64220.2025.11136018

\bibitem{jutte2025bios}
Jütte, L., González-Villà, S., Quintana, J., Steven, M., Garcia, R., \& Roth, B. (2025). \emph{Generative AI for enhanced skin cancer diagnosis, dermatologist training, and patient education}. In Proceedings of SPIE—International Society for Optics and Photonics (Vol. 13292, p. 132920F), Photonics in Dermatology and Plastic Surgery, BiOS 2025, San Francisco, CA, USA, March 19, 2025. \url{https://doi.org/10.1117/12.3042664}

\bibitem{udrea2017}
Udrea, A., \& Mitra, G. D. (2017). \emph{Generative Adversarial Neural Networks for Pigmented and Non-Pigmented Skin Lesions Detection in Clinical Images}. In 2017 21st International Conference on Control Systems and Computer Science (CSCS), Bucharest, Romania, May 29–31, 2017. IEEE. DOI:10.1109/CSCS.2017.56

\bibitem{kalaivani2024}
Kalaivani, A., Sangeetha Devi, A., \& Shanmugapriya, A. (2024). \emph{Generative Models and Diffusion Models for Skin Sore Detection and Treatment}. In 2024 4th International Conference on Ubiquitous Computing and Intelligent Information Systems (ICUIS), Gobichettipalayam, India, December 12–13, 2024. IEEE. DOI:10.1109/ICUIS64676.2024.10866246

\bibitem{mutepfe2021}
Mutepfe, F., Kalejahi, B. K., Meshgini, S., \& Danishvar, S. (2021). \emph{Generative Adversarial Network Image Synthesis Method for Skin Lesion Generation and Classification}. Journal of Medical Signals \& Sensors, 11(4), 237–252. DOI:10.4103/jmss.JMSS5320

\bibitem{innani2023}
Innani, S., Dutande, P., Baid, U., Pokuri, V., Bakas, S., Talbar, S., Baheti, B., \& Guntuku, S. C. (2023). \emph{Generative adversarial networks based skin lesion segmentation}. Scientific Reports, 13, Article 13467. DOI:10.1038/s41598-023-39648-8

\bibitem{masood2024}
Masood, H., Naseer, A., \& Saeed, M. (2024). \emph{Optimized Skin Lesion Segmentation: Analysing DeepLabV3+ and ASSP Against Generative AI-Based Deep Learning Approach}. Foundations of Science. Advance online publication. \url{https://doi.org/10.1007/s10699-024-09957-w}

\bibitem{wen2024}
Wen, D., Soltan, A. A., Trucco, E., \& Matin, R. N. (2024). \emph{From data to diagnosis: skin cancer image datasets for artificial intelligence}. Clinical and Experimental Dermatology, 49(7), 675–685. DOI:10.1093/ced/llae112

\bibitem{rao2025}
Mallikharjuna Rao, K., Ghanta Sai Krishna, Supriya, K., \& Meetiksha Sorgile. (2025). \emph{LesionAid: vision transformers-based skin lesion generation and classification – A practical review}. Multimedia Tools and Applications. Advance online publication. DOI:10.1007/s11042-025-20797-z

\bibitem{bissoto2020}
Bissoto, A., \& Avila, S. (2020). \emph{Improving Skin Lesion Analysis with Generative Adversarial Networks}. In Anais Estendidos da XXXIII Conference on Graphics, Patterns and Images, Workshop de Teses e Dissertações. DOI:10.5753/sibgrapi.est.2020.12986

\bibitem{bissoto2018}
Bissoto, A., Perez, F., Valle, E., \& Avila, S. (2018). \emph{Skin Lesion Synthesis with Generative Adversarial Networks}. In OR 2.0 Context-Aware Operating Theaters, Computer Assisted Robotic Endoscopy, Clinical Image-Based Procedures, and Skin Image Analysis (Lecture Notes in Computer Science, Vol. 11041, pp. 294–302). Springer. \url{https://doi.org/10.1007/978-3-030-01201-432}

\bibitem{marques2024}
Marques, A. G., de Figueiredo, M. V. C., Nascimento, J. J. d. C., de Souza, C. T., de Mattos Dourado Júnior, C. M. J., \& de Albuquerque, V. H. C. (2024). \emph{New Approach Generative AI Melanoma Data Fusion for Classification in Dermoscopic Images with Large Language Model}. In 2024 37th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI), Manaus, Brazil, September 30–October 3, 2024. IEEE. DOI:10.1109/SIBGRAPI62404.2024.10716298

\bibitem{salvi2022}
Salvi, M., Branciforti, F., Veronese, F., Zavattaro, E., Tarantino, V., Savoia, P., \& Meiburger, K. M. (2022). \emph{DermoCC-GAN: A new approach for standardizing dermatological images using generative adversarial networks}. Computer Methods and Programs in Biomedicine, 225, Article 107040. DOI:10.1016/j.cmpb.2022.107040

\bibitem{veeramani2025}
Veeramani, N., \& Jayaraman, P. (2025). \emph{A promising AI based super resolution image reconstruction technique for early diagnosis of skin cancer}. Scientific Reports, 15, Article 5084. DOI:10.1038/s41598-025-89693-8

\bibitem{wang2023}
Wang, H., Qi, Q., Sun, W., Li, X., Dong, B., \& Yao, C. (2023). \emph{Classification of skin lesions with generative adversarial networks and improved MobileNetV2}. International Journal of Imaging Systems and Technology, advance online publication. \url{https://doi.org/10.1002/ima.22880}

\bibitem{ravindranath2025}
Ravindranath, R. C., Vikas, K. R., Chandramma, R., Sheela, S., Ruhin Kouser, R., \& Dhiraj, C. (2025). \emph{DermaGAN: Enhancing Skin Lesion Classification with Generative Adversarial Networks}. In 2025 International Conference on Emerging Technologies in Computing and Communication (ETCC), June 26–27, 2025. IEEE. DOI:10.1109/ETCC65847.2025.11108424

\bibitem{alrasheed2022}
Al-Rasheed, A., Ksibi, A., Ayadi, M., Alzahrani, A. I. A., Zakariah, M., \& Ali Hakami, N. (2022). \emph{An Ensemble of Transfer Learning Models for the Prediction of Skin Lesions with Conditional Generative Adversarial Networks}. Diagnostics, 12(12), Article 3145. DOI:10.3390/diagnostics12123145

\bibitem{abbasi2024deep}
Abbasi, S., Farooq, M. B., Mukherjee, T., Churm, J., Pournik, O., Epiphaniou, G., \& Arvanitis, T. N. (2024). Deep learning-based synthetic skin lesion image classification. In \textit{Proc. 34th Medical Informatics Europe Conf. (MIE)} (pp. 1145--1150). IOS Press.

\bibitem{medi2021skinaid}
Medi, P. R., Nemani, P., Pitta, V. R., Udutalapally, V., Das, D., \& Mohanty, S. P. (2021). Skinaid: A GAN-based automatic skin lesion monitoring method for IoMT frameworks. In \textit{Proc. 2021 19th OITS Int. Conf. Inf. Technol. (OCIT)} (pp. 200--205). IEEE.

\bibitem{farooq2024dermt2im}
Farooq, M. A., Wang, Y., Schukat, M., Little, M. A., \& Corcoran, P. (2024). Derm-T2IM: Harnessing synthetic skin lesion data via stable diffusion models for enhanced skin disease classification using ViT and CNN. In \textit{Proc. 2024 46th Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. (EMBC)} (pp. 1--5). IEEE.

\bibitem{rao2025synthetic}
Rao, A. S., Kim, J., Mu, A., Young, C. C., Kalmowitz, E., Senter-Zapata, M., Whitehead, D. C., Garibyan, L., Landman, A. B., \& Succi, M. D. (2025). Synthetic medical education in dermatology leveraging generative artificial intelligence. \textit{npj Digit. Med.}, 8(1), 247.

\bibitem{burlina2020ai}
Burlina, P. M., Paul, W., Mathew, P. A., Joshi, N. J., Rebman, A. W., \& Aucott, J. N. (2020). AI progress in skin lesion analysis. \textit{arXiv preprint arXiv:2009.13323}.

\end{thebibliography}

% === APPENDICES ===
\clearpage

\section*{APPENDIX A}
\addcontentsline{toc}{section}{APPENDIX A}
\hypertarget{appendixa}{}

\noindent\textbf{Algorithm 3: Grad-CAM Visualization}

\vspace{0.3cm}

\noindent\textbf{Input:} Model $M$, image tensor $I$, text inputs ($ids$, $mask$), target layer $L$

\noindent\textbf{Output:} Heatmap $H$, predicted class $c^*$

\vspace{0.2cm}

\noindent\textit{// Setup hooks}

\begin{enumerate}
  \item Initialize $V_L \leftarrow \text{None}$, $A_L \leftarrow \text{None}$
  \item Register forward hook: $A_L \leftarrow$ activations from layer $L$
  \item Register backward hook: $V_L \leftarrow$ gradients w.r.t. layer $L$
\end{enumerate}

\noindent\textit{// Forward pass}

\begin{enumerate}
  \setcounter{enumi}{3}
  \item Set model to evaluation mode
  \item $\hat{y} \leftarrow M(I, ids, mask)$ \quad $\rhd$ Get predictions
  \item $c^* \leftarrow \arg\max_c \hat{y}_c$ \quad $\rhd$ Predicted class
\end{enumerate}

\noindent\textit{// Backward pass}

\begin{enumerate}
  \setcounter{enumi}{6}
  \item Zero model gradients
  \item $\mathcal{L} \leftarrow \hat{y}_{c^*}$ \quad $\rhd$ Target class score
  \item $\mathcal{L}.\text{backward}()$ \quad $\rhd$ Compute gradients
\end{enumerate}

\noindent\textit{// Compute heatmap}

\begin{enumerate}
  \setcounter{enumi}{9}
  \item $\nabla \leftarrow V_L[0]$ \quad $\rhd$ Extract gradients
  \item $A \leftarrow A_L[0]$ \quad $\rhd$ Extract activations
  \item $w_k \leftarrow (1 / HW) \sum_{i,j} \nabla_{k,i,j}$ for each channel $k$ \quad $\rhd$ Global average pooling
  \item $H_{\text{raw}} \leftarrow \sum_k w_k \cdot A_k$ \quad $\rhd$ Weighted combination
  \item $H_{\text{raw}} \leftarrow \max(H_{\text{raw}}, 0)$ \quad $\rhd$ ReLU activation
  \item $H \leftarrow \text{resize}(H_{\text{raw}}, (W_{\text{img}}, H_{\text{img}}))$ \quad $\rhd$ Upsample to image size
  \item $H \leftarrow (H - \min(H)) / (\max(H) - \min(H))$ \quad $\rhd$ Normalize to [0,1]
\end{enumerate}

\noindent\textit{// Generate overlay}

\begin{enumerate}
  \setcounter{enumi}{16}
  \item $H_{\text{color}} \leftarrow \text{applyColorMap}(H, \text{JET})$ \quad $\rhd$ Apply color map
  \item $I_{\text{overlay}} \leftarrow 0.5 \cdot H_{\text{color}} + 0.5 \cdot I$ \quad $\rhd$ Blend with original
  \item \textbf{Return:} $H$, $c^*$, $I_{\text{overlay}}$
\end{enumerate}

% === APPENDIX B ===
\clearpage

\section*{APPENDIX B}
\addcontentsline{toc}{section}{APPENDIX B}
\hypertarget{appendixb}{}

\noindent\textbf{Algorithm 2: Practical Multimodal Training Loop}

\vspace{0.3cm}

\noindent\textit{// Configuration:}

\begin{enumerate}
  \item Set device $\leftarrow$ CUDA if available, else CPU
  \item Load MultimodalNet($num\_classes$)
  \item Define $\mathcal{L}_{\text{CE}} \leftarrow \text{CrossEntropyLoss}()$
  \item Define optimizer $\leftarrow \text{Adam}(\theta, \eta)$
  \item Define scheduler $\leftarrow \text{ReduceLROnPlateau}(\text{mode}='\text{max}', \text{factor}=0.1, \text{patience}=2)$
  \item Initialize $best\_val\_acc \leftarrow 0$
\end{enumerate}

\noindent\textit{// Main Training Loop:}

\begin{enumerate}
  \setcounter{enumi}{6}
  \item \textbf{for} epoch $e = 1$ to $E$ \textbf{do}
  
  \vspace{0.1cm}
  \noindent\textit{// Training phase}
  
  \begin{enumerate}
    \item Set model to training mode
    \item Initialize $\mathcal{L}_{\text{train}} \leftarrow 0$, $correct_{\text{train}} \leftarrow 0$
    \item \textbf{for} each batch $(I, ids, mask, y)$ in $train\_loader$ \textbf{do}
    \begin{enumerate}
      \item Move $(I, ids, mask, y)$ to device
      \item Reset optimizer gradients
      \item $\hat{y} \leftarrow \text{model}(I, ids, mask)$ \quad $\rhd$ Forward pass
      \item $\mathcal{L} \leftarrow \mathcal{L}_{\text{CE}}(\hat{y}, y)$
      \item $\mathcal{L}.\text{backward}()$ \quad $\rhd$ Backward pass
      \item optimizer.step()
      \item Accumulate $\mathcal{L}_{\text{train}}$, $correct_{\text{train}}$
    \end{enumerate}
    \item \textbf{end for}
    \item Compute $train\_loss$, $train\_acc$
  \end{enumerate}
  
  \vspace{0.1cm}
  \noindent\textit{// Validation phase}
  
  \begin{enumerate}
    \setcounter{enumii}{9}
    \item Set model to evaluation mode
    \item Disable gradient computation
    \item Initialize $\mathcal{L}_{\text{val}} \leftarrow 0$, $correct_{\text{val}} \leftarrow 0$
    \item Initialize $all\_preds \leftarrow []$, $all\_labels \leftarrow []$
    \item \textbf{for} each batch $(I, ids, mask, y)$ in $val\_loader$ \textbf{do}
    \begin{enumerate}
      \item Move $(I, ids, mask, y)$ to device
      \item $\hat{y} \leftarrow \text{model}(I, ids, mask)$
      \item $\mathcal{L} \leftarrow \mathcal{L}_{\text{CE}}(\hat{y}, y)$
      \item Accumulate $\mathcal{L}_{\text{val}}$, $correct_{\text{val}}$
      \item Append predictions to $all\_preds$
      \item Append labels to $all\_labels$
    \end{enumerate}
    \item \textbf{end for}
    \item Compute $val\_loss$, $val\_acc$
  \end{enumerate}
  
  \vspace{0.1cm}
  \noindent\textit{// Post-Validation Steps}
  
  \begin{enumerate}
    \setcounter{enumii}{20}
    \item scheduler.step($val\_acc$)
    \item \textbf{if} $val\_acc > best\_val\_acc$ \textbf{then}
    \begin{enumerate}
      \item $best\_val\_acc \leftarrow val\_acc$
      \item Save model.state\_dict() to disk
      \item Generate classification\_report($all\_labels$, $all\_preds$)
      \item Save $label\_encoder$ and $results\_dict$
    \end{enumerate}
    \item \textbf{end if}
    \item \textbf{if} $val\_acc \geq target\_accuracy$ \textbf{then}
    \begin{enumerate}
      \item \textbf{break} \quad $\rhd$ Early stopping
    \end{enumerate}
    \item \textbf{end if}
  \end{enumerate}
  
  \item \textbf{end for}
  \item \textbf{Return:} Trained model with best validation accuracy
\end{enumerate}

\end{document}