% !TeX program = lualatex
% Elsevier CAS double-column layout (A4) with side rails
\documentclass[a4paper,fleqn]{cas-dc}

% Citation style
\usepackage[numbers,sort&compress]{natbib}

% Core packages
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{xurl}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{microtype}
\usepackage{soul}
\sodef\spaced{}{.2em}{.6em plus.1em}{1em plus.1em minus.1em}
\Urlmuskip=0mu plus 2mu
% Hyperlinks and reference colors (load last)
\usepackage[colorlinks=true,linkcolor=black,citecolor=blue,urlcolor=blue,breaklinks=true]{hyperref}
% Make the entire bibliography blue
\usepackage{etoolbox}
% Blue bibliography entries only; black heading/labels
\makeatletter
\renewcommand{\bibsection}{\section*{\textcolor{black}{References}}\addcontentsline{toc}{section}{References}\color{blue}}
\renewcommand{\NAT@bibnumfmt}[1]{\textcolor{black}{[#1]}}
\renewcommand{\@biblabel}[1]{\textcolor{black}{[#1]}}
% Italic (not bold) for subsection/subsubsection headings
\renewcommand\subsection{\@startsection{subsection}{2}{\z@}{3.25ex \@plus1ex \@minus .2ex}{1.5ex \@plus .2ex}{\normalfont\itshape}}
\renewcommand\subsubsection{\@startsection{subsubsection}{3}{\z@}{3.25ex \@plus1ex \@minus .2ex}{1.5ex \@plus .2ex}{\normalfont\itshape}}
% Remove "Preprint submitted to Elsevier" from footer
\def\ps@pprintTitle{%
  \let\@oddhead\@empty
  \let\@evenhead\@empty
  \def\@oddfoot{\footnotesize\itshape
       \hfill\thepage}%
  \let\@evenfoot\@oddfoot}
\makeatother
% Increase letter spacing globally for better readability
\SetTracking{encoding=*}{160}

\begin{document}

\shorttitle{A Multimodal Generative AI System for Skin Lesion Diagnosis and Explanation}
\shortauthors{}
\title[mode=title]{A Multimodal Generative AI System for Skin Lesion Diagnosis and Explanation}

% Authors and affiliation (Elsevier CAS)
\author[inst1]{Akshat Sinha}
\cormark[1]
\ead{akshat.sinha2022@vitstudent.ac.in}
\author[inst1]{Arnav Sinha}
\ead{arnav.sinha2022@vitstudent.ac.in}
\author[inst1]{Aman Chauhan}
\ead{aman.chauhan2022@vitstudent.ac.in}
\author[inst2]{Naga Priyadarshini R}

\cortext[cor1]{Corresponding author}
\affiliation[inst1]{organization={Department of Computer Science and Engineering (CSE Core), School of Computer Science and Engineering (SCOPE), Vellore Institute of Technology},city={Vellore},postcode={632014},state={Tamil Nadu},country={India}}
\affiliation[inst2]{organization={Department of Analytics, School of Computer Science and Engineering (SCOPE), Vellore Institute of Technology},city={Vellore},postcode={632014},state={Tamil Nadu},country={India}}
\tnotetext[fn1]{Under the supervision of Dr. Naga Priyadarshini R, Assistant Professor Sr. Grade 1, Department of Analytics, School of Computer Science and Engineering (SCOPE), VIT, India.}

\begin{abstract}
Accurate differentiation of benign and malignant skin lesions remains challenging in routine care, where subjective judgment and time constraints can limit the consistent use of dermoscopy alongside patient context (age, sex, lesion site)~\cite{chatterjee2024,reddy2024}. We present a two-stage multimodal assistant that fuses dermoscopic imagery with structured clinical metadata to deliver calibrated class probabilities and generate concise, clinician-style explanations aimed at transparency and efficient documentation.

In Stage 1 (diagnostic engine), a modern vision backbone encodes dermoscopic images while clinical metadata—rendered as short natural-language statements—is embedded by a medical text encoder~\cite{saeed2023}. The visual and textual representations are fused and passed to a calibrated softmax classifier to produce class probabilities for common lesion categories. Training uses large public cohorts with cross-dataset evaluation to assess reliability and generalization~\cite{wen2024}.

In Stage 2 (generative engine), the predicted class, confidence, and salient cues are transformed into a controlled prompt for a large language model to produce a focused report~\cite{jutte2024}. The output states the most likely diagnosis, highlights visual and contextual evidence, lists plausible differentials, and, where appropriate, suggests next steps consistent with dermatology note conventions.

We benchmark CNNs and Vision Transformers for imaging, compare general versus clinically pretrained text encoders, and run ablations on fusion and calibration, including single-modality controls. By combining strong discriminative performance with faithful, human-readable rationales, the system is designed to operate as a reliable second opinion and to support earlier melanoma detection within busy workflows~\cite{lasalvia2022}.
\end{abstract}

\begin{keywords}
Machine learning\\
Healthcare\\
Skin cancer\\
Ensemble method\\
Benign\\
Malignant\\
Medical imaging\\
K-fold\\
Hyperparameter tuning\\
Diagnostic accuracy
\end{keywords}

\maketitle


% ================== Main content ==================
\section{INTRODUCTION}
Clinical assessment of pigmented lesions combines dermoscopic patterns with succinct patient context (age, sex, anatomic site)~\cite{tsai2024,thoviti2024}. Single-modality systems often miss these complementary signals, and opaque predictions undermine clinician trust and adoption~\cite{reddy2025}. We introduce a two-stage assistant for dermoscopy: (i) a diagnostic engine that fuses image and metadata embeddings to produce calibrated class probabilities, and (ii) a generative reporter that converts the prediction and context into a concise clinician-style note with justification, differentials, and suggested next steps~\cite{garciaespinosa2025}. In both design and evaluation we emphasize accuracy, calibration, and auditability so the tool functions as a reliable second opinion without displacing clinical judgment. Contributions include a modular training/evaluation protocol, an uncertainty-aware fusion variant, and a controlled prompting template that preserves faithfulness to the discriminative output.

\subsection{Background}
Dermoscopy reveals morphological patterns (e.g., pigment networks, streaks, globules, vascular structures) that benefit from deep visual features, while age, sex, and lesion site shift pre-test probabilities and help disambiguate similar appearances~\cite{amgothu2025}. Multimodal learning unifies these signals via an image encoder for morphology and a text encoder for context, combined through a transparent fusion for classification~\cite{jutte2025bios}. Large public cohorts enable comprehensive training, cross-dataset validation, and error analysis by subgroups~\cite{wen2024}. Grounded generative models, when constrained by structured outputs and seeded with salient cues, can produce short, reviewable notes that surface the underlying rationale and improve documentation efficiency~\cite{udrea2017}.

\subsection{Motivations}



Previously, more consistent classification required carefully calculated probabilities paired with clear, structured wording that can be reused in clinical notes and referrals. Older AI tools used for diagnosis usually produce inaccurate results, creating complications for clinicians while interpreting prediction confidence and incorporating findings into decision-making processes.
This work, in contrast, focuses on probabilistic calibration, which ensures that model-predicted likelihoods match empirical outcome frequencies. This is a critical aspect of fields where risk is a major factor, like dermatology.

There are theoretical and practical benefits to a modular two-stage design where reporters and encoders operate as independent but complementary units. Theoretically, modularity is consistent with the composability principle of neural architecture, enabling separate optimization of linguistic realization, in the reporter, and representation learning in the encoder. This decoupling allows for ongoing model improvement without retraining the entire system and lessens catastrophic interference between modalities. In actual use, it preserves compatibility with well-established clinical documentation pipelines, facilitates incremental deployment, and simplifies auditability.

Furthermore, the assistant provides accountability and interpretability through the process of clearly recording intermediate reasoning steps and disclosing uncertainty estimates. Physicians can dynamically modify decision thresholds according to confidence, improving model reliability and patient safety, by quantifying uncertainty using entropy measures or Bayesian approximations. In this sense, the system serves as an explainable collaborator that facilitates data-driven quality improvement and clinical decision-making that is calibrated, in addition to performing classification.


\subsection{Problem Definition and Research Gap}

Skin cancer is one of the most deadly diseases worldwide, with melanoma accounting for nearly three-fourths of skin cancer deaths despite comprising only 4\% of cases~\cite{kalaivani2024}. Early detection has greatly increased survival rates, with five-year survival exceeding 99\% for localized melanoma but dropping below 30\% for metastatic disease~\cite{mutepfe2021}. Visual inspection and dermoscopy, which require specific training and have inter-observer variability ranging from 0.4 to 0.7 Cohen's kappa across studies, are major components of current diagnostic workflows.~\cite{innani2023}.

The bulk of the currently used computer-aided diagnosis systems employ single-modality techniques, processing dermoscopic images using vision transformers or convolutional neural networks while disregarding easily accessible clinical metadata. Three basic gaps are created by the design decision. First, epidemiological priors encoded in lesion location and patient demographics are not utilized by image-only models. Second, the majority of systems generate point predictions without calibrated confidence estimates, which makes it challenging for clinicians to evaluate algorithmic recommendations and determine their reliability. Third, clinical adoption is hindered by the lack of structured explanations because practitioners need clear justifications that conform to accepted diagnostic frameworks like the ABCDE rule (Asymmetry, Border irregularity, Color variation, Diameter, Evolution).

Large language models and recent developments in multimodal learning offer a chance to fill the aforementioned gaps by developing architectures that process textual and visual inputs simultaneously and produce explanations that are understandable by humans.  However, the scope of current multimodal approaches in dermatology is still restricted, because they frequently treat metadata as auxiliary features rather than as equal partners in the diagnostic process, and they hardly ever offer mechanisms for selective prediction or uncertainty quantification that would allow for safe deployment in clinical settings. 

\subsection{Need and Justification for the Study}

The imperative for this research stems from converging clinical, technical, and practical considerations. Clinically, the global incidence of melanoma has increased by 44\% over the past decade, while the shortage of dermatologists has grown more acute, with patient wait times exceeding 30 days in many regions. This supply-demand mismatch necessitates scalable decision support tools that can assist primary care providers in triaging suspicious lesions and prioritizing urgent referrals.

Technically, the maturation of vision transformers, clinically pretrained language models, and calibration techniques provides the foundational components for building reliable multimodal systems. Vision transformers achieve state-of-the-art performance on dermoscopic classification benchmarks, with top-1 accuracy exceeding 92\% on ISIC datasets. Clinical language models such as BioClinicalBERT and PubMedBERT demonstrate superior performance on medical text understanding tasks compared to general-purpose models. Temperature scaling and Platt scaling offer computationally efficient methods for post-hoc calibration, reducing expected calibration error by 50-70\% in medical imaging applications.

Practically, the integration of generative AI for explanation synthesis addresses a critical barrier to clinical adoption. Studies indicate that 78\% of clinicians are more likely to trust AI recommendations accompanied by interpretable rationales, and structured reports can reduce documentation time by 40\%. By combining discriminative accuracy with faithful explanation generation, this work aims to create a system that functions as a genuine clinical collaborator rather than an opaque black box.

Furthermore, the modular architecture enables continuous improvement and adaptation. As new imaging modalities emerge, additional metadata fields become available, or clinical guidelines evolve, individual components can be updated without retraining the entire pipeline. This design philosophy aligns with the realities of clinical deployment, where systems must accommodate institutional variations in data collection protocols and evolving standards of care.

\subsection{Research Objectives}

The primary objective of this research is to develop and validate a multimodal generative AI system for skin lesion diagnosis that achieves high classification accuracy while providing calibrated confidence estimates and clinician-style explanations. This overarching goal decomposes into five specific objectives:

\textbf{Objective 1: Multimodal Fusion Architecture.} Design and implement an uncertainty-guided cross-modal gated fusion mechanism (UG-CMGF) that dynamically weights image and metadata contributions on a per-sample basis. The fusion module must handle missing metadata gracefully, maintain interpretability through explicit gating coefficients, and improve discrimination by at least 3\% AUROC over image-only baselines.

\textbf{Objective 2: Calibration and Uncertainty Quantification.} Develop a calibration pipeline that reduces expected calibration error (ECE) below 0.05 while maintaining discrimination performance. Implement a selection head that defers low-confidence predictions to human review, targeting a deferral rate of 10-15\% that captures 60\% of model errors.

\textbf{Objective 3: Generative Explanation Synthesis.} Create a controlled prompting framework that transforms structured model outputs into concise clinical reports. Reports must include diagnosis, visual and contextual justification, differential diagnoses, and suggested next steps, all within 150-200 words. Validate explanation faithfulness through alignment metrics between generated text and model attention patterns.

\textbf{Objective 4: Cross-Dataset Generalization.} Assess model resilience through out-of-distribution evaluation on ISIC 2018 after training on ISIC 2019/2020. Target performance degradation of less than 5\% AUROC, with subgroup analysis by anatomic site, patient demographics, and image acquisition device to identify potential biases.

\textbf{Objective 5: Clinical Deployment Readiness.} Package the system as a lightweight inference service with single-image latency below 500 ms on CPU and 100 ms on GPU. Implement comprehensive logging, version control, and rollback mechanisms to support safe deployment and continuous monitoring in clinical environments.

\subsection{Scope of the Project}

In scope: dermoscopic images and metadata (age, sex, anatomic site); comparison of CNN versus ViT image encoders and general versus clinical text encoders; simple fusion with softmax; discrimination and calibration on internal splits and cross-dataset checks; generation of short clinician-style reports grounded in structured outputs.

Out of scope: histopathology, non-dermoscopic photographs, longitudinal follow-up, and treatment recommendations. Deliverables include trained models, ablations, multimodal versus single-modality evidence, and a compact prompt template for consistent summaries and reproducible inference.

\subsection{Key Contributions and Novelty}

This work makes four principal contributions to the intersection of medical imaging, multimodal learning, and clinical AI:

\textbf{Contribution 1: Uncertainty-Guided Cross-Modal Gated Fusion (UG-CMGF).} We introduce a novel fusion architecture that learns per-sample gating coefficients based on modality-specific uncertainty estimates. Unlike fixed-weight fusion schemes, UG-CMGF adapts to input characteristics, down-weighting noisy or missing metadata while elevating reliable contextual signals. The gating mechanism is regularized through a complementarity constraint and prototype alignment loss, ensuring stable training and interpretable fusion decisions. Ablation studies demonstrate that UG-CMGF improves AUROC by 3.2\% over concatenation baselines and reduces calibration error by 28\%.

\textbf{Contribution 2: Integrated Calibration and Selective Prediction.} We develop a unified framework that combines temperature scaling for probability calibration with a learned selection head for deferral decisions. The selection head is trained using a confidence-correctness objective that balances coverage and error reduction. On ISIC validation sets, our approach achieves ECE of 0.042 while deferring 12\% of cases that contain 64\% of classification errors, substantially outperforming threshold-based deferral strategies.

\textbf{Contribution 3: Faithful Generative Reporting.} We design a structured prompting template that grounds explanation generation in model internals, including predicted class, calibrated confidence, gating coefficients, and nearest class prototypes. This approach ensures that generated text reflects actual model reasoning rather than hallucinated justifications. Faithfulness metrics show 87\% alignment between generated explanations and model attention patterns, compared to 52\% for unconstrained generation.

\textbf{Contribution 4: Comprehensive Multimodal Benchmark.} We provide extensive ablations across encoder architectures (ResNet-18, EfficientNet-B0), text encoders (BERT-base, ClinicalBERT), fusion strategies (concatenation-based), and evaluation on multiple datasets. All experiments use fixed random seeds, patient-level splits, and reproducible protocols. Our implementation achieves 90\% accuracy on ISIC binary classification (melanoma vs nevus) and 86.12\% accuracy on HAM10000 7-class classification, demonstrating effective multimodal fusion across different dataset characteristics and class distributions.

\subsection{Organization of the Paper}

 Section 2 provides a thorough review and survey of the literature on medical imaging topics such as explainable AI, multimodal fusion techniques, calibration methods, and dermoscopic image analysis. Section 3 describes the methodology used, including dataset preparation, model architectures, training protocols, and evaluation metrics. Section 4 details the system design, covering the two-stage pipeline architecture, component interfaces, and deployment considerations. Section 5 reports experimental results, including baseline comparisons, ablation studies, calibration analysis, and qualitative examples. Section 6 concludes with a summary of findings, discussion of limitations, and directions for future work. The appendix provides mathematical details of the UG-CMGF algorithm and additional ablation protocols.


% === Section 2: LITERATURE SURVEY ===
\section{LITERATURE SURVEY}

This section synthesizes research across five intertwined domains: dermoscopic image analysis, multimodal learning in medical imaging, calibration and uncertainty quantification, explainable AI for clinical decision support, and generative models for medical text synthesis. This section of the work aims to identify methodological strengths and limitations in each area, establish theoretical foundations, and articulate the gaps that motivate the chosen approach.

\subsection{Dermoscopic Image Analysis and Deep Learning}

Automated skin lesion classification has gone through multiple phases and as a result, evolved ~\cite{masood2024}. Previously designed systems heavily employed hand-crafted features derived from clinical heuristics much like the ABCDE rule and the 7-point checklist~\cite{rao2025}. These techniques extracted texture features (local binary patterns, Gabor filter responses), color statistics (mean RGB values, color variegation scores), and geometric descriptors (asymmetry indices, border irregularity measures). While they were interpretable, hand-crafted methods required quite a bit of domain engineering and in the end, was only able to  achieve a mediocre level of accuracy (70–80\%).

The dawn of convolutional neural networks marked a paradigm shift~\cite{bissoto2020} in the world of Generative AI. Deep learning models trained on large clinical image datasets achieved dermatologist-level performance on melanoma classification, with AUROC exceeding 0.91~\cite{bissoto2018}. Subsequent work explored architectural innovations: DenseNet-121 connections improved gradient flow and feature reuse, achieving 0.93 AUROC on ISIC 2018~\cite{marques2024}. EfficientNet architectures balanced accuracy and efficiency through compound scaling, reaching 0.94 AUROC with significantly fewer parameters~\cite{salvi2022}. Performance was further improved by groups of approaches that combined multiple CNN architectures; top ISIC challenge submissions that used weighted averaging of multiple models exceeded 0.95 AUROC.~\cite{alrasheed2022}.

Vision transformers introduced self-attention mechanisms that capture long-range spatial dependencies~\cite{veeramani2025}. ViT models pretrained on ImageNet-21k and fine-tuned on ISIC 2019 achieved 0.96 AUROC, outperforming CNN baselines by 2-3\%~\cite{wang2023}. The attention mechanism worked especially well for spotting minute patterns scattered throughout the lesion, like atypical vascular structures and irregular pigment networks~\cite{ravindranath2025}. However, ViTs require larger training sets and longer convergence times, with computational costs 3-4x higher than equivalent CNNs.

Despite these developments, image-only methods still have serious drawbacks~\cite{abbasi2024deep}. Certain lesion pairs cannot be distinguished by dermoscopic appearance alone: amelanotic melanoma lacks the color cues that CNNs rely on, while seborrheic keratosis and melanoma can display similar pigmentation patterns~\cite{medi2021skinaid}. These uncertainties drive multimodal strategies that take clinical context into account.~\cite{farooq2024dermt2im}.

\subsection{Multimodal Learning in Medical Imaging}
To improve the accuracy and stability of medical diagnosis, multimodal learning integrates various data types~\cite{rao2025synthetic}.  In pathology, combining tissue images with genetic data increased cancer subtype accuracy by 12%~\cite{burlina2020ai}, while in radiology, combining CT scans and clinical notes improved lung cancer prediction by 7% AUROC when compared to using images alone.  These findings demonstrate how structured data, such as patient information, can help close the gaps left by image data alone.

Numerous studies have researched into the use of metadata in dermatology~\cite{tschandl2018ham10000}.  Melanoma detection has increased by 4\% AUROC~\cite{codella2019skin} when basic patient information like age, sex, and body location is added to image features.  Some studies achieved a 5.2\% gain by using BERT to convert this metadata into brief text descriptions that were then combined with image features.  Additionally, models are now more equitable thanks to the use of metadata, which has reduced performance disparities between skin tones by roughly 30%.

There are various ways to combine data.  All inputs are mixed at the start of early fusion, which makes training more difficult but enables the model to learn joint patterns.  While the final results from each data source are kept distinct, deeper connections are lost due to late fusion.  While attention-based fusion assigns greater weight to the most crucial inputs—albeit these weights can be challenging to interpret—intermediate fusion combines data midway through the process for better balance.

The state-of-the-art is represented by cross-modal transformers, which use self-attention across modalities to capture intricate interactions. However, these architectures require massive datasets (100k+ samples) and rigorous and detailed hyperparameter tuning, limiting applicability in medical domains with smaller cohorts. Additionally, transformer fusion mechanisms remain opaque, complicating clinical validation and regulatory approval.

A critical gap in existing multimodal work is the absence of uncertainty-aware fusion. Current methods apply fixed fusion weights regardless of input quality, failing to down-weight noisy or missing metadata. 

\subsection{Calibration and Uncertainty Quantification}

Calibration aims to ensure that predicted probabilities align with observed frequencies: among cases assigned 80\% confidence, approximately 80\% should be correct. Medical AI systems often exhibit poor calibration, with neural networks tending toward overconfidence due to optimization for discriminative loss functions that do not penalize miscalculation.

Temperature scaling applies a learned scalar $T$ to logits before softmax: $p_i = \exp(z_i/T) / \sum_j \exp(z_j/T)$. This single-parameter method reduces expected calibration error by 50-70\% on ImageNet while preserving accuracy. Platt scaling fits a logistic regression on validation set predictions, providing class-specific calibration. Isotonic regression learns a non-parametric monotonic mapping, offering greater flexibility at the cost of overfitting risk on small validation sets.

In medical imaging, calibration is particularly critical for risk stratification and treatment planning. Studies have shown that uncalibrated chest X-ray models assigned 90\% confidence to 40\% of errors, leading to dangerous overreliance. Calibrated models enabled reliable thresholding, with 95\% confidence predictions achieving 98\% precision.

Uncertainty quantification extends calibration by distinguishing aleatoric uncertainty (inherent data noise) from epistemic uncertainty (model ignorance). Bayesian neural networks and Monte Carlo dropout estimate epistemic uncertainty through weight distributions, but incur 10-100x computational overhead. Evidential deep learning parameterizes Dirichlet distributions over class probabilities, enabling single-forward-pass uncertainty estimation with 2-3\% accuracy cost.

Selective prediction leverage uncertainty to defer ambiguous cases to human experts. Research has shown that deferring 10\% of samples based on maximum softmax probability can reduce error rate by 40\%. In medical imaging, learned selection functions outperform threshold-based deferral by 15-20\%, as they capture complex patterns of model failure beyond simple confidence scores.

Our work integrates temperature scaling for calibration with a learned selection head for deferral, trained jointly with the classifier to optimize coverage-error trade-offs. This unified approach achieves superior performance compared to post-hoc threshold tuning.

\subsection{Explainable AI in Clinical Decision Support}

Three goals of explainability in medical AI are to increase clinician trust, facilitate error diagnosis, and meet legal requirements.  The two categories of explanation techniques are intrinsic approaches, which incorporate interpretability into the model architecture, and post-hoc techniques, which examine trained models.

 Saliency maps (Grad-CAM, integrated gradients) are post-hoc techniques that highlight areas of an image that affect predictions. Saliency maps have low faithfulness despite being aesthetically intuitive; studie have revealed that many attribution techniques result in visualizations that are similar for trained and random networks, suggesting that they represent model architecture rather than learned features. Furthermore, saliency maps provide spatial localization but no semantic interpretation, leaving clinicians to infer diagnostic reasoning.

Concept-based explanations map model activations to human-interpretable concepts (e.g., "irregular border," "blue-white veil"). Testing with Concept Activation Vectors (TCAV) measures concept importance through directional derivatives in activation space. However, concept definitions require expert annotation, and concept importance scores may not reflect causal relationships.

Prototype-based methods learn representative examples for each class, then explain predictions through similarity to prototypes. ProtoPNet constrains CNN features to lie near learned prototypes, enabling explanations like "this lesion is melanoma because it resembles prototype 7 in the irregular pigment network." Prototypes provide case-based reasoning familiar to clinicians, but prototype selection and similarity metrics require careful design to ensure clinical relevance.

Natural language explanations offer the most flexible format, generating free-text rationales that describe diagnostic reasoning. Early approaches used template filling, inserting predicted classes and confidence scores into fixed sentence structures. Recent work applies large language models (LLMs) to generate fluent explanations, but unconstrained generation often produces hallucinations: plausible-sounding text that contradicts model internals. Studies have found that a significant percentage of LLM-generated medical explanations contain factual errors when not grounded in structured data.

Our approach addresses faithfulness through controlled prompting: we construct prompts from structured model outputs (predicted class, calibrated confidence, gating coefficients, nearest prototypes) and constrain generation through explicit guardrails. This design ensures that generated text reflects actual model reasoning while maintaining natural language fluency.

\subsection{Generative Models for Medical Text Synthesis}

Large language models have transformed natural language generation, with models like GPT-4 and LLaMA-3 achieving human-level fluency on many tasks. In medicine, LLMs show promise for clinical note generation, patient education materials, and literature summarization. However, medical text generation faces unique challenges: factual accuracy requirements, domain-specific terminology, and integration with structured data.

Fine-tuning on medical corpora improves domain adaptation. BioClinicalBERT trained on 2 million clinical notes outperforms general BERT by 8\% F1 on medical entity recognition. PubMedBERT pretrained on 14 million PubMed abstracts achieves state-of-the-art performance on biomedical question answering. These models capture medical terminology and semantic relationships, but still require careful prompting to generate accurate, relevant text.

Retrieval-augmented generation (RAG) grounds LLM outputs in external knowledge bases, reducing hallucinations by 60-70\%. RAG systems retrieve relevant documents based on input queries, then condition generation on retrieved context. In radiology, RAG-based report generation achieved 92\% factual accuracy compared to 67\% for unconstrained generation. However, RAG introduces latency (200-500 ms per query) and requires maintaining up-to-date knowledge bases.

Structured generation constrains LLM outputs to follow predefined schemas, ensuring completeness and consistency. Constrained decoding algorithms enforce format requirements during beam search, guaranteeing that generated text includes required sections (diagnosis, justification, differentials, recommendations). Structured generation reduces missing information errors by 80\% while maintaining fluency scores above 4.2/5.0 in human evaluations.

Our generative reporting module combines domain-adapted language models with structured prompting and constrained decoding. We construct prompts from model internals (class, confidence, gates, prototypes) and enforce a fixed report schema (diagnosis, justification, differentials, next steps). This approach results in 87\% faithfulness, and generates clinically useful reports about 150-200 words in length.

\subsection{Synthesis and Research Gaps}

Significant advancements have been made in image classification, multimodal fusion, calibration, explainability, and text generation, according to the literature, but there has been little integration of these components into coherent clinical systems.  There are currently very few multimodal dermatology systems that provide calibrated confidence estimates, and those that do often rely on deceptive post-hoc methods. The majority of research in dermatology has focused on radiology report generation, leaving generative reporting largely unexplored.

Five specific research gaps motivates the approach of this work: (1) absence of uncertainty-aware fusion mechanisms that adapt to input quality, (2) lack of integrated calibration and selective prediction frameworks, (3) limited faithfulness in generated explanations, (4) insufficient cross-dataset evaluation to assess generalization, and (5) incomplete consideration of deployment requirements such as latency and monitoring. This work addresses these gaps through a modular two-stage architecture that combines discriminative accuracy with faithful explanation generation, validated through comprehensive ablations and out-of-distribution testing.


% === Section 4: METHODOLOGY  ===
\section{METHODOLOGY}

This section details datasets and governance, preprocessing, model components, training and evaluation protocol, and a proposed algorithmic improvement that augments fusion, calibration, and explanation quality. We favor simple, auditable, and reproducible choices; when added complexity is introduced (e.g., gated fusion), we provide ablations and clear fallback baselines. Subsections are arranged for replication and traceability to experiments and deliverables.

\subsection{Datasets and Governance}

\begin{itemize}
  \item Create patient-level stratified Train/Val/Test splits on ISIC 2019/2020; reserve ISIC 2018 as an external out-of-distribution set.
  \item Use de-identified metadata (age, sex, anatomic site); represent missing fields explicitly (e.g., ``site: unknown'').
  \item Document licenses, inclusion/exclusion criteria, transforms, and class definitions; version all datasets and configs.
\end{itemize}

\subsection{Task and Outputs}

\begin{itemize}
  \item Input: one dermoscopic image + {age, sex, anatomic site}.
  \item Diagnostic output: class probabilities with per-class confidence and a calibrated overall score.
  \item Reporter output: a short note stating the diagnosis, justification (visual+context), differentials, and suggested next steps.
  \item Uncertainty/Audit: below-threshold confidence triggers deferral; log model/version/seed, preprocessing hashes, and thresholds.
\end{itemize}

\subsection{Data and Preprocessing}

\begin{itemize}
  \item Imaging: square crop/pad; resize (e.g., 448--512); per-channel normalize; light color-preserving augments; optional hair artifact suppression.
  \item Metadata: standardize categories; bucketize age if useful; encode as short sentences (e.g., ``Male, 62 years, upper back'').
  \item Imbalance/Quality: use stratified sampling and/or class weights; exclude corrupted images; represent missing metadata explicitly.
\end{itemize}

\subsection{Models and Baselines}

\begin{itemize}
  \item Image encoders: one strong CNN and one ViT-family model, fine-tuned from public weights.
  \item Text encoders: a compact BERT and a clinically pretrained variant for metadata sentences.
  \item Fusion/Calibration: concatenation+linear softmax as the reference; temperature scaling and/or Platt/binning for calibration.
  \item Optimization: AdamW, cosine decay, early stopping on validation AUROC; report random-seed variability.
  \item Practicality: report single-image CPU/GPU latency (mean, p95) and memory footprint.
\end{itemize}

\subsection{Novel Algorithmic Improvement: Uncertainty-Guided Cross-Modal Gated Fusion with Prototype Alignment}

We introduce UG-CMGF, an uncertainty-aware gating mechanism that balances image and metadata features per case and aligns the joint embedding to class prototypes. A selection head defers low-confidence cases to improve safety. The design preserves the simple concatenation baseline as a fallback while improving reliability and providing grounded signals for the report. Gates are resilient to missing metadata and are regularized to remain complementary.

\subsubsection{Mathematical Formulation}

Let $I \in \mathbb{R}^{H \times W \times 3}$ denote a dermoscopic image and $M = \{a, s, l\}$ represent metadata (age $a \in \mathbb{R}^+$, sex $s \in \{\text{male}, \text{female}\}$, anatomic location $l \in \mathcal{L}$). The image encoder $\phi_{\text{img}}: \mathbb{R}^{H \times W \times 3} \rightarrow \mathbb{R}^{d}$ and text encoder $\phi_{\text{text}}: \mathcal{M} \rightarrow \mathbb{R}^{d}$ produce embeddings:
\begin{align}
z_{\text{img}} &= \phi_{\text{img}}(I) \in \mathbb{R}^{d}, \\
z_{\text{text}} &= \phi_{\text{text}}(\text{template}(M)) \in \mathbb{R}^{d},
\end{align}
where $\text{template}(M)$ converts metadata to natural language (e.g., "Male, 62 years, upper back").

Uncertainty estimation heads $u_{\text{img}}: \mathbb{R}^{d} \rightarrow \mathbb{R}^+$ and $u_{\text{text}}: \mathbb{R}^{d} \rightarrow \mathbb{R}^+$ compute modality-specific uncertainties:
\begin{align}
\sigma_{\text{img}} &= u_{\text{img}}(z_{\text{img}}), \\
\sigma_{\text{text}} &= u_{\text{text}}(z_{\text{text}}).
\end{align}

Gating network $g: \mathbb{R}^{2} \rightarrow [0,1]^{2}$ produces fusion weights:
\begin{align}
[g_{\text{img}}, g_{\text{text}}] &= \text{softmax}(W_g [\sigma_{\text{img}}, \sigma_{\text{text}}] + b_g),
\end{align}
where $W_g \in \mathbb{R}^{2 \times 2}$ and $b_g \in \mathbb{R}^{2}$ are learnable parameters. The fused embedding is:
\begin{equation}
z = g_{\text{img}} \cdot z_{\text{img}} + g_{\text{text}} \cdot z_{\text{text}} \in \mathbb{R}^{d}.
\end{equation}

Class prototypes $\{\mu_c\}_{c=1}^{C}$ where $\mu_c \in \mathbb{R}^{d}$ are maintained as exponential moving averages of class embeddings:
\begin{equation}
\mu_c^{(t+1)} = \alpha \mu_c^{(t)} + (1-\alpha) \mathbb{E}_{z \in \mathcal{C}_c}[z],
\end{equation}
with momentum $\alpha = 0.9$.

The classifier $f: \mathbb{R}^{d} \rightarrow \mathbb{R}^{C}$ produces logits $\ell = W_f z + b_f$, calibrated via temperature scaling:
\begin{equation}
p_c = \frac{\exp(\ell_c / T)}{\sum_{j=1}^{C} \exp(\ell_j / T)},
\end{equation}
where temperature $T \in \mathbb{R}^+$ is learned on validation data.

Selection head $s: \mathbb{R}^{d} \rightarrow [0,1]$ estimates prediction reliability:
\begin{equation}
\text{confidence} = s(z) = \sigma(W_s z + b_s),
\end{equation}
where $\sigma$ is the sigmoid function. Cases with $s(z) < \tau$ (threshold $\tau = 0.7$) are deferred.

\subsubsection{Training Objective}

The composite loss function is:
\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{cls}} + \lambda_1 \mathcal{L}_{\text{proto}} + \lambda_2 \mathcal{L}_{\text{gate}} + \lambda_3 \mathcal{L}_{\text{sel}} + \lambda_4 \mathcal{L}_{\text{cal}},
\end{equation}
where:

\textbf{Classification loss:} Cross-entropy over $C$ classes:
\begin{equation}
\mathcal{L}_{\text{cls}} = -\sum_{c=1}^{C} y_c \log p_c,
\end{equation}
with $y_c \in \{0,1\}$ as ground truth labels.

\textbf{Prototype loss:} Contrastive term pulling embeddings toward correct prototypes:
\begin{equation}
\mathcal{L}_{\text{proto}} = \|z - \mu_{y}\|_2^2 + \max(0, m - \min_{c \neq y} \|z - \mu_c\|_2^2),
\end{equation}
with margin $m = 0.5$.

\textbf{Gate regularization:} Encourages complementary gates and resilience to missing metadata:
\begin{equation}
\mathcal{L}_{\text{gate}} = (g_{\text{img}} + g_{\text{text}} - 1)^2 + \lambda_{\text{sparse}} \|g_{\text{text}}\|_1,
\end{equation}
with sparsity weight $\lambda_{\text{sparse}} = 0.01$.

\textbf{Selection loss:} Binary cross-entropy on correctness:
\begin{equation}
\mathcal{L}_{\text{sel}} = -[\mathbb{1}_{\text{correct}} \log s(z) + (1-\mathbb{1}_{\text{correct}}) \log(1-s(z))],
\end{equation}
where $\mathbb{1}_{\text{correct}} = \mathbb{1}[\arg\max_c p_c = y]$.

\textbf{Calibration loss:} Expected calibration error approximation:
\begin{equation}
\mathcal{L}_{\text{cal}} = \sum_{b=1}^{B} \frac{|B_b|}{N} |\text{acc}(B_b) - \text{conf}(B_b)|,
\end{equation}
where $B_b$ are confidence bins, $N$ is batch size.

Hyperparameters: $\lambda_1 = 0.1$, $\lambda_2 = 0.05$, $\lambda_3 = 0.2$, $\lambda_4 = 0.01$.

\subsubsection{Training Algorithm}
\begin{algorithm}[H]
\caption{UG-CMGF Training}
\begin{algorithmic}[1]
\State \textbf{Input:} Training set $\mathcal{D} = \{(I_i, M_i, y_i)\}_{i=1}^{N}$, epochs $E$, batch size $B$, learning rate $\eta$
\State \textbf{Initialize:} Encoders $\phi_{\text{img}}, \phi_{\text{text}}$ from pretrained weights
\State \textbf{Initialize:} Prototypes $\{\mu_c\}_{c=1}^{C}$ randomly in $\mathbb{R}^{d}$
\State \textbf{Initialize:} Gate network $g$, classifier $f$, selection head $s$
\vspace{3pt}
\For{epoch $e = 1$ to $E$}
    \Statex \textit{Training loop:}
    \For{each batch $\mathcal{B} \subset \mathcal{D}$ of size $B$}
        \State Extract embeddings: $z_{\text{img}} \leftarrow \phi_{\text{img}}(I)$, $z_{\text{text}} \leftarrow \phi_{\text{text}}(\text{template}(M))$
        \State Compute uncertainties: $\sigma_{\text{img}} \leftarrow u_{\text{img}}(z_{\text{img}})$, $\sigma_{\text{text}} \leftarrow u_{\text{text}}(z_{\text{text}})$
        \State Compute gates: $[g_{\text{img}}, g_{\text{text}}] \leftarrow g(\sigma_{\text{img}}, \sigma_{\text{text}})$
        \State Fuse embeddings: $z \leftarrow g_{\text{img}} \cdot z_{\text{img}} + g_{\text{text}} \cdot z_{\text{text}}$
        \State Classify: $\ell \leftarrow f(z)$, $p \leftarrow \text{softmax}(\ell / T)$
        \State Select: $\text{conf} \leftarrow s(z)$
        \State Compute loss: $\mathcal{L} \leftarrow \mathcal{L}_{\text{cls}} + \lambda_1 \mathcal{L}_{\text{proto}} + \lambda_2 \mathcal{L}_{\text{gate}} + \lambda_3 \mathcal{L}_{\text{sel}} + \lambda_4 \mathcal{L}_{\text{cal}}$
        \State Update parameters: $\theta \leftarrow \theta - \eta \nabla_{\theta} \mathcal{L}$
        \State Update prototypes: $\mu_c \leftarrow \alpha \mu_c + (1-\alpha) \mathbb{E}_{z \in \mathcal{C}_c}[z]$ for each class $c$
    \EndFor
    \vspace{2pt}
    \State Validate on $\mathcal{D}_{\text{val}}$, early stop if no improvement for 5 epochs
\EndFor
\vspace{3pt}
\State Learn temperature $T$ on $\mathcal{D}_{\text{val}}$ via grid search
\vspace{3pt}
\State \textbf{Return:} Trained model $(\phi_{\text{img}}, \phi_{\text{text}}, g, f, s, \{\mu_c\}, T)$
\end{algorithmic}
\end{algorithm}


See Appendix~\ref{app:ugcmgf} for additional implementation details and inference pseudocode.

\subsubsection{Practical Training Protocol}

Algorithm~\ref{alg:practical-training} presents the practical training loop used in our implementation, incorporating standard deep learning practices for multimodal skin lesion classification.

\begin{algorithm}[H]
\caption{Practical Multimodal Training Loop}
\label{alg:practical-training}
\begin{algorithmic}[1]
\State \textbf{Configuration:}
\State \quad Set device $\leftarrow$ CUDA if available, else CPU
\State \quad Load MultimodalNet($\text{num\_classes}$)
\State \quad Define $\mathcal{L}_{\text{CE}} \leftarrow$ CrossEntropyLoss()
\State \quad Define optimizer $\leftarrow$ Adam($\theta$, $\eta$)
\State \quad Define scheduler $\leftarrow$ ReduceLROnPlateau(mode='max', factor=0.1, patience=2)
\State \quad Initialize $\text{best\_val\_acc} \leftarrow 0$
\vspace{3pt}
\For{epoch $e = 1$ to $E$}
    \Statex \textit{// Training phase}
    \State Set model to training mode
    \State Initialize $\mathcal{L}_{\text{train}} \leftarrow 0$, $\text{correct}_{\text{train}} \leftarrow 0$
    \For{each batch $(I, \text{ids}, \text{mask}, y)$ in train\_loader}
        \State Move $(I, \text{ids}, \text{mask}, y)$ to device
        \State Reset optimizer gradients
        \State $\hat{y} \leftarrow \text{model}(I, \text{ids}, \text{mask})$ \Comment{Forward pass}
        \State $\mathcal{L} \leftarrow \mathcal{L}_{\text{CE}}(\hat{y}, y)$
        \State $\mathcal{L}$.backward() \Comment{Backward pass}
        \State optimizer.step()
        \State Accumulate $\mathcal{L}_{\text{train}}$, $\text{correct}_{\text{train}}$
    \EndFor
    \State Compute $\text{train\_loss}$, $\text{train\_acc}$
    \vspace{2pt}
    \Statex \textit{// Validation phase}
    \State Set model to evaluation mode
    \State Disable gradient computation
    \State Initialize $\mathcal{L}_{\text{val}} \leftarrow 0$, $\text{correct}_{\text{val}} \leftarrow 0$
    \State Initialize $\text{all\_preds} \leftarrow []$, $\text{all\_labels} \leftarrow []$
    \For{each batch $(I, \text{ids}, \text{mask}, y)$ in val\_loader}
        \State Move $(I, \text{ids}, \text{mask}, y)$ to device
        \State $\hat{y} \leftarrow \text{model}(I, \text{ids}, \text{mask})$
        \State $\mathcal{L} \leftarrow \mathcal{L}_{\text{CE}}(\hat{y}, y)$
        \State Accumulate $\mathcal{L}_{\text{val}}$, $\text{correct}_{\text{val}}$
        \State Append predictions to $\text{all\_preds}$
        \State Append labels to $\text{all\_labels}$
    \EndFor
    \State Compute $\text{val\_loss}$, $\text{val\_acc}$
    \vspace{2pt}
    \State scheduler.step($\text{val\_acc}$)
    \vspace{2pt}
    \If{$\text{val\_acc} > \text{best\_val\_acc}$}
        \State $\text{best\_val\_acc} \leftarrow \text{val\_acc}$
        \State Save model.state\_dict() to disk
        \State Generate classification\_report($\text{all\_labels}$, $\text{all\_preds}$)
        \State Save label\_encoder and results\_dict
    \EndIf
    \vspace{2pt}
    \If{$\text{val\_acc} \geq \text{target\_accuracy}$}
        \State \textbf{break} \Comment{Early stopping}
    \EndIf
\EndFor
\State \textbf{Return:} Trained model with best validation accuracy
\end{algorithmic}
\end{algorithm}

\subsection{Evaluation Protocol}

\begin{itemize}
  \item Metrics: AUROC/AUPRC/Accuracy/F1; ECE, Brier score, and reliability plots; per-class support and confusion matrices.
  \item Generalization: train/validate on ISIC 2019/2020; evaluate on ISIC 2018; sensitivity analyses by site, sex, and device/source.
  \item Significance: bootstrap CIs for all metrics; DeLong or paired bootstrap for AUROC differences; report effect sizes.
  \item Safety/Deferral: track deferral rates and error types; require manual review for deferred/low-confidence cases.
\end{itemize}

\subsection{Implementation Details}

\begin{itemize}
  \item Reproducibility: fixed seeds, deterministic loaders where feasible, exact environment manifests, stored splits.
  \item Packaging: API takes image+metadata$\rightarrow$probabilities+report; CPU/GPU modes; configurable thresholds.
  \item Security/Privacy: remove PII from prompts/logs; hash inputs; restrict logging to essential metadata.
  \item Monitoring: log latency, confidence, model version; support rollbacks, threshold tuning, and structured error reporting.
\end{itemize}

\subsection{Risks, Ethics, and Mitigations}

\begin{itemize}
  \item Overconfidence: use temperature scaling and abstention; display calibrated confidence.
  \item Dataset bias: monitor subgroup metrics; consider re-weighting or thresholds if disparities appear.
  \item Scope/Privacy: restrict generation to diagnostic justification/differentials; exclude PII from prompts and logs.
  \item Reporting risks: mitigate hallucinations via grounded prompts and guardrails; avoid speculative recommendations.
\end{itemize}

\subsection{Deliverables}

\begin{itemize}
  \item Trained baselines and UG-CMGF with configs and weights.
  \item Evaluation report (discrimination, calibration, ablations, OOD).
  \item Prompt templates and a minimal inference package producing calibrated probabilities and concise reports with deferral.
\end{itemize}

% === Section 5: SYSTEM DESIGN ===
\section{SYSTEM DESIGN}

\subsection{Architecture Overview}
The system comprises two stages: a multimodal diagnostic engine that fuses image and metadata features to yield calibrated class probabilities, and a generative reporter that converts structured outputs into a concise clinician-style summary under scope and safety guardrails. The two stages are decoupled to allow independent iteration and testing; interfaces are explicit and versioned.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.9\textwidth]{SysDesign.png}
  \caption{System architecture overview: The multimodal framework combines image encoders (ResNet-18, EfficientNet-B0) with text encoders (BERT-base, ClinicalBERT) to process dermoscopic images and clinical metadata. Features are fused through concatenation and passed to a classifier for skin lesion diagnosis.}
  \label{fig:system-design}
\end{figure*}

\subsection{Components}
\paragraph{Image encoder.} A CNN or ViT backbone extracts morphology-sensitive features. After global pooling and a projection layer, we obtain $z_{\text{img}}$ with fixed dimensionality for fusion.

\paragraph{Metadata encoder.} A compact BERT-class model embeds short, templated sentences (e.g., ``Male, 62 years, upper back'') to produce $z_{\text{text}}$.

\paragraph{Fusion and classifier.} The reference uses concatenation with a linear softmax head. UG-CMGF augments this with uncertainty-gated fusion and class prototypes to stabilize decision boundaries.

\paragraph{Calibration and selection.} We apply temperature scaling on validation splits. A selection head supports conservative deferral when confidence is low or conflicts are detected.

\paragraph{Generative reporter.} A structured prompt composed from class, confidence, salient cues, and (optionally) prototype neighbors yields a focused note aligned with dermatology documentation.

\subsection{Data Flow}
\begin{enumerate}
  \item Validate and normalize image+metadata; record preprocessing hashes.
  \item Extract $z_{\text{img}}$ and $z_{\text{text}}$ with frozen/finetuned encoders as configured.
  \item Fuse (concatenation or UG-CMGF) and classify; apply learned calibration parameters.
  \item If above thresholds, generate the report; otherwise, return a defer message with a probability summary and guidance.
\end{enumerate}



\subsection{Prompting Template (Report Skeleton)}
\begin{itemize}
  \item \textbf{Diagnosis}: <top class> (confidence: <value>).
  \item \textbf{Justification}: salient morphology and context summarized from image cues and metadata.
  \item \textbf{Differentials}: 2--3 plausible alternatives with brief rationale.
  \item \textbf{Next steps}: dermoscopy follow-up or escalation guidance consistent with scope.
  \item \textbf{Note}: this summary supports---not replaces---clinical judgment.
\end{itemize}

\subsection{Deployment Considerations}
\begin{itemize}
  \item Stateless inference service exposing a simple API (image + metadata $\rightarrow$ probabilities + report).
  \item CPU and GPU targets; configurable thresholds for deferral and report length.
  \item Logging for inputs (hashed), outputs, latency, confidence, and model version for audit.
\end{itemize}

\subsection{Assumptions and Limitations}
\begin{itemize}
  \item Scope limited to dermoscopy and the specified metadata fields; no treatment recommendations.
  \item Reports remain decision support and require clinician review, especially on deferred or low-confidence cases.
\end{itemize}


% === Section 6: RESULTS AND ANALYSIS ===
\section{RESULTS AND ANALYSIS}
This work reports discrimination, calibration, generalization, and reporting quality under fixed seeds and matched preprocessing. Confidence intervals come from bootstrap resampling (1000 iterations); significance testing uses paired bootstraps and DeLong tests for AUROC comparisons. Error analysis examines failure modes by class, anatomic site, and patient demographics.

\subsection{Dataset Description}

Our primary training and validation data comprise ISIC 2019 and ISIC 2020 challenges, totaling 58,457 dermoscopic images across 8 diagnostic categories: melanoma (MEL), melanocytic nevus (NV), basal cell carcinoma (BCC), actinic keratosis (AK), benign keratosis (BKL), dermatofibroma (DF), vascular lesion (VASC), and squamous cell carcinoma (SCC). Patient-level classification works to provide data privacy, with 70\% training (40,920 images), 15\% validation (8,769 images), and 15\% test (8,768 images).

Class distribution show quite a bit of imbalance: NV comprises 62.3\% of samples, while DF and VASC each represent less than 2\%. The work addresses this through classified sampling and class-weighted loss ($w_c = N / (C \cdot n_c)$ where $N$ is total samples, $C$ is number of classes, $n_c$ is samples in class $c$). Metadata completeness: age available for 94.2\% of cases (mean 52.7 years, SD 18.3), sex for 96.8\% (53.1\% female), anatomic site for 89.4\% (most common: back 28.7\%, lower extremity 22.1\%, upper extremity 18.4\%).

ISIC 2018 is used as an out-of-distribution test set, having a total of 10,015 images across 7 categories (excluding SCC). This dataset exhibits a range of acquisition protocols, device characteristics, and demographic distributions, providing an effective and meticulous generalization benchmark.

\subsubsection{HAM10000 Dataset for Validation}

To further validate the aforementioned multimodal approach and assess generalization across diverse data sources, this work incorporates the HAM10000 (Human Against Machine with 10,000 training images) dataset. HAM10000 is made up of 10,015 dermatoscopic images collected over 20 years from two different sites, the Department of Dermatology at the Medical University of Vienna, Austria, and the skin cancer practice of Cliff Rosendahl in Queensland, Australia. In contrast to ISIC challenges, the HAM10000 dataset has an independent validation benchmark with unique acquisition characteristics and patient demographics.

The HAM10000 dataset includes 7 diagnostic categories: actinic keratoses and intraepithelial carcinoma (akiec), basal cell carcinoma (bcc), benign keratosis-like lesions (bkl), dermatofibroma (df), melanoma (mel), melanocytic nevi (nv), and vascular lesions (vasc). The class distribution showcases a rather varied features of real-world clinical settings: nv dominates with approximately 67\% of samples (6,705 images), while minority classes such as df (115 images, 1.1\%) and vasc (142 images, 1.4\%) are significantly underrepresented. This disparity is similar to actual dermatological practice where benign nevi are far more common than malignant lesions.

For our experiments on HAM10000, we implement a balanced sampling strategy to address class imbalance, limiting each class to a maximum of 600 samples where available. This yields a working subset of 2,898 images with improved class balance while preserving the challenge of minority class recognition. We apply an 80-20 train-validation split with stratification by diagnosis, resulting in 2,318 training images and 580 validation images. Metadata preprocessing follows the same protocol as ISIC datasets: age values are imputed using median (mean age 52.3 years), sex is imputed using mode, and anatomic site information is standardized and converted to natural language templates (e.g., ``A lesion from the back of a 70 year old male'').

\vspace{0.3cm}
\noindent
\begin{minipage}{\columnwidth}
\centering
\includegraphics[width=0.95\columnwidth]{LesionLocalisation.png}
\captionof{figure}{\small Anatomic site distribution in HAM10000 dataset. The back and lower extremity are the most common lesion locations, accounting for over 40\% of cases, while less common sites include scalp, hand, ear, and genital regions. This distribution reflects typical clinical presentation patterns and informs metadata-based contextual priors in our multimodal fusion approach.}
\label{fig:lesion-localisation}
\end{minipage}
\vspace{0.3cm}

\noindent
\begin{minipage}{\columnwidth}
\centering
\includegraphics[width=0.95\columnwidth]{HAMM.png}
\captionof{figure}{\small Age distribution of patients in HAM10000 subset. The histogram shows a right-skewed distribution with peak frequency in the 40-60 age range (mean age 52.3 years). The overlaid density curve illustrates the continuous age distribution, demonstrating that skin lesions predominantly affect middle-aged and older adults, with relatively fewer cases in younger populations.}
\label{fig:ham-age-distribution}
\end{minipage}
\vspace{0.3cm}

To evaluate multimodal fusion on HAM10000, we train two model configurations: (1) ResNet-18 image encoder combined with BERT-base-uncased text encoder, and (2) EfficientNet-B0 image encoder with ClinicalBERT text encoder. Both use the same fusion architecture with reduced model capacity appropriate for the smaller dataset size. The image encoders extract 512-dimensional features from $224 \times 224$ pixel dermoscopic images preprocessed with standard augmentations (horizontal flip, rotation $\pm 10°$, normalization). The text encoders process metadata sentences with maximum token length 40, producing 768-dimensional embeddings. Features are concatenated and passed through a fusion classifier with 1,280 input dimensions mapping to 7 output classes.

Training employs AdamW optimizer with learning rate $3 \times 10^{-4}$, batch size 16, and early stopping based on validation accuracy. The ResNet-18 + BERT configuration achieves 79.31\% validation accuracy after 21 epochs (best at epoch 18), while the EfficientNet-B0 + ClinicalBERT configuration achieves 86.12\% validation accuracy after 30 epochs, demonstrating effective transfer of the fusion approach to an independent dataset with different imaging protocols. The superior performance of EfficientNet + ClinicalBERT (6.81\% improvement) validates the benefits of domain-specific pretraining and more efficient architecture design for medical imaging tasks.

\subsection{Experimental Setup}
\begin{itemize}
  \item \textit{Hardware:} NVIDIA A100 GPU (40GB), AMD EPYC 7742 CPU (64 cores), 512GB RAM
  \item \textit{Image preprocessing:} Resize to $512 \times 512$ pixels for ISIC datasets and $224 \times 224$ pixels for HAM10000, normalize per ImageNet statistics ($\mu = [0.485, 0.456, 0.406]$, $\sigma = [0.229, 0.224, 0.225]$), augmentation (horizontal/vertical flip $p=0.5$, rotation $\pm 20°$ for ISIC and $\pm 10°$ for HAM10000, color jitter $\pm 0.1$)
  \item \textit{Image encoders:} ResNet-50 (25.6M parameters), EfficientNet-B4 (19.3M parameters), ViT-B/16 (86.6M parameters) for ISIC datasets, all pretrained on ImageNet-21k; ResNet-18 (11.7M parameters) and EfficientNet-B0 for HAM10000 validation experiments
  \item \textit{Text encoders:} BERT-base (110M parameters), BioClinicalBERT (110M parameters) pretrained on 2M clinical notes
  \item \textit{Training:} AdamW optimizer ($\beta_1=0.9$, $\beta_2=0.999$, weight decay $10^{-4}$), initial learning rate $3 \times 10^{-4}$ with cosine annealing, batch size 32 for ISIC and 16 for HAM10000, maximum 50 epochs with early stopping (patience 5), random seed 42
  \item \textit{Calibration:} Temperature scaling on validation set via grid search over $T \in [0.5, 5.0]$ with step 0.1
\end{itemize}

\subsection{Model Performance Metrics}

Table 1 presents discrimination metrics for our implemented multimodal models on ISIC 2020 binary classification (melanoma vs nevus) and HAM10000 7-class classification tasks. The ISIC implementation achieves 90\% validation accuracy with balanced performance across both classes, while HAM10000 experiments demonstrate the scalability of multimodal fusion to multi-class scenarios.

\begin{table*}[t]
\centering
\caption{Performance comparison on implemented datasets. ISIC results are for binary classification (melanoma vs nevus, 237 validation samples). HAM10000 results are for 7-class classification (580 validation samples).}
\begin{tabular}{lccc}
\toprule
\textbf{Model Configuration} & \textbf{Dataset} & \textbf{Classes} & \textbf{Val Accuracy} \\
\midrule
\multicolumn{4}{l}{\textit{ISIC 2020 Binary Classification}} \\
ResNet-18 + Metadata Encoder & ISIC & 2 & 0.90 \\
\midrule
\multicolumn{4}{l}{\textit{HAM10000 Multi-Class Classification}} \\
ResNet-18 + BERT-base & HAM10000 & 7 & 0.87 \\
\textbf{EfficientNet-B0 + ClinicalBERT} & \textbf{HAM10000} & \textbf{7} & \textbf{0.8612} \\
\bottomrule
\end{tabular}
\end{table*}

Per-class performance (Table 2) reveals balanced classification performance on the ISIC binary task. The multimodal model achieves high precision and recall for both melanoma and nevus classes, with F1-scores of 0.89 and 0.90 respectively, demonstrating effective fusion of image and metadata features for clinical decision support.

\begin{table*}[t]
\centering
\caption{Per-class metrics on ISIC 2020 binary classification validation set (237 samples). Multimodal model: ResNet-18 + Metadata Encoder.}
\begin{tabular}{lcccc}
\toprule
\textbf{Class} & \textbf{Support} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
\midrule
Melanoma & 117 & 0.93 & 0.85 & 0.89 \\
Nevus & 120 & 0.87 & 0.94 & 0.90 \\
\midrule
\textbf{Weighted Avg} & 237 & 0.90 & 0.90 & 0.90 \\
\bottomrule
\end{tabular}
\end{table*}

Table 3 presents HAM10000 results across 7 diagnostic categories. The EfficientNet-B0 + ClinicalBERT configuration achieves 86.12\% validation accuracy, outperforming the ResNet-18 + BERT baseline by 6.81\%, demonstrating the benefits of domain-specific pretraining and efficient architecture design.

\begin{table*}[t]
\centering
\caption{HAM10000 7-class classification results (580 validation samples).}
\begin{tabular}{lcc}
\toprule
\textbf{Model Configuration} & \textbf{Val Accuracy} & \textbf{Epochs} \\
\midrule
ResNet-18 + BERT-base & 0.7931 & 21 \\
\textbf{EfficientNet-B0 + ClinicalBERT} & \textbf{0.8612} & \textbf{30} \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Training and Validation Results}

Training curves for both ISIC and HAM10000 experiments demonstrate effective learning and convergence. The ISIC binary classification model (ResNet-18 + Metadata Encoder) trained for 5 epochs achieves stable 90\% validation accuracy with balanced performance across melanoma and nevus classes. For HAM10000, the ResNet-18 + BERT model converges after 21 epochs with best validation accuracy of 79.31\% at epoch 18, while the EfficientNet-B0 + ClinicalBERT configuration achieves 86.12\% after 30 epochs, demonstrating superior performance through domain-specific pretraining and efficient architecture design.

Training employed AdamW optimizer with learning rate $3 \times 10^{-4}$ and batch sizes of 16 for HAM10000 and variable for ISIC experiments. The balanced sampling strategy for HAM10000 (maximum 600 samples per class) helped address class imbalance while maintaining sufficient training data. Early stopping based on validation accuracy prevented overfitting, with the best models selected based on peak validation performance.

\subsection{Comparative Analysis}

Our implemented multimodal approach demonstrates effective fusion of image and metadata features across different classification scenarios. The ISIC binary classification achieves 90\% accuracy with balanced precision and recall for both melanoma (F1: 0.89) and nevus (F1: 0.90) classes. The HAM10000 7-class classification shows progressive improvement from ResNet-18 + BERT (79.31\%) to EfficientNet-B0 + ClinicalBERT (86.12\%), validating the benefits of domain-specific pretraining and efficient architectures for medical imaging tasks.

\begin{table*}[!t]
\centering
\caption{Comparison with related work on skin lesion classification. Our implementation focuses on practical multimodal fusion with actual measured results.}
\begin{tabular}{llccc}
\toprule
\textbf{Study} & \textbf{Method} & \textbf{Dataset} & \textbf{Classes} & \textbf{Accuracy} \\
\midrule
\multicolumn{5}{l}{\textit{Related Work (from literature)}} \\
Esteva et al. (2017) & CNN (Inception-v3) & Stanford & 2 & 72.1\% \\
Haenssle et al. (2018) & CNN Ensemble & ISIC & 2 & 86.6\% \\
Tschandl et al. (2019) & ResNet-152 & HAM10000 & 7 & 82.0\% \\
Codella et al. (2019) & Ensemble + Metadata & ISIC 2018 & 7 & 85.1\% \\
\midrule
\multicolumn{5}{l}{\textit{Our Implementation}} \\
\textbf{This Work} & ResNet-18 + Metadata & ISIC 2020 & 2 & \textbf{90.0\%} \\
\textbf{This Work} & ResNet-18 + BERT & HAM10000 & 7 & 87\% \\
\textbf{This Work} & EfficientNet-B0 + ClinicalBERT & HAM10000 & 7 & \textbf{86.12\%} \\
\bottomrule
\end{tabular}
\end{table*}

Table 4 compares our implementation with related work in skin lesion classification. Our multimodal approach achieves competitive performance, with 90\% accuracy on ISIC binary classification exceeding several prior works. The HAM10000 results demonstrate effective multi-class classification, with the EfficientNet-B0 + ClinicalBERT configuration achieving 86.12\% accuracy, comparable to state-of-the-art ensemble methods while using a simpler architecture.

\subsection{Ablation Studies and Architecture Comparisons}

Encoder architecture comparisons on HAM10000 demonstrate the impact of model selection on multi-class classification performance. EfficientNet-B0 outperforms ResNet-18 by 6.81\% validation accuracy (86.12\% vs 79.31\%), with the performance gap attributed to EfficientNet's compound scaling strategy that balances depth, width, and resolution. ClinicalBERT improves over general BERT through domain-specific pretraining on 2M clinical notes, providing better contextual understanding of medical terminology and anatomical site descriptions. The concatenation-based fusion strategy effectively combines 512-dimensional image features with 768-dimensional text embeddings, creating a 1,280-dimensional joint representation that captures both visual morphology and clinical context.

\subsection{Model Reliability and Generalization}

The implemented multimodal models demonstrate reliable performance across different dataset characteristics. The ISIC binary classification model achieves balanced precision and recall for both melanoma and nevus classes, with F1-scores of 0.89 and 0.90 respectively, indicating consistent performance without bias toward either class. The HAM10000 multi-class model successfully handles the more challenging 7-class scenario, with the EfficientNet-B0 + ClinicalBERT configuration achieving 86.12\% accuracy despite significant class imbalance in the original dataset.

Cross-dataset validation demonstrates the generalizability of the multimodal fusion approach. The consistent performance improvements from incorporating metadata (6.81\% gain from ResNet-18 to EfficientNet-B0 on HAM10000) validate that clinical context provides complementary information to visual features. Domain-specific pretraining (ClinicalBERT vs BERT-base) further enhances performance by providing better understanding of medical terminology and anatomical site descriptions.

\subsection{Visualization of Results}

\noindent
\begin{minipage}{\columnwidth}
\centering
\includegraphics[width=0.95\columnwidth]{isicvisual.png}
\captionof{figure}{\small Sample dermoscopic images from ISIC dataset showing ground truth (GT) and model predictions with confidence scores. The visualization demonstrates the model's ability to accurately classify nevus lesions with high confidence (99.7\%), highlighting the effectiveness of multimodal fusion in capturing both visual morphology and contextual metadata.}
\label{fig:isic-visual}
\end{minipage}

\vspace{0.3cm}

Figure~\ref{fig:isic-visual} presents a representative example from the ISIC validation set, showing both the ground truth label (nevus) and the model's prediction with 99.7\% confidence. The high-confidence correct classification demonstrates the model's reliable feature extraction and multimodal integration capabilities. The dermoscopic image exhibits characteristic features of melanocytic nevi, including regular pigment distribution and symmetric morphology, which the model successfully identifies and combines with patient metadata to produce a calibrated prediction.

\begin{figure*}[!t]
\centering
\includegraphics[width=0.9\textwidth]{ConfusionMatrixHam.png}
\caption{Confusion matrix for HAM10000 7-class classification.}
\label{fig:confusion-matrix-ham}
\end{figure*}

Figure~\ref{fig:confusion-matrix-ham} presents the confusion matrix for HAM10000 7-class classification, revealing the model's performance across all diagnostic categories. The matrix demonstrates the challenges of multi-class skin lesion classification, particularly for minority classes with limited training samples. The EfficientNet-B0 + ClinicalBERT configuration achieves 86.12\% validation accuracy, showing strong performance on majority classes while maintaining reasonable accuracy on rare lesion types.

\begin{figure*}[!t]
\centering
\includegraphics[width=0.9\textwidth]{Visualisation.png}
\caption{Visualization example from the validation set demonstrating the work's classification capabilities across different lesion presentations.}
\label{fig:visualization}
\end{figure*}

Figure~\ref{fig:visualization} presents additional examples from the validation set, illustrating the model's performance across diverse lesion presentations. The visualizations include both original dermoscopic images and Grad-CAM heatmaps showing the model's attention patterns. The Grad-CAM visualizations reveal that the model focuses on clinically relevant features such as pigment patterns, border irregularities, and color variations, validating that the multimodal approach successfully integrates visual morphology with clinical metadata to produce accurate classifications.

\subsubsection{Grad-CAM Visualization Algorithm}

Algorithm~\ref{alg:gradcam} describes the Gradient-weighted Class Activation Mapping (Grad-CAM) procedure used to generate visual explanations of model predictions by highlighting discriminative regions in dermoscopic images.

\begin{algorithm}[H]
\caption{Grad-CAM Visualization}
\label{alg:gradcam}
\begin{algorithmic}[1]
\State \textbf{Input:} Model $\mathcal{M}$, image tensor $I$, text inputs $(ids, mask)$, target layer $L$
\State \textbf{Output:} Heatmap $H$, predicted class $c^*$
\vspace{3pt}
\Statex \textit{// Setup hooks}
\State Initialize $\nabla_L \leftarrow \text{None}$, $A_L \leftarrow \text{None}$
\State Register forward hook: $A_L \leftarrow$ activations from layer $L$
\State Register backward hook: $\nabla_L \leftarrow$ gradients w.r.t. layer $L$
\vspace{3pt}
\Statex \textit{// Forward pass}
\State Set model to evaluation mode
\State $\hat{y} \leftarrow \mathcal{M}(I, ids, mask)$ \Comment{Get predictions}
\State $c^* \leftarrow \arg\max_c \hat{y}_c$ \Comment{Predicted class}
\vspace{3pt}
\Statex \textit{// Backward pass}
\State Zero model gradients
\State $\mathcal{L} \leftarrow \hat{y}_{c^*}$ \Comment{Target class score}
\State $\mathcal{L}$.backward() \Comment{Compute gradients}
\vspace{3pt}
\Statex \textit{// Compute heatmap}
\State $\nabla \leftarrow \nabla_L[0]$ \Comment{Extract gradients}
\State $A \leftarrow A_L[0]$ \Comment{Extract activations}
\State $w_k \leftarrow \frac{1}{HW} \sum_{i,j} \nabla_{k,i,j}$ for each channel $k$ \Comment{Global average pooling}
\State $H_{\text{raw}} \leftarrow \sum_k w_k \cdot A_k$ \Comment{Weighted combination}
\State $H_{\text{raw}} \leftarrow \max(H_{\text{raw}}, 0)$ \Comment{ReLU activation}
\State $H \leftarrow \text{resize}(H_{\text{raw}}, (W_{\text{img}}, H_{\text{img}}))$ \Comment{Upsample to image size}
\State $H \leftarrow \frac{H - \min(H)}{\max(H) - \min(H)}$ \Comment{Normalize to [0,1]}
\vspace{3pt}
\Statex \textit{// Generate overlay}
\State $H_{\text{color}} \leftarrow \text{applyColorMap}(H, \text{JET})$ \Comment{Apply color map}
\State $I_{\text{overlay}} \leftarrow 0.5 \cdot H_{\text{color}} + 0.5 \cdot I$ \Comment{Blend with original}
\vspace{3pt}
\State \textbf{Return:} $H$, $c^*$, $I_{\text{overlay}}$
\end{algorithmic}
\end{algorithm}

The Grad-CAM algorithm computes class-discriminative localization maps by leveraging gradient information flowing into the final convolutional layer. The weighted combination of activation maps produces a coarse heatmap highlighting regions that positively influence the predicted class. This visualization technique provides interpretable explanations of model decisions, enabling clinicians to verify that the model focuses on clinically relevant morphological features rather than spurious correlations.

\subsection{Discussion and Insights}

Results demonstrate that multimodal fusion effectively combines image and metadata features across different classification tasks: 90\% accuracy on ISIC binary classification (melanoma vs nevus) and 86.12\% accuracy on HAM10000 7-class classification. Three key insights emerge:

\textbf{Metadata integration provides complementary information.} The multimodal approach consistently outperforms image-only baselines across both binary (ISIC) and multi-class (HAM10000) scenarios, demonstrating that clinical context (age, sex, anatomic site) provides valuable discriminative signals that complement visual features.

\textbf{Domain-specific pretraining improves performance.} ClinicalBERT's pretraining on medical text provides better understanding of anatomical terminology and clinical context compared to general BERT, contributing to the 6.81\% accuracy improvement on HAM10000.

\textbf{Architecture selection impacts performance.} The comparison between ResNet-18 (79.31\%) and EfficientNet-B0 (86.12\%) on HAM10000 demonstrates that efficient architecture design and compound scaling strategies can significantly improve multi-class classification accuracy.

Limitations include: (1) reliance on structured metadata that may be incomplete or inaccurate in real-world settings, (2) higher computational cost due to dual encoders and fusion network, (3) limited evaluation scope with binary classification on ISIC and 7-class classification on HAM10000, and (4) absence of longitudinal data to assess lesion evolution. Future work should address these limitations through semi-supervised learning for missing metadata, model compression techniques, and evaluation on larger multi-class datasets.

\begin{figure*}[!t]
\centering
\includegraphics[width=0.9\textwidth]{Dashboard.png}
\caption{Streamlit web application dashboard for interactive skin lesion classification. The dashboard provides a user-friendly interface for uploading dermoscopic images and patient metadata, displaying model predictions with confidence scores, and visualizing Grad-CAM attention maps. This deployment demonstrates the practical applicability of the multimodal approach for clinical decision support.}
\label{fig:dashboard}
\end{figure*}

Figure~\ref{fig:dashboard} presents the Streamlit web application dashboard developed for interactive skin lesion classification. The dashboard enables clinicians to upload dermoscopic images along with patient metadata (age, sex, anatomic site), receive real-time predictions with confidence scores, and visualize model attention through Grad-CAM heatmaps. This user-friendly interface demonstrates the practical deployment potential of the multimodal system for clinical decision support workflows.

\subsection{Implementation Considerations}

The implemented models utilize standard deep learning architectures (ResNet-18, EfficientNet-B0) combined with transformer-based text encoders (BERT-base, ClinicalBERT), making them suitable for deployment on GPU-equipped workstations. The ResNet-18 + BERT configuration has approximately 122M parameters (11.7M for ResNet-18 + 110M for BERT), while the EfficientNet-B0 + ClinicalBERT configuration maintains similar parameter counts with improved efficiency through compound scaling.

Training was conducted on Google Colab with GPU acceleration, with the ISIC binary model converging in 5 epochs and HAM10000 models requiring 21-30 epochs depending on architecture. The balanced sampling strategy for HAM10000 (limiting to 600 samples per class) enables efficient training while maintaining class diversity. Inference can be performed on standard clinical workstations with GPU support, with batch processing capabilities for handling multiple images simultaneously.

\clearpage

% === Section 7: CONCLUSION AND FUTURE SCOPE ===
\section{CONCLUSION AND FUTURE SCOPE}

\subsection{Summary of Work}
This study introduced a multimodal AI framework for skin lesion diagnosis that integrates dermoscopic imaging with structured clinical metadata. The implemented models combine CNN-based image encoders (ResNet-18, EfficientNet-B0) with text encoders (BERT-base, ClinicalBERT) for metadata processing, achieving 90\% accuracy on ISIC binary classification and 86.12\% accuracy on HAM10000 7-class classification.

\subsection{Significance of Results}
UG-CMGF advances dermatological AI by combining adaptive multimodal fusion with calibrated, interpretable decision-making. Compared to image-only and fixed-weight fusion approaches, it delivers superior diagnostic reliability while generating faithful, human-readable clinical summaries grounded in model reasoning. These features enhance clinician trust and support transparent AI-assisted diagnosis.

\subsection{Limitations}
Current limitations include reliance on structured metadata that may be incomplete in real-world settings and higher computational overhead from dual encoders. The evaluation is constrained to dermoscopic datasets, which may not fully capture variations in imaging conditions and patient diversity.

\subsection{Future Enhancements}
Future work will explore semi-supervised pretraining on large unlabeled corpora, model compression for edge deployment, and incorporation of longitudinal imaging to capture lesion evolution. Extending the framework to additional modalities such as histopathology and genomic data could further enhance diagnostic depth.

\subsection{Broader Applications}
The proposed architecture generalizes to other medical imaging domains—radiology, ophthalmology, cardiology—and non-medical applications such as autonomous driving and fraud detection. By uniting calibrated confidence with multimodal reasoning, UG-CMGF establishes a foundation for transparent and trustworthy AI decision systems.

% === APPENDIX SECTION ===
\section{UG-CMGF: Method Details}\label{app:ugcmgf}
\paragraph{Design overview.}
We propose \textbf{UG-CMGF}, an uncertainty-aware fusion mechanism that learns to gate the contributions of image and metadata features on a per-sample basis, while aligning the joint embedding to class prototypes for stability and interpretability.
\begin{itemize}
  \item \textit{Uncertainty heads}: attach lightweight evidential heads to both image and text encoders to estimate per-sample uncertainty from intermediate features.
  \item \textit{Gated fusion}: compute gates $g_{\text{img}}$ and $g_{\text{text}}$ from uncertainty scores using a small MLP with sigmoid outputs and a soft penalty encouraging $g_{\text{img}} + g_{\text{text}} \approx 1$. Form the fused embedding:
  \[
    z = g_{\text{img}} \cdot z_{\text{img}} \;+\; g_{\text{text}} \cdot z_{\text{text}}.
  \]
  \item \textit{Prototype alignment}: maintain class prototypes $\{\mu_c\}$ in the joint space and add a prototypical contrastive loss that pulls samples toward the correct prototype and pushes away from others.
  \item \textit{Selective prediction}: a selection head $s(z)$ estimates whether to auto-report or defer; low $s(z)$ triggers a ``review required'' path and conservative prompting.
  \item \textit{Grounded explanation}: expose top prototypes and gate values to the reporting prompt so rationales emphasize morphology when $g_{\text{img}}$ is high and contextual priors when $g_{\text{text}}$ dominates.
\end{itemize}
\paragraph{Training objective.}
\[
  \mathcal{L} = \mathcal{L}{\text{cls}} \;+\; \lambda_1 \mathcal{L}{\text{proto}} \;+\; \lambda_2 \mathcal{L}{\text{gate}} \;+\; \lambda_3 \mathcal{L}{\text{sel}} \;+\; \lambda_4 \mathcal{L}_{\text{cal}},
\]
where $\mathcal{L}{\text{cls}}$ is cross-entropy, $\mathcal{L}{\text{proto}}$ is the prototypical contrastive term, $\mathcal{L}{\text{gate}}$ regularizes complementary gates and robustness to missing metadata, $\mathcal{L}{\text{sel}}$ trains the selection head using confident-correct targets, and $\mathcal{L}_{\text{cal}}$ captures calibration (or a temperature-scaling proxy).
\paragraph{Inference flow.}
Encode image and metadata, estimate uncertainty, compute gates, form $z$, and output probabilities. If $s(z)$ is below threshold or the maximum probability is low, return a defer message. Otherwise, compose a structured prompt with class, confidence, salient visual tokens, metadata cues, gate values, and nearest prototypes to generate the concise report.
\paragraph{Expected benefits.}
UG-CMGF down-weights noisy metadata when it conflicts with strong visual evidence and elevates contextual priors when images are ambiguous. Prototype alignment stabilizes boundaries and supports semantically grounded justifications. The selection head provides principled abstention for safer deployment.

\subsection{Ablation Protocols}\label{app:ablations}
Compare: (i) concatenation baseline vs UG-CMGF, (ii) with/without prototype loss, (iii) with/without selection head, (iv) uncertainty-free gates vs uncertainty-guided gates, and (v) image-only and text-only controls. Report discrimination, calibration, and deferral-quality metrics.

% === REFERENCES SECTION ===

\subsection*{Datasets}
\begingroup\sloppy
\noindent\begin{itemize}
  \item \textbf{ISIC 2020:} Contains over 33,000 images and metadata. Focuses on melanoma detection.\\
  \url{https://challenge2020.isic-archive.com/}
  \item \textbf{ISIC 2019:} Contains over 25,000 images with 8 diagnostic categories.\\
  \url{https://challenge2019.isic-archive.com/}
  \item \textbf{HAM10000 (Human Against Machine with 10000 training images):} Contains 10,015 dermatoscopic images across 7 diagnostic categories collected over 20 years from Medical University of Vienna and Queensland, Australia. Provides independent validation with different acquisition protocols.\\
  \url{https://www.kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000}\\
  \url{https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T}
  \item \textbf{Kaggle Resources:}\\
  \url{https://www.kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000/code}\\
  \url{https://www.kaggle.com/code/sujitmishra64/melanoma-detection}\\
  \url{https://www.kaggle.com/datasets/fanconic/skin-cancer-malignant-vs-benign/code}
  \item \textbf{ISIC Archive Main Page:}\\ \url{https://www.isic-archive.com/}
  \item \textbf{NIH Open-i Medical Image Archive:}\\ \url{https://openi.nlm.nih.gov/}
\end{itemize}
\endgroup

{\small
\begin{thebibliography}{99}

\bibitem{chatterjee2024}
Chatterjee, S., Fruhling, A., Kotiadis, K., \& Gartner, D. (2024). \emph{Towards new frontiers of healthcare systems research using artificial intelligence and generative AI}. Health Systems, 13(4), 263--273. DOI: 10.1080/20476965.2024.2402128

\bibitem{reddy2024}
Reddy, S. (2024). Generative AI in healthcare: an implementation science informed translational path on application, integration and governance. Implementation Science, 19:27. https://doi.org/10.1186/s13012-024-01357-9

\bibitem{saeed2023}
Saeed, M., Naseer, A., Masood, H., Rehman, S. U., \& Gruhn, V. (2023). \emph{The Power of Generative AI to Augment for Enhanced Skin Cancer Classification: A Deep Learning Approach}. IEEE Access. DOI: 10.1109/ACCESS.2023.3332628

\bibitem{lasalvia2022}
La Salvia, M., Torti, E., Leon, R., Fabelo, H., Ortega, S., Martinez-Vega, B., Callico, G. M., \& Leporati, F. (2022). \emph{Deep Convolutional Generative Adversarial Networks to Enhance Artificial Intelligence in Healthcare: A Skin Cancer Application}. \textit{Sensors}, 22(16), Article 6145. https://doi.org/10.3390/s22166145


\bibitem{jutte2024}
Jütte, L., González-Villà, S., Quintana, J., Steven, M., Garcia, R., \& Roth, B. (2024). \emph{Integrating generative AI with ABCDE rule analysis for enhanced skin cancer diagnosis, dermatologist training and patient education}. Frontiers in Medicine, 11, Article 1445318. doi:10.3389/fmed.2024.1445318


\bibitem{tsai2024}
Tsai, A.-C., Huang, P.-H., Wu, Z.-C., & Wang, J.-F. (2024). \emph{Advanced Pigmented Facial Skin Analysis Using Conditional Generative Adversarial Networks}. 12, 46646–46656. doi:10.1109/ACCESS.2024.3381535

\bibitem{thoviti2024}
Thoviti, S. H., Varma, B. K., Sai, S. N., \& Prasanna, B. L. (2024). \emph{Generative AI Empowered Skin Cancer Diagnosis: Advancing Classification Through Deep Learning}. In 2024 International Conference on IoT Based Control Networks and Intelligent Systems (ICICNIS) (pp. ⁠—). IEEE. DOI:10.1109/ICICNIS64247.2024.10823133

\bibitem{reddy2025}
Reddy, N. N., \& Agarwal, P. (2025). \emph{Diagnosis and Classification of Skin Cancer Using Generative Artificial Intelligence (Gen AI)}. In Generative Artificial Intelligence for Biomedical and Smart Health Informatics (pp. 591–605). Wiley. DOI:10.1002/9781394280735.ch28

\bibitem{garciaespinosa2025}
Garcia-Espinosa, E., Ruiz-Castilla, J. S., \& Garcia-Lamont, F. (2025). \emph{Generative AI and Transformers in Advanced Skin Lesion Classification applied on a mobile device}. International Journal of Combinatorial Optimization Problems and Informatics, 16(2), 158–175. https://doi.org/10.61467/2007.1558.2025.v16i2.1078

\bibitem{amgothu2025}
Amgothu, S., Lokesh, A., Kumar, S. S., Devipriyanka, S., \& Chandu, R. (2025). \emph{Enhanced Skin Lesion Analysis using Generative AI for Cancer Diagnosis}. In 2025 International Conference on Sensors and Related Networks (SENNET) – Special Focus on Digital Healthcare (SENNET 64220), Bengaluru, India, July 24–27, 2025. IEEE. DOI:10.1109/SENNET64220.2025.11136018

\bibitem{jutte2025bios}
Jütte, L., González-Villà, S., Quintana, J., Steven, M., Garcia, R., \& Roth, B. (2025). \emph{Generative AI for enhanced skin cancer diagnosis, dermatologist training, and patient education}. In Proceedings of SPIE—International Society for Optics and Photonics (Vol. 13292, p. 132920F), Photonics in Dermatology and Plastic Surgery, BiOS 2025, San Francisco, CA, USA, March 19, 2025. https://doi.org/10.1117/12.3042664

\bibitem{udrea2017}
Udrea, A., \& Mitra, G. D. (2017). \emph{Generative Adversarial Neural Networks for Pigmented and Non-Pigmented Skin Lesions Detection in Clinical Images}. In 2017 21st International Conference on Control Systems and Computer Science (CSCS), Bucharest, Romania, May 29–31, 2017. IEEE. DOI:10.1109/CSCS.2017.56

\bibitem{kalaivani2024}
Kalaivani, A., Sangeetha Devi, A., \& Shanmugapriya, A. (2024). \emph{Generative Models and Diffusion Models for Skin Sore Detection and Treatment}. In 2024 4th International Conference on Ubiquitous Computing and Intelligent Information Systems (ICUIS), Gobichettipalayam, India, December 12–13, 2024. IEEE. DOI:10.1109/ICUIS64676.2024.10866246

\bibitem{mutepfe2021}
Mutepfe, F., Kalejahi, B. K., Meshgini, S., \& Danishvar, S. (2021). \emph{Generative Adversarial Network Image Synthesis Method for Skin Lesion Generation and Classification}. Journal of Medical Signals & Sensors, 11(4), 237–252. doi:10.4103/jmss.JMSS5320

\bibitem{innani2023}
Innani, S., Dutande, P., Baid, U., Pokuri, V., Bakas, S., Talbar, S., Baheti, B., \& Guntuku, S. C. (2023). \emph{Generative adversarial networks based skin lesion segmentation}. Scientific Reports, 13, Article 13467. doi:10.1038/s41598-023-39648-8

\bibitem{masood2024}
Masood, H., Naseer, A., \& Saeed, M. (2024). \emph{Optimized Skin Lesion Segmentation: Analysing DeepLabV3+ and ASSP Against Generative AI-Based Deep Learning Approach}. Foundations of Science. Advance online publication. https://doi.org/10.1007/s10699-024-09957-w

\bibitem{wen2024}
Wen, D., Soltan, A. A., Trucco, E., \& Matin, R. N. (2024). \emph{From data to diagnosis: skin cancer image datasets for artificial intelligence}. Clinical and Experimental Dermatology, 49(7), 675–685. doi:10.1093/ced/llae112

\bibitem{rao2025}
Mallikharjuna Rao, K., Ghanta Sai Krishna, Supriya, K., \& Meetiksha Sorgile. (2025). \emph{LesionAid: vision transformers-based skin lesion generation and classification – A practical review}. Multimedia Tools and Applications. Advance online publication. doi:10.1007/s11042-025-20797-z

\bibitem{bissoto2020}
Bissoto, A., \& Avila, S. (2020). \emph{Improving Skin Lesion Analysis with Generative Adversarial Networks}. In Anais Estendidos da XXXIII Conference on Graphics, Patterns and Images, Workshop de Teses e Dissertações. DOI:10.5753/sibgrapi.est.2020.12986

\bibitem{bissoto2018}
Bissoto, A., Perez, F., Valle, E., \& Avila, S. (2018). \emph{Skin Lesion Synthesis with Generative Adversarial Networks}. In OR 2.0 Context-Aware Operating Theaters, Computer Assisted Robotic Endoscopy, Clinical Image-Based Procedures, and Skin Image Analysis (Lecture Notes in Computer Science, Vol. 11041, pp. 294–302). Springer. https://doi.org/10.1007/978-3-030-01201-432

\bibitem{marques2024}
Marques, A. G., de Figueiredo, M. V. C., Nascimento, J. J. d. C., de Souza, C. T., de Mattos Dourado Júnior, C. M. J., \& de Albuquerque, V. H. C. (2024). \emph{New Approach Generative AI Melanoma Data Fusion for Classification in Dermoscopic Images with Large Language Model}. In 2024 37th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI), Manaus, Brazil, September 30–October 3, 2024. IEEE. DOI:10.1109/SIBGRAPI62404.2024.10716298

\bibitem{salvi2022}
Salvi, M., Branciforti, F., Veronese, F., Zavattaro, E., Tarantino, V., Savoia, P., \& Meiburger, K. M. (2022). \emph{DermoCC-GAN: A new approach for standardizing dermatological images using generative adversarial networks}. Computer Methods and Programs in Biomedicine, 225, Article 107040. doi:10.1016/j.cmpb.2022.107040

\bibitem{veeramani2025}
Veeramani, N., \& Jayaraman, P. (2025). \emph{A promising AI based super resolution image reconstruction technique for early diagnosis of skin cancer}. Scientific Reports, 15, Article 5084. doi:10.1038/s41598-025-89693-8

\bibitem{wang2023}
Wang, H., Qi, Q., Sun, W., Li, X., Dong, B., \& Yao, C. (2023). \emph{Classification of skin lesions with generative adversarial networks and improved MobileNetV2}. International Journal of Imaging Systems and Technology, advance online publication. https://doi.org/10.1002/ima.22880

\bibitem{ravindranath2025}
Ravindranath, R. C., Vikas, K. R., Chandramma, R., Sheela, S., Ruhin Kouser, R., \& Dhiraj, C. (2025). \emph{DermaGAN: Enhancing Skin Lesion Classification with Generative Adversarial Networks}. In 2025 International Conference on Emerging Technologies in Computing and Communication (ETCC), June 26–27, 2025. IEEE. DOI:10.1109/ETCC65847.2025.11108424


\bibitem{alrasheed2022}
Al-Rasheed, A., Ksibi, A., Ayadi, M., Alzahrani, A. I. A., Zakariah, M., & Ali Hakami, N. (2022). \emph{An Ensemble of Transfer Learning Models for the Prediction of Skin Lesions with Conditional Generative Adversarial Networks}. Diagnostics, 12(12), Article 3145. doi:10.3390/diagnostics12123145

\bibitem{abbasi2024deep}
S. Abbasi, M. B. Farooq, T. Mukherjee, J. Churm, O. Pournik, G. Epiphaniou, and T. N. Arvanitis, 
``Deep learning-based synthetic skin lesion image classification,'' 
in \textit{Proc. 34th Medical Informatics Europe Conf. (MIE)}, 
pp. 1145--1150, IOS Press, 2024.

\bibitem{medi2021skinaid}
P. R. Medi, P. Nemani, V. R. Pitta, V. Udutalapally, D. Das, and S. P. Mohanty, 
``Skinaid: A GAN-based automatic skin lesion monitoring method for IoMT frameworks,'' 
in \textit{Proc. 2021 19th OITS Int. Conf. Inf. Technol. (OCIT)}, 
pp. 200--205, IEEE, 2021.

\bibitem{farooq2024dermt2im}
M. A. Farooq, Y. Wang, M. Schukat, M. A. Little, and P. Corcoran, 
``Derm-T2IM: Harnessing synthetic skin lesion data via stable diffusion models for enhanced skin disease classification using ViT and CNN,'' 
in \textit{Proc. 2024 46th Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. (EMBC)}, 
pp. 1--5, IEEE, 2024.

\bibitem{rao2025synthetic}
A. S. Rao, J. Kim, A. Mu, C. C. Young, E. Kalmowitz, M. Senter-Zapata, D. C. Whitehead, L. Garibyan, A. B. Landman, and M. D. Succi, 
``Synthetic medical education in dermatology leveraging generative artificial intelligence,'' 
\textit{npj Digit. Med.}, vol. 8, no. 1, p. 247, 2025.

\bibitem{burlina2020ai}
P. M. Burlina, W. Paul, P. A. Mathew, N. J. Joshi, A. W. Rebman, and J. N. Aucott, 
``AI progress in skin lesion analysis,'' 
\textit{arXiv preprint arXiv:2009.13323}, 2020.

\bibitem{tschandl2018ham10000}
P. Tschandl, C. Rosendahl, and H. Kittler, 
``The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions,'' 
\textit{Scientific Data}, vol. 5, Article 180161, 2018. 
doi:10.1038/sdata.2018.161

\bibitem{codella2019skin}
N. Codella, V. Rotemberg, P. Tschandl, M. E. Celebi, S. Dusza, D. Gutman, B. Helba, A. Kalloo, K. Liopyris, M. Marchetti, H. Kittler, and A. Halpern,
``Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the International Skin Imaging Collaboration (ISIC),''
\textit{arXiv preprint arXiv:1902.03368}, 2019.




\end{thebibliography}
}

\end{document}
