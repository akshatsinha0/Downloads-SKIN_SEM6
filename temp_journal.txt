% !TeX program = lualatex
% Elsevier CAS double-column layout (A4) with side rails
\documentclass[a4paper,fleqn]{cas-dc}

% Citation style
\usepackage[numbers,sort&compress]{natbib}

% Core packages
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{xurl}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{microtype}
\usepackage{soul}
\sodef\spaced{}{.2em}{.6em plus.1em}{1em plus.1em minus.1em}
\Urlmuskip=0mu plus 2mu
% Hyperlinks and reference colors (load last)
\usepackage[colorlinks=true,linkcolor=black,citecolor=blue,urlcolor=blue,breaklinks=true]{hyperref}
% Make the entire bibliography blue
\usepackage{etoolbox}
% Blue bibliography entries only; black heading/labels
\makeatletter
\renewcommand{\bibsection}{\section*{\textcolor{black}{References}}\addcontentsline{toc}{section}{References}\color{blue}}
\renewcommand{\NAT@bibnumfmt}[1]{\textcolor{black}{[#1]}}
\renewcommand{\@biblabel}[1]{\textcolor{black}{[#1]}}
% Italic (not bold) for subsection/subsubsection headings
\renewcommand\subsection{\@startsection{subsection}{2}{\z@}{3.25ex \@plus1ex \@minus .2ex}{1.5ex \@plus .2ex}{\normalfont\itshape}}
\renewcommand\subsubsection{\@startsection{subsubsection}{3}{\z@}{3.25ex \@plus1ex \@minus .2ex}{1.5ex \@plus .2ex}{\normalfont\itshape}}
% Remove "Preprint submitted to Elsevier" from footer
\def\ps@pprintTitle{%
  \let\@oddhead\@empty
  \let\@evenhead\@empty
  \def\@oddfoot{\footnotesize\itshape
       \hfill\thepage}%
  \let\@evenfoot\@oddfoot}
\makeatother
% Increase letter spacing globally for better readability
\SetTracking{encoding=*}{160}

\begin{document}

\shorttitle{A Multimodal Generative AI System for Skin Lesion Diagnosis and Explanation}
\shortauthors{}
\title[mode=title]{A Multimodal Generative AI System for Skin Lesion Diagnosis and Explanation}

% Authors and affiliation (Elsevier CAS)
\author[inst1]{Akshat Sinha}
\cormark[1]
\ead{akshat.sinha2022@vitstudent.ac.in}
\author[inst1]{Arnav Sinha}
\ead{arnav.sinha2022@vitstudent.ac.in}
\author[inst1]{Aman Chauhan}
\ead{aman.chauhan2022@vitstudent.ac.in}
\author[inst2]{Naga Priyadarshini R}

\cortext[cor1]{Corresponding author}
\affiliation[inst1]{organization={Department of Computer Science and Engineering (CSE Core), School of Computer Science and Engineering (SCOPE), Vellore Institute of Technology},city={Vellore},postcode={632014},state={Tamil Nadu},country={India}}
\affiliation[inst2]{organization={Department of Analytics, School of Computer Science and Engineering (SCOPE), Vellore Institute of Technology},city={Vellore},postcode={632014},state={Tamil Nadu},country={India}}
\tnotetext[fn1]{Under the supervision of Dr. Naga Priyadarshini R, Assistant Professor Sr. Grade 1, Department of Analytics, School of Computer Science and Engineering (SCOPE), VIT, India.}

\begin{abstract}
Skin cancer accounts for millions of cases annually, with melanoma responsible for a disproportionate share of mortality. This study presents a two‑stage multimodal assistant that fuses dermoscopic images with structured clinical metadata (age, sex, anatomic site) to deliver calibrated diagnostic probabilities and concise clinician‑style reports. The discriminative module combines CNN/efficient backbones (ResNet‑18, EfficientNet‑B0) with text encoders (BERT‑base, ClinicalBERT); embeddings are concatenated and classified with temperature scaling for probability calibration. On ISIC 2020 (melanoma vs nevus; 237 validation samples) the system attains 90.0\% validation accuracy; on HAM10000 (7 classes; 580 validation samples) the EfficientNet‑B0 + ClinicalBERT configuration achieves 86.12\% accuracy. Post‑hoc calibration yields Expected Calibration Error $<0.05$. The generative reporter converts predicted class, calibrated confidence, and salient cues into 150–200 word explanations with 87\% faithfulness to model attention. Single‑image latency is $<500$ ms on CPU and $\sim$100 ms on GPU. These results indicate that calibrated multimodal fusion improves discrimination and transparency over image‑only baselines, providing a practical path toward reliable dermatology decision support.
\end{abstract}

\begin{keywords}
Machine learning\\
Healthcare\\
Skin cancer\\
Ensemble method\\
Benign\\
Malignant\\
Medical imaging\\
K-fold\\
Hyperparameter tuning\\
Diagnostic accuracy
\end{keywords}

\maketitle


% ================== Main content ==================
\section{INTRODUCTION}
Skin cancer represents one of the most prevalent malignancies worldwide, with melanoma accounting for approximately 75\% of skin cancer deaths despite comprising only 4\% of cases. Early detection significantly improves survival rates, with five-year survival exceeding 99\% for localized melanoma but dropping below 30\% for metastatic disease. Clinical assessment of pigmented lesions combines dermoscopic patterns with succinct patient context (age, sex, anatomic site)~\cite{tsai2024,thoviti2024}. However, current diagnostic workflows exhibit inter-observer variability ranging from 0.4 to 0.7 Cohen's kappa across studies, highlighting the need for reliable decision support systems.

Single-modality systems often miss these complementary signals, and opaque predictions undermine clinician trust and adoption~\cite{reddy2025}. This work introduces a two-stage assistant for dermoscopy: (i) a diagnostic engine that fuses image and metadata embeddings to produce calibrated class probabilities, and (ii) a generative reporter that converts the prediction and context into a concise clinician-style note with justification, differentials, and suggested next steps~\cite{garciaespinosa2025}. In both design and evaluation, the emphasis is on accuracy, calibration, and auditability so the tool functions as a reliable second opinion without displacing clinical judgment. The proposed approach includes a modular training/evaluation protocol, an uncertainty-aware fusion variant, and a controlled prompting template that preserves faithfulness to the discriminative output.

\subsection{Background}
Dermoscopy reveals morphological patterns (e.g., pigment networks, streaks, globules, vascular structures) that benefit from deep visual features, while age, sex, and lesion site shift pre-test probabilities and help disambiguate similar appearances~\cite{amgothu2025}. Multimodal learning unifies these signals via an image encoder for morphology and a text encoder for context, combined through a transparent fusion for classification~\cite{jutte2025bios}. Large public cohorts enable comprehensive training, cross-dataset validation, and error analysis by subgroups~\cite{wen2024}. Grounded generative models, when constrained by structured outputs and seeded with salient cues, can produce short, reviewable notes that surface the underlying rationale and improve documentation efficiency~\cite{udrea2017}.

\subsection{Motivations}

Earlier, more consistent triage requires calibrated probabilities paired with clear, structured wording that can be reused in clinical notes and referrals. Traditional AI diagnostic tools often produce uncalibrated outputs, making it difficult for clinicians to interpret prediction confidence and integrate results into decision-making workflows. In contrast, the proposed approach emphasizes probabilistic calibration, ensuring that model-predicted likelihoods align with empirical outcome frequencies, which is crucial for risk-sensitive domains such as dermatology.

A modular two-stage design—where encoders and reporters function as separable but interoperable units—provides both theoretical and operational advantages. From a theoretical perspective, modularity aligns with the principle of composability in neural architectures, allowing independent optimization of representation learning (in the encoder) and linguistic realization (in the reporter). This decoupling reduces catastrophic interference between modalities and enables continuous model improvement without retraining the entire system. Practically, it simplifies auditability, supports incremental deployment, and maintains compatibility with established clinical documentation pipelines.

Moreover, by explicitly logging intermediate reasoning steps and exposing uncertainty estimates, the system facilitates both interpretability and accountability. Uncertainty quantification—through entropy measures or Bayesian approximations—allows clinicians to adjust decision thresholds dynamically based on confidence, enhancing patient safety and model reliability. In this way, the system not only performs triage but also functions as an explainable collaborator that supports data-driven quality improvement and calibrated clinical decision-making.


\subsection{Problem Definition and Research Gap}

Skin cancer represents one of the most prevalent malignancies worldwide, with melanoma accounting for approximately 75\% of skin cancer deaths despite comprising only 4\% of cases~\cite{kalaivani2024}. Early detection significantly improves survival rates, with five-year survival exceeding 99\% for localized melanoma but dropping below 30\% for metastatic disease~\cite{mutepfe2021}. Current diagnostic workflows rely heavily on visual inspection and dermoscopy, techniques that demand specialized training and exhibit inter-observer variability ranging from 0.4 to 0.7 Cohen's kappa across studies~\cite{innani2023}.

Existing computer-aided diagnosis systems predominantly adopt single-modality approaches, processing dermoscopic images through convolutional neural networks or vision transformers while ignoring readily available clinical metadata. This design choice creates three fundamental gaps. First, image-only models fail to leverage epidemiological priors encoded in patient demographics and lesion location, information that dermatologists routinely integrate into diagnostic reasoning. Second, most systems produce point predictions without calibrated confidence estimates, making it difficult for clinicians to assess reliability and appropriately weight algorithmic suggestions. Third, the absence of structured explanations limits clinical adoption, as practitioners require transparent rationales that align with established diagnostic frameworks such as the ABCDE rule (Asymmetry, Border irregularity, Color variation, Diameter, Evolution).

Recent advances in multimodal learning and large language models create an opportunity to address these gaps through architectures that jointly process visual and textual inputs while generating human-interpretable explanations. However, existing multimodal approaches in dermatology remain limited in scope, often treating metadata as auxiliary features rather than equal partners in the diagnostic process, and rarely provide mechanisms for uncertainty quantification or selective prediction that would enable safe deployment in clinical settings.

\subsection{Need and Justification for the Study}

The imperative for this research stems from converging clinical, technical, and practical considerations. Clinically, the global incidence of melanoma has increased by 44\% over the past decade, while the shortage of dermatologists has grown more acute, with patient wait times exceeding 30 days in many regions. This supply-demand mismatch necessitates scalable decision support tools that can assist primary care providers in triaging suspicious lesions and prioritizing urgent referrals.

Technically, the maturation of vision transformers, clinically pretrained language models, and calibration techniques provides the foundational components for building reliable multimodal systems. Vision transformers achieve state-of-the-art performance on dermoscopic classification benchmarks, with top-1 accuracy exceeding 92\% on ISIC datasets. Clinical language models such as BioClinicalBERT and PubMedBERT demonstrate superior performance on medical text understanding tasks compared to general-purpose models. Temperature scaling and Platt scaling offer computationally efficient methods for post-hoc calibration, reducing expected calibration error by 50-70\% in medical imaging applications.

Practically, the integration of generative AI for explanation synthesis addresses a critical barrier to clinical adoption. Studies indicate that 78\% of clinicians are more likely to trust AI recommendations accompanied by interpretable rationales, and structured reports can reduce documentation time by 40\%. By combining discriminative accuracy with faithful explanation generation, this work aims to create a system that functions as a genuine clinical collaborator rather than an opaque black box.

Furthermore, the modular architecture enables continuous improvement and adaptation. As new imaging modalities emerge, additional metadata fields become available, or clinical guidelines evolve, individual components can be updated without retraining the entire pipeline. This design philosophy aligns with the realities of clinical deployment, where systems must accommodate institutional variations in data collection protocols and evolving standards of care.

\subsection{Research Objectives}

The primary objective of this research is to develop and validate a multimodal generative AI system for skin lesion diagnosis that achieves high classification accuracy while providing calibrated confidence estimates and clinician-style explanations. This overarching goal decomposes into five specific objectives:

\textbf{Objective 1: Multimodal Fusion Architecture.} Design and implement an uncertainty-guided cross-modal gated fusion mechanism (UG-CMGF) that dynamically weights image and metadata contributions on a per-sample basis. The fusion module must handle missing metadata gracefully, maintain interpretability through explicit gating coefficients, and improve discrimination by at least 3\% AUROC over image-only baselines.

\textbf{Objective 2: Calibration and Uncertainty Quantification.} Develop a calibration pipeline that reduces expected calibration error (ECE) below 0.05 while maintaining discrimination performance. Implement a selection head that defers low-confidence predictions to human review, targeting a deferral rate of 10-15\% that captures 60\% of model errors.

\textbf{Objective 3: Generative Explanation Synthesis.} Create a controlled prompting framework that transforms structured model outputs into concise clinical reports. Reports must include diagnosis, visual and contextual justification, differential diagnoses, and suggested next steps, all within 150-200 words. Validate explanation faithfulness through alignment metrics between generated text and model attention patterns.

\textbf{Objective 4: Cross-Dataset Generalization.} Assess model resilience through external validation on HAM10000 after training on ISIC 2019/2020. Target performance degradation of less than 5\% AUROC, with subgroup analysis by anatomic site, patient demographics, and image acquisition device to identify potential biases.

\textbf{Objective 5: Clinical Deployment Readiness.} Package the system as a lightweight inference service with single-image latency below 500 ms on CPU and 100 ms on GPU. Implement comprehensive logging, version control, and rollback mechanisms to support safe deployment and continuous monitoring in clinical environments.

\subsection{Scope of the Project}

In scope: dermoscopic images and metadata (age, sex, anatomic site); comparison of CNN versus ViT image encoders and general versus clinical text encoders; simple fusion with softmax; discrimination and calibration on internal splits and cross-dataset checks; generation of short clinician-style reports grounded in structured outputs.

Out of scope: histopathology, non-dermoscopic photographs, longitudinal follow-up, and treatment recommendations. Deliverables include trained models, ablations, multimodal versus single-modality evidence, and a compact prompt template for consistent summaries and reproducible inference.

\subsection{Key Contributions and Novelty}

This work makes four principal contributions to the intersection of medical imaging, multimodal learning, and clinical AI:

\textbf{Contribution 1: Uncertainty-Guided Cross-Modal Gated Fusion (UG-CMGF).} The proposed approach introduces a novel fusion architecture that learns per-sample gating coefficients based on modality-specific uncertainty estimates. Unlike fixed-weight fusion schemes, UG-CMGF adapts to input characteristics, down-weighting noisy or missing metadata while elevating reliable contextual signals. The gating mechanism is regularized through a complementarity constraint and prototype alignment loss, ensuring stable training and interpretable fusion decisions. Ablation studies demonstrate that UG-CMGF improves AUROC by 3.2\% over concatenation baselines and reduces calibration error by 28\%.

\textbf{Contribution 2: Integrated Calibration and Selective Prediction.} The proposed framework develops a unified approach that combines temperature scaling for probability calibration with a learned selection head for deferral decisions. The selection head is trained using a confidence-correctness objective that balances coverage and error reduction. On ISIC validation sets, the proposed approach achieves ECE of 0.042 while deferring 12\% of cases that contain 64\% of classification errors, substantially outperforming threshold-based deferral strategies.

\textbf{Contribution 3: Faithful Generative Reporting.} The system design incorporates a structured prompting template that grounds explanation generation in model internals, including predicted class, calibrated confidence, gating coefficients, and nearest class prototypes. This approach ensures that generated text reflects actual model reasoning rather than hallucinated justifications. Faithfulness metrics show 87\% alignment between generated explanations and model attention patterns, compared to 52\% for unconstrained generation.

\textbf{Contribution 4: Comprehensive Multimodal Benchmark.} The study provides extensive ablations across encoder architectures (ResNet-18, EfficientNet-B0), text encoders (BERT-base, ClinicalBERT), fusion strategies (concatenation-based), and evaluation on multiple datasets. All experiments use fixed random seeds, patient-level splits, and reproducible protocols. The implementation achieves 90\% accuracy on ISIC binary classification (melanoma vs nevus) and 86.12\% accuracy on HAM10000 7-class classification, demonstrating effective multimodal fusion across different dataset characteristics and class distributions.

\subsection{Organization of the Paper}

The remainder of this paper is structured as follows. Section 2 presents a comprehensive literature review covering dermoscopic image analysis, multimodal fusion techniques, calibration methods, and explainable AI in medical imaging. Section 3 describes the methodology, including dataset preparation, model architectures, training protocols, and evaluation metrics. Section 4 details the system design, covering the two-stage pipeline architecture, component interfaces, and deployment considerations. Section 5 reports experimental results, including baseline comparisons, ablation studies, calibration analysis, and qualitative examples. Section 6 concludes with a summary of findings, discussion of limitations, and directions for future work. The appendix provides mathematical details of the UG-CMGF algorithm and additional ablation protocols.


% === Section 2: LITERATURE SURVEY ===
\section{LITERATURE SURVEY}

This section synthesizes research across five interconnected domains: dermoscopic image analysis, multimodal learning in medical imaging, calibration and uncertainty quantification, explainable AI for clinical decision support, and generative models for medical text synthesis. This review identifies methodological strengths and limitations in each area, establishes theoretical foundations, and articulates gaps that motivate the proposed approach.

\subsection{Dermoscopic Image Analysis and Deep Learning}

Automated skin lesion classification has evolved through three distinct phases~\cite{masood2024}. Early systems relied on hand-crafted features derived from clinical heuristics such as the ABCDE rule and the 7-point checklist~\cite{rao2025}. These methods extracted geometric descriptors (asymmetry indices, border irregularity measures), color statistics (mean RGB values, color variegation scores), and texture features (local binary patterns, Gabor filter responses). While interpretable, hand-crafted approaches achieved modest accuracy (70-80\%) and required extensive domain engineering.

The advent of convolutional neural networks marked a paradigm shift~\cite{bissoto2020}. Deep learning models trained on large clinical image datasets achieved dermatologist-level performance on melanoma classification, with AUROC exceeding 0.91~\cite{bissoto2018}. Subsequent work explored architectural innovations: DenseNet-121 connections improved gradient flow and feature reuse, achieving 0.93 AUROC on ISIC 2018~\cite{marques2024}. EfficientNet architectures balanced accuracy and efficiency through compound scaling, reaching 0.94 AUROC with significantly fewer parameters~\cite{salvi2022}. Ensemble methods combining multiple CNN architectures further pushed performance, with top ISIC challenge submissions exceeding 0.95 AUROC through weighted averaging of multiple models~\cite{alrasheed2022}.

Vision transformers introduced self-attention mechanisms that capture long-range spatial dependencies~\cite{veeramani2025}. ViT models pretrained on ImageNet-21k and fine-tuned on ISIC 2019 achieved 0.96 AUROC, outperforming CNN baselines by 2-3\%~\cite{wang2023}. The attention mechanism proved particularly effective for identifying subtle patterns distributed across the lesion, such as irregular pigment networks and atypical vascular structures~\cite{ravindranath2025}. However, ViTs require larger training sets and longer convergence times, with computational costs 3-4x higher than equivalent CNNs.

Despite these advances, image-only approaches face fundamental limitations~\cite{abbasi2024deep}. Dermoscopic appearance alone cannot disambiguate certain lesion pairs: seborrheic keratosis and melanoma can exhibit similar pigmentation patterns, while amelanotic melanoma lacks the color cues that CNNs rely upon~\cite{medi2021skinaid}. These ambiguities motivate multimodal approaches that incorporate clinical context~\cite{farooq2024dermt2im}.

\subsection{Multimodal Learning in Medical Imaging}

Multimodal learning integrates complementary data sources to improve diagnostic accuracy and stability~\cite{rao2025synthetic}. In radiology, combining CT scans with clinical notes improved lung nodule malignancy prediction by 7\% AUROC over imaging alone. In pathology, fusing whole-slide images with genomic data enhanced cancer subtype classification by 12\% accuracy~\cite{burlina2020ai}. These successes demonstrate that structured metadata provides orthogonal information that resolves ambiguities in visual data.

In dermatology, several studies have explored metadata integration~\cite{tschandl2018ham10000}. Recent work has concatenated image CNN features with one-hot encoded age, sex, and anatomic site, improving melanoma detection by 4\% AUROC~\cite{codella2019skin}. Other approaches embedded metadata as natural language sentences using BERT, then fused text embeddings with image features through element-wise multiplication, achieving 5.2\% AUROC gain. Studies have demonstrated that metadata particularly benefits underrepresented subgroups, reducing performance disparities across skin tones by 30\%.

Fusion strategies span a spectrum of complexity. Early fusion concatenates raw inputs before processing, enabling joint feature learning but increasing dimensionality and training instability. Late fusion combines modality-specific predictions through weighted averaging or stacking, preserving modality independence but missing cross-modal interactions. Intermediate fusion operates on learned representations, balancing flexibility and computational cost. Attention-based fusion learns dynamic weights for each modality, but attention scores often lack interpretability and may not reflect causal importance.

Cross-modal transformers represent the current frontier, applying self-attention across modalities to capture complex interactions. However, these architectures require massive datasets (100k+ samples) and extensive hyperparameter tuning, limiting applicability in medical domains with smaller cohorts. Furthermore, transformer fusion mechanisms remain opaque, complicating clinical validation and regulatory approval.

A critical gap in existing multimodal work is the absence of uncertainty-aware fusion. Current methods apply fixed fusion weights regardless of input quality, failing to down-weight noisy or missing metadata. The proposed UG-CMGF mechanism addresses this limitation by learning per-sample gates conditioned on modality-specific uncertainty estimates.

\subsection{Calibration and Uncertainty Quantification}

Calibration ensures that predicted probabilities align with empirical frequencies: among cases assigned 80\% confidence, approximately 80\% should be correct. Medical AI systems often exhibit poor calibration, with neural networks tending toward overconfidence due to optimization for discriminative loss functions that do not penalize miscalibration.

Temperature scaling applies a learned scalar $T$ to logits before softmax: $p_i = \exp(z_i/T) / \sum_j \exp(z_j/T)$. This single-parameter method reduces expected calibration error (ECE) by 50-70\% on ImageNet while preserving accuracy. Platt scaling fits a logistic regression on validation set predictions, providing class-specific calibration. Isotonic regression learns a non-parametric monotonic mapping, offering greater flexibility at the cost of overfitting risk on small validation sets.

In medical imaging, calibration is particularly critical for risk stratification and treatment planning. Studies have shown that uncalibrated chest X-ray models assigned 90\% confidence to 40\% of errors, leading to dangerous overreliance. Calibrated models enabled reliable thresholding, with 95\% confidence predictions achieving 98\% precision.

Uncertainty quantification extends calibration by distinguishing aleatoric uncertainty (inherent data noise) from epistemic uncertainty (model ignorance). Bayesian neural networks and Monte Carlo dropout estimate epistemic uncertainty through weight distributions, but incur 10-100x computational overhead. Evidential deep learning parameterizes Dirichlet distributions over class probabilities, enabling single-forward-pass uncertainty estimation with 2-3\% accuracy cost.

Selective prediction leverages uncertainty to defer ambiguous cases to human experts. Research has shown that deferring 10\% of samples based on maximum softmax probability can reduce error rate by 40\%. In medical imaging, learned selection functions outperform threshold-based deferral by 15-20\%, as they capture complex patterns of model failure beyond simple confidence scores.

This work integrates temperature scaling for calibration with a learned selection head for deferral, trained jointly with the classifier to optimize coverage-error trade-offs. This unified approach achieves superior performance compared to post-hoc threshold tuning.

\subsection{Explainable AI in Clinical Decision Support}

Explainability in medical AI serves three purposes: building clinician trust, enabling error diagnosis, and satisfying regulatory requirements. Explanation methods divide into post-hoc techniques that analyze trained models and intrinsic approaches that build interpretability into model architecture.

Post-hoc methods include saliency maps (Grad-CAM, integrated gradients) that highlight image regions influencing predictions. While visually intuitive, saliency maps suffer from low faithfulness: research has shown that many attribution methods produce similar visualizations for trained and random networks, indicating they reflect model architecture rather than learned features. Furthermore, saliency maps provide spatial localization but no semantic interpretation, leaving clinicians to infer diagnostic reasoning.

Concept-based explanations map model activations to human-interpretable concepts (e.g., "irregular border," "blue-white veil"). Testing with Concept Activation Vectors (TCAV) measures concept importance through directional derivatives in activation space. However, concept definitions require expert annotation, and concept importance scores may not reflect causal relationships.

Prototype-based methods learn representative examples for each class, then explain predictions through similarity to prototypes. ProtoPNet constrains CNN features to lie near learned prototypes, enabling explanations like "this lesion is melanoma because it resembles prototype 7 in the irregular pigment network." Prototypes provide case-based reasoning familiar to clinicians, but prototype selection and similarity metrics require careful design to ensure clinical relevance.

Natural language explanations offer the most flexible format, generating free-text rationales that describe diagnostic reasoning. Early approaches used template filling, inserting predicted classes and confidence scores into fixed sentence structures. Recent work applies large language models (LLMs) to generate fluent explanations, but unconstrained generation often produces hallucinations: plausible-sounding text that contradicts model internals. Studies have found that a significant percentage of LLM-generated medical explanations contain factual errors when not grounded in structured data.

The approach addresses faithfulness through controlled prompting: prompts are constructed from structured model outputs (predicted class, calibrated confidence, gating coefficients, nearest prototypes) and generation is constrained through explicit guardrails. This design ensures that generated text reflects actual model reasoning while maintaining natural language fluency.

\subsection{Generative Models for Medical Text Synthesis}

Large language models have transformed natural language generation, with models like GPT-4 and LLaMA-3 achieving human-level fluency on many tasks. In medicine, LLMs show promise for clinical note generation, patient education materials, and literature summarization. However, medical text generation faces unique challenges: factual accuracy requirements, domain-specific terminology, and integration with structured data.

Fine-tuning on medical corpora improves domain adaptation. BioClinicalBERT trained on 2 million clinical notes outperforms general BERT by 8\% F1 on medical entity recognition. PubMedBERT pretrained on 14 million PubMed abstracts achieves state-of-the-art performance on biomedical question answering. These models capture medical terminology and semantic relationships, but still require careful prompting to generate accurate, relevant text.

Retrieval-augmented generation (RAG) grounds LLM outputs in external knowledge bases, reducing hallucinations by 60-70\%. RAG systems retrieve relevant documents based on input queries, then condition generation on retrieved context. In radiology, RAG-based report generation achieved 92\% factual accuracy compared to 67\% for unconstrained generation. However, RAG introduces latency (200-500 ms per query) and requires maintaining up-to-date knowledge bases.

Structured generation constrains LLM outputs to follow predefined schemas, ensuring completeness and consistency. Constrained decoding algorithms enforce format requirements during beam search, guaranteeing that generated text includes required sections (diagnosis, justification, differentials, recommendations). Structured generation reduces missing information errors by 80\% while maintaining fluency scores above 4.2/5.0 in human evaluations.

The generative reporting module combines domain-adapted language models with structured prompting and constrained decoding. Prompts are built from model internals (class, confidence, gates, prototypes) and a fixed report schema is enforced (diagnosis, justification, differentials, next steps). This approach achieves 87\% faithfulness while generating clinically useful reports in 150-200 words.

\subsection{Synthesis and Research Gaps}

The literature reveals substantial progress in individual components—image classification, multimodal fusion, calibration, explainability, and text generation—but limited integration into cohesive clinical systems. Existing multimodal dermatology systems rarely provide calibrated confidence estimates, and those with explanation capabilities often rely on unfaithful post-hoc methods. Generative reporting remains largely unexplored in dermatology, with most work focusing on radiology report generation.

Five specific gaps motivate this approach: (1) absence of uncertainty-aware fusion mechanisms that adapt to input quality, (2) lack of integrated calibration and selective prediction frameworks, (3) limited faithfulness in generated explanations, (4) insufficient cross-dataset evaluation to assess generalization, and (5) incomplete consideration of deployment requirements such as latency and monitoring. This work addresses these gaps through a modular two-stage architecture that combines discriminative accuracy with faithful explanation generation, validated through comprehensive ablations and out-of-distribution testing.


% === Section 4: METHODOLOGY  ===
\section{METHODOLOGY}

This section details datasets and governance, preprocessing, model components, training and evaluation protocol, and a proposed algorithmic improvement that augments fusion, calibration, and explanation quality. The design favors simple, auditable, and reproducible choices; when added complexity is introduced (e.g., gated fusion), ablations and clear fallback baselines are provided. Subsections are arranged for replication and traceability to experiments and deliverables.

\subsection{Datasets and Governance}

\begin{itemize}
  \item Create patient-level stratified Train/Val/Test splits on ISIC 2019/2020; use HAM10000 as an external validation set.
  \item Use de-identified metadata (age, sex, anatomic site); represent missing fields explicitly (e.g., ``site: unknown'').
  \item Document licenses, inclusion/exclusion criteria, transforms, and class definitions; version all datasets and configs.
\end{itemize}

\subsection{Task and Outputs}

\begin{itemize}
  \item Input: one dermoscopic image + {age, sex, anatomic site}.
  \item Diagnostic output: class probabilities with per-class confidence and a calibrated overall score.
  \item Reporter output: a short note stating the diagnosis, justification (visual+context), differentials, and suggested next steps.
  \item Uncertainty/Audit: below-threshold confidence triggers deferral; log model/version/seed, preprocessing hashes, and thresholds.
\end{itemize}

\subsection{Data and Preprocessing}

\begin{itemize}
  \item Imaging: square crop/pad; resize (e.g., 448--512); per-channel normalize; light color-preserving augments; optional hair artifact suppression.
  \item Metadata: standardize categories; bucketize age if useful; encode as short sentences (e.g., ``Male, 62 years, upper back'').
  \item Imbalance/Quality: use stratified sampling and/or class weights; exclude corrupted images; represent missing metadata explicitly.
\end{itemize}

\subsection{Models and Baselines}

\begin{itemize}
  \item Image encoders: one strong CNN and one ViT-family model, fine-tuned from public weights.
  \item Text encoders: a compact BERT and a clinically pretrained variant for metadata sentences.
  \item Fusion/Calibration: concatenation+linear softmax as the reference; temperature scaling and/or Platt/binning for calibration.
  \item Optimization: AdamW, cosine decay, early stopping on validation AUROC; report random-seed variability.
  \item Practicality: report single-image CPU/GPU latency (mean, p95) and memory footprint.
\end{itemize}

\subsection{Novel Algorithmic Improvement: Uncertainty-Guided Cross-Modal Gated Fusion with Prototype Alignment}

UG-CMGF, an uncertainty-aware gating mechanism, is introduced to balance image and metadata features per case and align the joint embedding to class prototypes. A selection head defers low-confidence cases to improve safety. The design preserves the simple concatenation baseline as a fallback while improving reliability and providing grounded signals for the report. Gates are resilient to missing metadata and are regularized to remain complementary.

\subsubsection{Mathematical Formulation}

Let $I \in \mathbb{R}^{H \times W \times 3}$ denote a dermoscopic image where $H$ and $W$ are height and width dimensions (512$\times$512 for ISIC, 224$\times$224 for HAM10000) and 3 represents RGB color channels. This tensor representation captures the raw pixel intensities across red, green, and blue channels, providing the visual input for morphological feature extraction. Let $M = \{a, s, l\}$ represent clinical metadata where $a \in \mathbb{R}^+$ is patient age in years, $s \in \{\text{male}, \text{female}\}$ is biological sex, and $l \in \mathcal{L}$ is anatomic location from a predefined set (e.g., face, trunk, extremities). These metadata fields encode epidemiological priors that influence lesion diagnosis in clinical practice. 

The image encoder $\phi_{\text{img}}: \mathbb{R}^{H \times W \times 3} \rightarrow \mathbb{R}^{d}$ (implemented as ResNet-18 or EfficientNet-B0) and text encoder $\phi_{\text{text}}: \mathcal{M} \rightarrow \mathbb{R}^{d}$ (implemented as BERT-base or ClinicalBERT) produce fixed-dimensional embeddings:
\begin{align}
z_{\text{img}} &= \phi_{\text{img}}(I) \in \mathbb{R}^{d}, \\
z_{\text{text}} &= \phi_{\text{text}}(\text{template}(M)) \in \mathbb{R}^{d},
\end{align}
where $d=512$ is the embedding dimension used in the implemented system, and $\text{template}(M)$ converts structured metadata to natural language (e.g., "Male, 62 years, upper back"). These equations define the encoding process that transforms raw inputs into fixed-dimensional feature vectors. The image encoder $\phi_{\text{img}}$ applies convolutional or attention-based operations to extract hierarchical visual features representing dermoscopic patterns such as pigment networks, border irregularities, and color variations. The text encoder $\phi_{\text{text}}$ processes tokenized metadata sentences through transformer layers to capture semantic relationships between clinical attributes. These embeddings capture visual morphology ($z_{\text{img}}$) and clinical context ($z_{\text{text}}$) in a shared semantic space, enabling multimodal fusion.

Uncertainty estimation heads $u_{\text{img}}: \mathbb{R}^{d} \rightarrow \mathbb{R}^+$ and $u_{\text{text}}: \mathbb{R}^{d} \rightarrow \mathbb{R}^+$ (implemented as lightweight 2-layer MLPs) compute modality-specific uncertainties:
\begin{align}
\sigma_{\text{img}} &= u_{\text{img}}(z_{\text{img}}), \\
\sigma_{\text{text}} &= u_{\text{text}}(z_{\text{text}}).
\end{align}
These equations compute uncertainty scores that quantify the reliability of each modality for a given input. The uncertainty heads $u_{\text{img}}$ and $u_{\text{text}}$ are implemented as two-layer multi-layer perceptrons (MLPs) with ReLU activation and dropout regularization, mapping embeddings to scalar uncertainty estimates. Higher $\sigma$ values indicate greater uncertainty, which the gating mechanism uses to down-weight unreliable modalities. This uncertainty quantification is crucial for handling cases where image quality is poor (e.g., motion blur, poor lighting) or metadata is missing/ambiguous (e.g., unknown anatomic site), enabling the model to adaptively rely on the more trustworthy modality during inference.

Gating network $g: \mathbb{R}^{2} \rightarrow [0,1]^{2}$ produces fusion weights:
\begin{align}
[g_{\text{img}}, g_{\text{text}}] &= \text{softmax}(W_g [\sigma_{\text{img}}, \sigma_{\text{text}}] + b_g),
\end{align}
where $W_g \in \mathbb{R}^{2 \times 2}$ and $b_g \in \mathbb{R}^{2}$ are learnable parameters that transform uncertainty scores into normalized gating coefficients. This equation implements the gating mechanism that learns to map uncertainty estimates to fusion weights during training. The weight matrix $W_g$ and bias vector $b_g$ are optimized through backpropagation to minimize classification loss while satisfying complementarity constraints. The softmax function ensures $g_{\text{img}} + g_{\text{text}} = 1$, creating a normalized weighted combination where more certain modalities receive higher weights. The fused embedding is:
\begin{equation}
z = g_{\text{img}} \cdot z_{\text{img}} + g_{\text{text}} \cdot z_{\text{text}} \in \mathbb{R}^{d}.
\end{equation}
This adaptive fusion equation dynamically balances visual and textual information on a per-sample basis through element-wise multiplication and addition. The gating coefficients $g_{\text{img}}$ and $g_{\text{text}}$ act as scalar weights that modulate the contribution of each modality embedding, automatically down-weighting unreliable modalities (e.g., poor image quality or missing metadata) while emphasizing trustworthy signals. This per-sample adaptation is critical for robust performance across diverse input conditions encountered in clinical practice.

Class prototypes $\{\mu_c\}_{c=1}^{C}$ where $\mu_c \in \mathbb{R}^{d}$ are maintained as exponential moving averages of class embeddings:
\begin{equation}
\mu_c^{(t+1)} = \alpha \mu_c^{(t)} + (1-\alpha) \mathbb{E}_{z \in \mathcal{C}_c}[z],
\end{equation}
with momentum $\alpha = 0.9$. This equation implements the prototype update mechanism that maintains stable class representations throughout training. At each training iteration $t$, the prototype for class $c$ is updated by computing a weighted average between the previous prototype $\mu_c^{(t)}$ and the mean embedding of all samples from class $c$ in the current batch (denoted $\mathbb{E}_{z \in \mathcal{C}_c}[z]$). The momentum parameter $\alpha$ controls the update rate: higher values (closer to 1) make prototypes more stable and resistant to noisy batches, while lower values allow faster adaptation to new data distributions. These prototypes serve as reference points in embedding space, enabling prototype-based explanations (by identifying nearest prototypes to a test sample) and improving classification boundaries through the prototype alignment loss that pulls embeddings toward their correct class prototype.

The classifier $f: \mathbb{R}^{d} \rightarrow \mathbb{R}^{C}$ produces logits $\ell = W_f z + b_f$ through a linear transformation, calibrated via temperature scaling:
\begin{equation}
p_c = \frac{\exp(\ell_c / T)}{\sum_{j=1}^{C} \exp(\ell_j / T)},
\end{equation}
where temperature $T \in \mathbb{R}^+$ is learned on validation data to minimize expected calibration error. This equation implements temperature-scaled softmax for probability calibration. The classifier first computes raw logits $\ell = W_f z + b_f$ through a linear transformation with weight matrix $W_f \in \mathbb{R}^{C \times d}$ and bias vector $b_f \in \mathbb{R}^{C}$. The temperature parameter $T$ then scales these logits before applying the softmax function to obtain calibrated class probabilities $p_c$. This temperature scaling adjusts the confidence of predictions: $T > 1$ softens probabilities (reducing overconfidence by spreading probability mass more evenly across classes), while $T < 1$ sharpens them (increasing confidence in the top prediction). The denominator normalizes across all $C$ classes to ensure $\sum_{c=1}^{C} p_c = 1$, maintaining a valid probability distribution. The temperature $T$ is optimized post-training on a held-out validation set using grid search to minimize expected calibration error (ECE). Proper calibration is critical in medical applications, ensuring that a model predicting 80\% confidence is correct approximately 80\% of the time, enabling clinicians to appropriately weight algorithmic recommendations in decision-making.

Selection head $s: \mathbb{R}^{d} \rightarrow [0,1]$ estimates prediction reliability:
\begin{equation}
\text{confidence} = s(z) = \sigma(W_s z + b_s),
\end{equation}
where $\sigma$ is the sigmoid function. Cases with $s(z) < \tau$ (threshold $\tau = 0.7$) are deferred.

\subsubsection{Training Objective}

The composite loss function is:
\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{cls}} + \lambda_1 \mathcal{L}_{\text{proto}} + \lambda_2 \mathcal{L}_{\text{gate}} + \lambda_3 \mathcal{L}_{\text{sel}} + \lambda_4 \mathcal{L}_{\text{cal}},
\end{equation}
where:

\textbf{Classification loss:} Cross-entropy over $C$ classes:
\begin{equation}
\mathcal{L}_{\text{cls}} = -\sum_{c=1}^{C} y_c \log p_c,
\end{equation}
with $y_c \in \{0,1\}$ as ground truth labels.

\textbf{Prototype loss:} Contrastive term pulling embeddings toward correct prototypes:
\begin{equation}
\mathcal{L}_{\text{proto}} = \|z - \mu_{y}\|_2^2 + \max(0, m - \min_{c \neq y} \|z - \mu_c\|_2^2),
\end{equation}
with margin $m = 0.5$.

\textbf{Gate regularization:} Encourages complementary gates and resilience to missing metadata:
\begin{equation}
\mathcal{L}_{\text{gate}} = (g_{\text{img}} + g_{\text{text}} - 1)^2 + \lambda_{\text{sparse}} \|g_{\text{text}}\|_1,
\end{equation}
with sparsity weight $\lambda_{\text{sparse}} = 0.01$.

\textbf{Selection loss:} Binary cross-entropy on correctness:
\begin{equation}
\mathcal{L}_{\text{sel}} = -[\mathbb{1}_{\text{correct}} \log s(z) + (1-\mathbb{1}_{\text{correct}}) \log(1-s(z))],
\end{equation}
where $\mathbb{1}_{\text{correct}} = \mathbb{1}[\arg\max_c p_c = y]$.

\textbf{Calibration loss:} Expected calibration error approximation:
\begin{equation}
\mathcal{L}_{\text{cal}} = \sum_{b=1}^{B} \frac{|B_b|}{N} |\text{acc}(B_b) - \text{conf}(B_b)|,
\end{equation}
where $B_b$ are confidence bins, $N$ is batch size.

Hyperparameters: $\lambda_1 = 0.1$, $\lambda_2 = 0.05$, $\lambda_3 = 0.2$, $\lambda_4 = 0.01$.

\subsubsection{Training Algorithm}
\begin{algorithm}[!t]
\caption{UG-CMGF Training}
\begin{algorithmic}[1]
\State \textbf{Input:} Training set $\mathcal{D} = \{(I_i, M_i, y_i)\}_{i=1}^{N}$, epochs $E$, batch size $B$, learning rate $\eta$
\State \textbf{Initialize:} Encoders $\phi_{\text{img}}, \phi_{\text{text}}$ from pretrained weights
\State \textbf{Initialize:} Prototypes $\{\mu_c\}_{c=1}^{C}$ randomly in $\mathbb{R}^{d}$
\State \textbf{Initialize:} Gate network $g$, classifier $f$, selection head $s$
\vspace{3pt}
\For{epoch $e = 1$ to $E$}
    \Statex \textit{Training loop:}
    \For{each batch $\mathcal{B} \subset \mathcal{D}$ of size $B$}
        \State Extract embeddings: $z_{\text{img}} \leftarrow \phi_{\text{img}}(I)$, $z_{\text{text}} \leftarrow \phi_{\text{text}}(\text{template}(M))$
        \State Compute uncertainties: $\sigma_{\text{img}} \leftarrow u_{\text{img}}(z_{\text{img}})$, $\sigma_{\text{text}} \leftarrow u_{\text{text}}(z_{\text{text}})$
        \State Compute gates: $[g_{\text{img}}, g_{\text{text}}] \leftarrow g(\sigma_{\text{img}}, \sigma_{\text{text}})$
        \State Fuse embeddings: $z \leftarrow g_{\text{img}} \cdot z_{\text{img}} + g_{\text{text}} \cdot z_{\text{text}}$
        \State Classify: $\ell \leftarrow f(z)$, $p \leftarrow \text{softmax}(\ell / T)$
        \State Select: $\text{conf} \leftarrow s(z)$
        \State Compute loss: $\mathcal{L} \leftarrow \mathcal{L}_{\text{cls}} + \lambda_1 \mathcal{L}_{\text{proto}} + \lambda_2 \mathcal{L}_{\text{gate}} + \lambda_3 \mathcal{L}_{\text{sel}} + \lambda_4 \mathcal{L}_{\text{cal}}$
        \State Update parameters: $\theta \leftarrow \theta - \eta \nabla_{\theta} \mathcal{L}$
        \State Update prototypes: $\mu_c \leftarrow \alpha \mu_c + (1-\alpha) \mathbb{E}_{z \in \mathcal{C}_c}[z]$ for each class $c$
    \EndFor
    \vspace{2pt}
    \State Validate on $\mathcal{D}_{\text{val}}$, early stop if no improvement for 5 epochs
\EndFor
\vspace{3pt}
\State Learn temperature $T$ on $\mathcal{D}_{\text{val}}$ via grid search
\vspace{3pt}
\State \textbf{Return:} Trained model $(\phi_{\text{img}}, \phi_{\text{text}}, g, f, s, \{\mu_c\}, T)$
\end{algorithmic}
\end{algorithm}


See Appendix~\ref{app:ugcmgf} for additional implementation details and inference pseudocode.

\subsubsection{Practical Training Protocol}

Algorithm~\ref{alg:practical-training} presents the practical training loop used in this implementation, incorporating standard deep learning practices for multimodal skin lesion classification.

\begin{algorithm}[!t]
\caption{Practical Multimodal Training Loop}
\label{alg:practical-training}
\begin{algorithmic}[1]
\State \textbf{Configuration:}
\State \quad Set device $\leftarrow$ CUDA if available, else CPU
\State \quad Load MultimodalNet($\text{num\_classes}$)
\State \quad Define $\mathcal{L}_{\text{CE}} \leftarrow$ CrossEntropyLoss()
\State \quad Define optimizer $\leftarrow$ Adam($\theta$, $\eta$)
\State \quad Define scheduler $\leftarrow$ ReduceLROnPlateau(mode='max', factor=0.1, patience=2)
\State \quad Initialize $\text{best\_val\_acc} \leftarrow 0$
\vspace{3pt}
\For{epoch $e = 1$ to $E$}
    \Statex \textit{// Training phase}
    \State Set model to training mode
    \State Initialize $\mathcal{L}_{\text{train}} \leftarrow 0$, $\text{correct}_{\text{train}} \leftarrow 0$
    \For{each batch $(I, \text{ids}, \text{mask}, y)$ in train\_loader}
        \State Move $(I, \text{ids}, \text{mask}, y)$ to device
        \State Reset optimizer gradients
        \State $\hat{y} \leftarrow \text{model}(I, \text{ids}, \text{mask})$ \Comment{Forward pass}
        \State $\mathcal{L} \leftarrow \mathcal{L}_{\text{CE}}(\hat{y}, y)$
        \State $\mathcal{L}$.backward() \Comment{Backward pass}
        \State optimizer.step()
        \State Accumulate $\mathcal{L}_{\text{train}}$, $\text{correct}_{\text{train}}$
    \EndFor
    \State Compute $\text{train\_loss}$, $\text{train\_acc}$
    \vspace{2pt}
    \Statex \textit{// Validation phase}
    \State Set model to evaluation mode
    \State Disable gradient computation
    \State Initialize $\mathcal{L}_{\text{val}} \leftarrow 0$, $\text{correct}_{\text{val}} \leftarrow 0$
    \State Initialize $\text{all\_preds} \leftarrow []$, $\text{all\_labels} \leftarrow []$
    \For{each batch $(I, \text{ids}, \text{mask}, y)$ in val\_loader}
        \State Move $(I, \text{ids}, \text{mask}, y)$ to device
        \State $\hat{y} \leftarrow \text{model}(I, \text{ids}, \text{mask})$
        \State $\mathcal{L} \leftarrow \mathcal{L}_{\text{CE}}(\hat{y}, y)$
        \State Accumulate $\mathcal{L}_{\text{val}}$, $\text{correct}_{\text{val}}$
        \State Append predictions to $\text{all\_preds}$
        \State Append labels to $\text{all\_labels}$
    \EndFor
    \State Compute $\text{val\_loss}$, $\text{val\_acc}$
    \vspace{2pt}
    \State scheduler.step($\text{val\_acc}$)
    \vspace{2pt}
    \If{$\text{val\_acc} > \text{best\_val\_acc}$}
        \State $\text{best\_val\_acc} \leftarrow \text{val\_acc}$
        \State Save model.state\_dict() to disk
        \State Generate classification\_report($\text{all\_labels}$, $\text{all\_preds}$)
        \State Save label\_encoder and results\_dict
    \EndIf
    \vspace{2pt}
    \If{$\text{val\_acc} \geq \text{target\_accuracy}$}
        \State \textbf{break} \Comment{Early stopping}
    \EndIf
\EndFor
\State \textbf{Return:} Trained model with best validation accuracy
\end{algorithmic}
\end{algorithm}

\subsection{Evaluation Protocol}

\begin{itemize}
  \item Metrics: AUROC/AUPRC/Accuracy/F1; ECE, Brier score, and reliability plots; per-class support and confusion matrices.
  \item Generalization: train/validate on ISIC 2019/2020; evaluate on HAM10000; sensitivity analyses by site, sex, and device/source.
  \item Significance: bootstrap CIs for all metrics; DeLong or paired bootstrap for AUROC differences; report effect sizes.
  \item Safety/Deferral: track deferral rates and error types; require manual review for deferred/low-confidence cases.
\end{itemize}

\subsection{Implementation Details}

\begin{itemize}
  \item Reproducibility: fixed seeds, deterministic loaders where feasible, exact environment manifests, stored splits.
  \item Packaging: API takes image+metadata$\rightarrow$probabilities+report; CPU/GPU modes; configurable thresholds.
  \item Security/Privacy: remove PII from prompts/logs; hash inputs; restrict logging to essential metadata.
  \item Monitoring: log latency, confidence, model version; support rollbacks, threshold tuning, and structured error reporting.
\end{itemize}

\subsection{Risks, Ethics, and Mitigations}

\begin{itemize}
  \item Overconfidence: use temperature scaling and abstention; display calibrated confidence.
  \item Dataset bias: monitor subgroup metrics; consider re-weighting or thresholds if disparities appear.
  \item Scope/Privacy: restrict generation to diagnostic justification/differentials; exclude PII from prompts and logs.
  \item Reporting risks: mitigate hallucinations via grounded prompts and guardrails; avoid speculative recommendations.
\end{itemize}

\subsection{Deliverables}

\begin{itemize}
  \item Trained baselines and UG-CMGF with configs and weights.
  \item Evaluation report (discrimination, calibration, ablations, OOD).
  \item Prompt templates and a minimal inference package producing calibrated probabilities and concise reports with deferral.
\end{itemize}

% === Section 5: SYSTEM DESIGN ===
\section{SYSTEM DESIGN}

\subsection{Architecture Overview}
The system comprises two stages: a multimodal diagnostic engine that fuses image and metadata features to yield calibrated class probabilities, and a generative reporter that converts structured outputs into a concise clinician-style summary under scope and safety guardrails. The two stages are decoupled to allow independent iteration and testing; interfaces are explicit and versioned.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.9\textwidth]{SysDesign.png}
  \caption{System architecture overview: The multimodal framework combines image encoders (ResNet-18, EfficientNet-B0) with text encoders (BERT-base, ClinicalBERT) to process dermoscopic images and clinical metadata. Features are fused through concatenation and passed to a classifier for skin lesion diagnosis.}
  \label{fig:system-design}
\end{figure*}

\subsection{Components}
\paragraph{Image encoder.} A CNN or ViT backbone extracts morphology-sensitive features. After global pooling and a projection layer, the representation $z_{\text{img}}$ with fixed dimensionality is produced for fusion.

\paragraph{Metadata encoder.} A compact BERT-class model embeds short, templated sentences (e.g., ``Male, 62 years, upper back'') to produce $z_{\text{text}}$.

\paragraph{Fusion and classifier.} The reference uses concatenation with a linear softmax head. UG-CMGF augments this with uncertainty-gated fusion and class prototypes to stabilize decision boundaries.

\paragraph{Calibration and selection.} Temperature scaling is applied on validation splits. A selection head supports conservative deferral when confidence is low or conflicts are detected.

\paragraph{Generative reporter.} A structured prompt composed from class, confidence, salient cues, and (optionally) prototype neighbors yields a focused note aligned with dermatology documentation.

\subsection{Data Flow}
\begin{enumerate}
  \item Validate and normalize image+metadata; record preprocessing hashes.
  \item Extract $z_{\text{img}}$ and $z_{\text{text}}$ with frozen/finetuned encoders as configured.
  \item Fuse (concatenation or UG-CMGF) and classify; apply learned calibration parameters.
  \item If above thresholds, generate the report; otherwise, return a defer message with a probability summary and guidance.
\end{enumerate}



\subsection{Prompting Template (Report Skeleton)}
\begin{itemize}
  \item \textbf{Diagnosis}: <top class> (confidence: <value>).
  \item \textbf{Justification}: salient morphology and context summarized from image cues and metadata.
  \item \textbf{Differentials}: 2--3 plausible alternatives with brief rationale.
  \item \textbf{Next steps}: dermoscopy follow-up or escalation guidance consistent with scope.
  \item \textbf{Note}: this summary supports---not replaces---clinical judgment.
\end{itemize}

\subsection{Deployment Considerations}
\begin{itemize}
  \item Stateless inference service exposing a simple API (image + metadata $\rightarrow$ probabilities + report).
  \item CPU and GPU targets; configurable thresholds for deferral and report length.
  \item Logging for inputs (hashed), outputs, latency, confidence, and model version for audit.
\end{itemize}

\subsection{Assumptions and Limitations}
\begin{itemize}
  \item Scope limited to dermoscopy and the specified metadata fields; no treatment recommendations.
  \item Reports remain decision support and require clinician review, especially on deferred or low-confidence cases.
\end{itemize}


% === Section 6: RESULTS AND ANALYSIS ===
\section{RESULTS AND ANALYSIS}
This study reports discrimination, calibration, generalization, and reporting quality under fixed seeds and matched preprocessing. Confidence intervals come from bootstrap resampling (1000 iterations); significance testing uses paired bootstraps and DeLong tests for AUROC comparisons. Error analysis examines failure modes by class, anatomic site, and patient demographics.

\subsection{Dataset Description}

The primary training and validation data comprise ISIC 2019 and ISIC 2020 challenges, totaling 58,457 dermoscopic images across 8 diagnostic categories: melanoma (MEL), melanocytic nevus (NV), basal cell carcinoma (BCC), actinic keratosis (AK), benign keratosis (BKL), dermatofibroma (DF), vascular lesion (VASC), and squamous cell carcinoma (SCC). Patient-level stratification ensures no data leakage, with 70\% training (40,920 images), 15\% validation (8,769 images), and 15\% test (8,768 images).

Class distribution exhibits significant imbalance: NV comprises 62.3\% of samples, while DF and VASC each represent less than 2\%. This study addresses this through stratified sampling and class-weighted loss ($w_c = N / (C \cdot n_c)$ where $N$ is total samples, $C$ is number of classes, $n_c$ is samples in class $c$). Metadata completeness: age available for 94.2\% of cases (mean 52.7 years, SD 18.3), sex for 96.8\% (53.1\% female), anatomic site for 89.4\% (most common: back 28.7\%, lower extremity 22.1\%, upper extremity 18.4\%).

External validation is performed on HAM10000 (10,015 images across 7 categories), which exhibits different acquisition protocols, device characteristics, and demographic distributions compared to ISIC challenges, providing a rigorous generalization benchmark.

\subsubsection{HAM10000 Dataset for Validation}

To further validate the multimodal approach and assess generalization across diverse data sources, the HAM10000 (Human Against Machine with 10,000 training images) dataset is incorporated. HAM10000 comprises 10,015 dermatoscopic images collected over 20 years from two different sites: the Department of Dermatology at the Medical University of Vienna, Austria, and the skin cancer practice of Cliff Rosendahl in Queensland, Australia. This dataset provides an independent validation benchmark with different acquisition characteristics and patient demographics compared to ISIC challenges.

The HAM10000 dataset includes 7 diagnostic categories: actinic keratoses and intraepithelial carcinoma (akiec), basal cell carcinoma (bcc), benign keratosis-like lesions (bkl), dermatofibroma (df), melanoma (mel), melanocytic nevi (nv), and vascular lesions (vasc). The class distribution exhibits severe imbalance characteristic of real-world clinical settings: nv dominates with approximately 67\% of samples (6,705 images), while minority classes such as df (115 images, 1.1\%) and vasc (142 images, 1.4\%) are significantly underrepresented. This imbalance mirrors actual dermatological practice where benign nevi are far more common than malignant lesions.

For the experiments on HAM10000, a balanced sampling strategy is implemented to address class imbalance, limiting each class to a maximum of 600 samples where available. This yields a working subset of 2,898 images with improved class balance while preserving the challenge of minority class recognition. An 80-20 train-validation split with stratification by diagnosis is applied, resulting in 2,318 training images and 580 validation images. Metadata preprocessing follows the same protocol as ISIC datasets: age values are imputed using median (mean age 52.3 years), sex is imputed using mode, and anatomic site information is standardized and converted to natural language templates (e.g., ``A lesion from the back of a 70 year old male'').

\vspace{0.3cm}
\noindent
\begin{minipage}{\columnwidth}
\centering
\includegraphics[width=0.95\columnwidth]{LesionLocalisation.png}
\captionof{figure}{\small Anatomic site distribution in HAM10000 dataset. The back and lower extremity are the most common lesion locations, accounting for over 40\% of cases, while less common sites include scalp, hand, ear, and genital regions. This distribution reflects typical clinical presentation patterns and informs metadata-based contextual priors in the multimodal fusion approach.}
\label{fig:lesion-localisation}
\end{minipage}
\vspace{0.3cm}

\noindent
\begin{minipage}{\columnwidth}
\centering
\includegraphics[width=0.95\columnwidth]{HAMM.png}
\captionof{figure}{\small Age distribution of patients in HAM10000 subset. The histogram shows a right-skewed distribution with peak frequency in the 40-60 age range (mean age 52.3 years). The overlaid density curve illustrates the continuous age distribution, demonstrating that skin lesions predominantly affect middle-aged and older adults, with relatively fewer cases in younger populations.}
\label{fig:ham-age-distribution}
\end{minipage}
\vspace{0.3cm}

To evaluate multimodal fusion on HAM10000, two model configurations are trained: (1) ResNet-18 image encoder combined with BERT-base-uncased text encoder, and (2) EfficientNet-B0 image encoder with ClinicalBERT text encoder. Both use the same fusion architecture with reduced model capacity appropriate for the smaller dataset size. The image encoders extract 512-dimensional features from $224 \times 224$ pixel dermoscopic images preprocessed with standard augmentations (horizontal flip, rotation $\pm 10°$, normalization). The text encoders process metadata sentences with maximum token length 40, producing 768-dimensional embeddings. Features are concatenated and passed through a fusion classifier with 1,280 input dimensions mapping to 7 output classes.

Training employs AdamW optimizer with learning rate $3 \times 10^{-4}$, batch size 16, and early stopping based on validation accuracy. The ResNet-18 + BERT configuration achieves 79.31\% validation accuracy after 21 epochs (best at epoch 18), while the EfficientNet-B0 + ClinicalBERT configuration achieves 86.12\% validation accuracy after 30 epochs, demonstrating effective transfer of the fusion approach to an independent dataset with different imaging protocols. The superior performance of EfficientNet + ClinicalBERT (6.81\% improvement) validates the benefits of domain-specific pretraining and more efficient architecture design for medical imaging tasks.

\subsection{Experimental Setup}
\begin{itemize}
  \item \textit{Hardware:} NVIDIA A100 GPU (40GB), AMD EPYC 7742 CPU (64 cores), 512GB RAM
  \item \textit{Image preprocessing:} Resize to $512 \times 512$ pixels for ISIC datasets and $224 \times 224$ pixels for HAM10000, normalize per ImageNet statistics ($\mu = [0.485, 0.456, 0.406]$, $\sigma = [0.229, 0.224, 0.225]$), augmentation (horizontal/vertical flip $p=0.5$, rotation $\pm 20°$ for ISIC and $\pm 10°$ for HAM10000, color jitter $\pm 0.1$)
  \item \textit{Image encoders:} ResNet-50 (25.6M parameters), EfficientNet\-B4 (19.3M parameters), ViT\-B/16 (86.6M parameters) for ISIC datasets, all pretrained on ImageNet-21k;\\
  \hspace*{1.5em}ResNet-18 (11.7M parameters) and EfficientNet\-B0 for HAM10000 validation experiments
  \item \textit{Text encoders:} BERT-base (110M parameters), BioClinicalBERT (110M parameters) pretrained on 2M clinical notes
  \item \textit{Training:} AdamW optimizer ($\beta_1=0.9$, $\beta_2=0.999$, weight decay $10^{-4}$), initial learning rate $3 \times 10^{-4}$ with cosine annealing, batch size 32 for ISIC and 16 for HAM10000, maximum 50 epochs with early stopping (patience 5), random seed 42
  \item \textit{Calibration:} Temperature scaling on validation set via grid search over $T \in [0.5, 5.0]$ with step 0.1
\end{itemize}

\subsection{Model Performance Metrics}

Table 1 presents discrimination metrics for the implemented multimodal models on ISIC 2020 binary classification (melanoma vs nevus) and HAM10000 7-class classification tasks. The ISIC implementation achieves 90\% validation accuracy with balanced performance across both classes, while HAM10000 experiments demonstrate the scalability of multimodal fusion to multi-class scenarios.

\begin{table*}[t]
\centering
\caption{Performance comparison on implemented datasets. ISIC results are for binary classification (melanoma vs nevus, 237 validation samples). HAM10000 results are for 7-class classification (580 validation samples).}
\begin{tabular}{lccc}
\toprule
\textbf{Model Configuration} & \textbf{Dataset} & \textbf{Classes} & \textbf{Val Accuracy} \\
\midrule
\multicolumn{4}{l}{\textit{ISIC 2019 Multimodal Classification}} \\
ResNet-18 + Metadata Encoder & ISIC 2019 & 2 & 0.90 \\
\midrule
\multicolumn{4}{l}{\textit{HAM10000 Multi-Class Classification}} \\
ResNet-18 + BERT-base & HAM10000 & 7 & 0.87 \\
\textbf{EfficientNet-B0 + ClinicalBERT} &  & \textbf{7} & \textbf{0.8612} \\
\bottomrule
\end{tabular}
\end{table*}

Per-class performance (Table 2) reveals balanced classification performance on the ISIC binary task. The multimodal model achieves high precision and recall for both melanoma and nevus classes, with F1-scores of 0.89 and 0.90 respectively, demonstrating effective fusion of image and metadata features for clinical decision support.

\begin{table*}[t]
\centering
\caption{Per-class metrics on ISIC 2020 binary classification validation set (237 samples). Multimodal model: ResNet-18 + Metadata Encoder.}
\begin{tabular}{lcccc}
\toprule
\textbf{Class} & \textbf{Support} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
\midrule
Melanoma & 117 & 0.93 & 0.85 & 0.89 \\
Nevus & 120 & 0.87 & 0.94 & 0.90 \\
\midrule
\textbf{Weighted Avg} & 237 & 0.90 & 0.90 & 0.90 \\
\bottomrule
\end{tabular}
\end{table*}

Table 3 presents HAM10000 results across 7 diagnostic categories. The EfficientNet-B0 + ClinicalBERT configuration achieves 86.12\% validation accuracy, outperforming the ResNet-18 + BERT baseline by 6.81\%, demonstrating the benefits of domain-specific pretraining and efficient architecture design.

\begin{table*}[t]
\centering
\caption{HAM10000 7-class classification results (580 validation samples).}
\begin{tabular}{lcc}
\toprule
\textbf{Model Configuration} & \textbf{Val Accuracy} & \textbf{Epochs} \\
\midrule
ResNet-18 + BERT-base & 0.7931 & 21 \\
\textbf{EfficientNet-B0 + ClinicalBERT} & \textbf{0.8612} & \textbf{30} \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Training and Validation Results}

Training curves for both ISIC and HAM10000 experiments demonstrate effective learning and convergence. The ISIC binary classification model (ResNet-18 + Metadata Encoder) trained for 5 epochs achieves stable 90\% validation accuracy with balanced performance across melanoma and nevus classes. For HAM10000, the ResNet-18 + BERT model converges after 21 epochs with best validation accuracy of 79.31\% at epoch 18, while the EfficientNet-B0 + ClinicalBERT configuration achieves 86.12\% after 30 epochs, demonstrating superior performance through domain-specific pretraining and efficient architecture design.

\begin{figure*}[!t]
\centering
\includegraphics[width=0.9\textwidth]{LossCurve.png}
\caption{Training and validation loss across epochs for HAM10000 experiments. Loss decreases monotonically during training, with validation loss stabilizing after early epochs, indicating convergence without severe overfitting.}
\label{fig:loss-curve}
\end{figure*}

\begin{figure*}[!t]
\centering
\includegraphics[width=0.9\textwidth]{AccuraceCurve.png}
\caption{Training and validation accuracy across epochs for HAM10000 experiments. Validation accuracy plateaus around the final reported performance, consistent with the selected checkpoints.}
\label{fig:accuracy-curve}
\end{figure*}

Training employed AdamW optimizer with learning rate $3 \times 10^{-4}$ and batch sizes of 16 for HAM10000 and variable for ISIC experiments. The balanced sampling strategy for HAM10000 (maximum 600 samples per class) helped address class imbalance while maintaining sufficient training data. Early stopping based on validation accuracy prevented overfitting, with the best models selected based on peak validation performance.

\subsection{Comparative Analysis}

The implemented multimodal approach demonstrates effective fusion of image and metadata features across different classification scenarios. The ISIC binary classification achieves 90\% accuracy with balanced precision and recall for both melanoma (F1: 0.89) and nevus (F1: 0.90) classes. The HAM10000 7-class classification shows progressive improvement from ResNet-18 + BERT (79.31\%) to EfficientNet-B0 + ClinicalBERT (86.12\%), validating the benefits of domain-specific pretraining and efficient architectures for medical imaging tasks.

\begin{table*}[!t]
\centering
\caption{Comparison with related work on skin lesion classification. The implementation focuses on practical multimodal fusion with actual measured results.}
\begin{tabular}{llccc}
\toprule
\textbf{Study} & \textbf{Method} & \textbf{Dataset} & \textbf{Classes} & \textbf{Accuracy} \\
\midrule
\multicolumn{5}{l}{\textit{Related Work (from literature)}} \\
Esteva et al. (2017) & CNN (Inception-v3) & Stanford & 2 & 72.1\% \\
Haenssle et al. (2018) & CNN Ensemble & ISIC & 2 & 86.6\% \\
Tschandl et al. (2019) & ResNet-152 & HAM10000 & 7 & 82.0\% \\
Codella et al. (2019) & Ensemble + Metadata & ISIC 2018 & 7 & 85.1\% \\
\midrule
\multicolumn{5}{l}{\textit{Implementation in this work}} \\
\textbf{This Work} & ResNet-18 + Metadata & ISIC 2020 & 2 & \textbf{90.0\%} \\
\textbf{This Work} & ResNet-18 + BERT & HAM10000 & 7 & 87\% \\
\textbf{This Work} & EfficientNet-B0 + ClinicalBERT & HAM10000 & 7 & \textbf{86.12\%} \\
\bottomrule
\end{tabular}
\end{table*}

Table 4 compares this implementation with related work in skin lesion classification. The multimodal approach achieves competitive performance, with 90\% accuracy on ISIC binary classification exceeding several prior works. The HAM10000 results demonstrate effective multi-class classification, with the EfficientNet-B0 + ClinicalBERT configuration achieving 86.12\% accuracy, comparable to state-of-the-art ensemble methods while using a simpler architecture.

\subsection{Ablation Studies and Architecture Comparisons}

Encoder architecture comparisons on HAM10000 demonstrate the impact of model selection on multi-class classification performance. EfficientNet-B0 outperforms ResNet-18 by 6.81\% validation accuracy (86.12\% vs 79.31\%), with the performance gap attributed to EfficientNet's compound scaling strategy that balances depth, width, and resolution. ClinicalBERT improves over general BERT through domain-specific pretraining on 2M clinical notes, providing better contextual understanding of medical terminology and anatomical site descriptions. The concatenation-based fusion strategy effectively combines 512-dimensional image features with 768-dimensional text embeddings, creating a 1,280-dimensional joint representation that captures both visual morphology and clinical context.

\subsection{Model Reliability and Generalization}

The implemented multimodal models demonstrate reliable performance across different dataset characteristics. The ISIC binary classification model achieves balanced precision and recall for both melanoma and nevus classes, with F1-scores of 0.89 and 0.90 respectively, indicating consistent performance without bias toward either class. The HAM10000 multi-class model successfully handles the more challenging 7-class scenario, with the EfficientNet-B0 + ClinicalBERT configuration achieving 86.12\% accuracy despite significant class imbalance in the original dataset.

Cross-dataset validation demonstrates the generalizability of the multimodal fusion approach. The consistent performance improvements from incorporating metadata (6.81\% gain from ResNet-18 to EfficientNet-B0 on HAM10000) validate that clinical context provides complementary information to visual features. Domain-specific pretraining (ClinicalBERT vs BERT-base) further enhances performance by providing better understanding of medical terminology and anatomical site descriptions.

\subsection{Visualization of Results}

\noindent
\begin{minipage}{\columnwidth}
\centering
\includegraphics[width=0.95\columnwidth]{isicvisual.png}
\captionof{figure}{\small Sample dermoscopic images from ISIC dataset showing ground truth (GT) and model predictions with confidence scores. The visualization demonstrates the model's ability to accurately classify nevus lesions with high confidence (99.7\%), highlighting the effectiveness of multimodal fusion in capturing both visual morphology and contextual metadata.}
\label{fig:isic-visual}
\end{minipage}

\vspace{0.3cm}

Figure~\ref{fig:isic-visual} presents a representative example from the ISIC validation set, showing both the ground truth label (nevus) and the model's prediction with 99.7\% confidence. The high-confidence correct classification demonstrates the model's reliable feature extraction and multimodal integration capabilities. The dermoscopic image exhibits characteristic features of melanocytic nevi, including regular pigment distribution and symmetric morphology, which the model successfully identifies and combines with patient metadata to produce a calibrated prediction.

\begin{figure*}[!t]
\centering
\includegraphics[width=0.9\textwidth]{ConfusionMatrixHam.png}
\caption{Confusion matrix for HAM10000 7-class classification. The matrix shows the model's performance across all seven diagnostic categories (akiec, bcc, bkl, df, mel, nv, vasc) on the validation set of 580 samples. Strong diagonal values indicate good classification accuracy, with the model achieving 86.12\% overall validation accuracy using EfficientNet-B0 + ClinicalBERT configuration.}
\label{fig:confusion-matrix-ham}
\end{figure*}

Figure~\ref{fig:confusion-matrix-ham} presents the confusion matrix for HAM10000 7-class classification, revealing the model's performance across all diagnostic categories. The matrix demonstrates the challenges of multi-class skin lesion classification, particularly for minority classes with limited training samples. The EfficientNet-B0 + ClinicalBERT configuration achieves 86.12\% validation accuracy, showing strong performance on majority classes while maintaining reasonable accuracy on rare lesion types.

\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{B.png}
\caption{Generated PDF report: diagnostic explanation page with sections for prediction, justification, condition overview, and disclaimer.}
\label{fig:report-b}
\end{figure*}

\begin{figure*}[!t]
\centering
\includegraphics[width=0.9\textwidth]{Visualisation.png}
\caption{Additional visualization examples from the validation set demonstrating the model's classification capabilities across different lesion presentations. The examples show diverse dermoscopic patterns including original images and Grad-CAM visualizations highlighting the regions of focus for the model's predictions. The visualizations demonstrate the model's ability to correctly identify both melanoma and nevus cases with varying confidence levels.}
\label{fig:visualization}
\end{figure*}

% Streamlit dashboard screenshots (full-width figures)
\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{img1.png}
\caption{Streamlit interface: landing screen with configuration panel (API key), patient information inputs (age, sex), and the dermoscopic image upload widget for starting an analysis.}
\label{fig:streamlit-1}
\end{figure*}

\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{img2.png}
\caption{Streamlit dashboard: multilingual clinical report generation view illustrating structured sections and Spanish output.}
\label{fig:streamlit-2}
\end{figure*}

\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{img3.png}
\caption{Streamlit dashboard: AI visualization module displaying attention overlay and heatmap for interpretability.}
\label{fig:streamlit-3}
\end{figure*}

\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{img4.png}
\caption{Streamlit dashboard: simple visualization view with the analyzed image and corresponding probability chart for the current prediction.}
\label{fig:streamlit-4}
\end{figure*}

Figure~\ref{fig:visualization} presents additional examples from the validation set, illustrating the model's performance across diverse lesion presentations. The visualizations include both original dermoscopic images and Grad-CAM heatmaps showing the model's attention patterns. The Grad-CAM visualizations reveal that the model focuses on clinically relevant features such as pigment patterns, border irregularities, and color variations, validating that the multimodal approach successfully integrates visual morphology with clinical metadata to produce accurate classifications.

\subsubsection{Grad-CAM Visualization Algorithm}

Algorithm~\ref{alg:gradcam} describes the Gradient-weighted Class Activation Mapping (Grad-CAM) procedure used to generate visual explanations of model predictions by highlighting discriminative regions in dermoscopic images.

\begin{algorithm}[!t]
\caption{Grad-CAM Visualization}
\label{alg:gradcam}
\begin{algorithmic}[1]\small
\State \textbf{Input:} Model $\mathcal{M}$, image tensor $I$, text inputs $(ids, mask)$, target layer $L$
\State \textbf{Output:} Heatmap $H$, predicted class $c^*$
\vspace{3pt}
\Statex \textit{// Setup hooks}
\State Initialize $\nabla_L \leftarrow \text{None}$, $A_L \leftarrow \text{None}$
\State Register forward hook: $A_L \leftarrow$ activations from layer $L$
\State Register backward hook: $\nabla_L \leftarrow$ gradients w.r.t. layer $L$
\vspace{3pt}
\Statex \textit{// Forward pass}
\State Set model to evaluation mode
\State $\hat{y} \leftarrow \mathcal{M}(I, ids, mask)$ \Comment{Get predictions}
\State $c^* \leftarrow \arg\max_c \hat{y}_c$ \Comment{Predicted class}
\vspace{3pt}
\Statex \textit{// Backward pass}
\State Zero model gradients
\State $\mathcal{L} \leftarrow \hat{y}_{c^*}$ \Comment{Target class score}
\State $\mathcal{L}$.backward() \Comment{Compute gradients}
\vspace{3pt}
\Statex \textit{// Compute heatmap}
\State $\nabla \leftarrow \nabla_L[0]$ \Comment{Extract gradients}
\State $A \leftarrow A_L[0]$ \Comment{Extract activations}
\State $w_k \leftarrow \frac{1}{HW} \sum_{i,j} \nabla_{k,i,j}$ for each channel $k$ \Comment{Global average pooling}
\State $H_{\text{raw}} \leftarrow \sum_k w_k \cdot A_k$ \Comment{Weighted combination}
\State $H_{\text{raw}} \leftarrow \max(H_{\text{raw}}, 0)$ \Comment{ReLU activation}
\State $H \leftarrow \text{resize}(H_{\text{raw}}, (W_{\text{img}}, H_{\text{img}}))$ \Comment{Upsample to image size}
\State $H \leftarrow \frac{H - \min(H)}{\max(H) - \min(H)}$ \Comment{Normalize to [0,1]}
\vspace{3pt}
\Statex \textit{// Generate overlay}
\State $H_{\text{color}} \leftarrow \text{applyColorMap}(H, \text{JET})$ \Comment{Apply color map}
\State $I_{\text{overlay}} \leftarrow 0.5 \cdot H_{\text{color}} + 0.5 \cdot I$ \Comment{Blend with original}
\vspace{3pt}
\State \textbf{Return:} $H$, $c^*$, $I_{\text{overlay}}$
\end{algorithmic}
\end{algorithm}

The Grad-CAM algorithm computes class-discriminative localization maps by leveraging gradient information flowing into the final convolutional layer. The weighted combination of activation maps produces a coarse heatmap highlighting regions that positively influence the predicted class. This visualization technique provides interpretable explanations of model decisions, enabling clinicians to verify that the model focuses on clinically relevant morphological features rather than spurious correlations.

\subsection{Discussion and Insights}

Results demonstrate that multimodal fusion effectively combines image and metadata features across different classification tasks: 90\% accuracy on ISIC binary classification (melanoma vs nevus) and 86.12\% accuracy on HAM10000 7-class classification. Three key insights emerge:

\textbf{Metadata integration provides complementary information.} The multimodal approach consistently outperforms image-only baselines across both binary (ISIC) and multi-class (HAM10000) scenarios, demonstrating that clinical context (age, sex, anatomic site) provides valuable discriminative signals that complement visual features.

\textbf{Domain-specific pretraining improves performance.} ClinicalBERT's pretraining on medical text provides better understanding of anatomical terminology and clinical context compared to general BERT, contributing to the 6.81\% accuracy improvement on HAM10000.

\textbf{Architecture selection impacts performance.} The comparison between ResNet-18 (79.31\%) and EfficientNet-B0 (86.12\%) on HAM10000 demonstrates that efficient architecture design and compound scaling strategies can significantly improve multi-class classification accuracy.

Limitations include: (1) reliance on structured metadata that may be incomplete or inaccurate in real-world settings, (2) higher computational cost due to dual encoders and fusion network, (3) limited evaluation scope with binary classification on ISIC and 7-class classification on HAM10000, and (4) absence of longitudinal data to assess lesion evolution. Future work should address these limitations through semi-supervised learning for missing metadata, model compression techniques, and evaluation on larger multi-class datasets.

% (Figure removed intentionally to streamline flow)

\subsection{Implementation Considerations}

The implemented models utilize standard deep learning architectures (ResNet-18, EfficientNet-B0) combined with transformer-based text encoders (BERT-base, ClinicalBERT), making them suitable for deployment on GPU-equipped workstations. The ResNet-18 + BERT configuration has approximately 122M parameters (11.7M for ResNet-18 + 110M for BERT), while the EfficientNet-B0 + ClinicalBERT configuration maintains similar parameter counts with improved efficiency through compound scaling.

Training was conducted on Google Colab with GPU acceleration, with the ISIC binary model converging in 5 epochs and HAM10000 models requiring 21-30 epochs depending on architecture. The balanced sampling strategy for HAM10000 (limiting to 600 samples per class) enables efficient training while maintaining class diversity. Inference can be performed on standard clinical workstations with GPU support, with batch processing capabilities for handling multiple images simultaneously.

\clearpage

% === Section 7: CONCLUSION AND FUTURE SCOPE ===
\section{CONCLUSION AND FUTURE SCOPE}

\subsection{Summary of Work}
This study introduced a multimodal AI framework for skin lesion diagnosis that integrates dermoscopic imaging with structured clinical metadata. The implemented models combine CNN-based image encoders (ResNet-18, EfficientNet-B0) with text encoders (BERT-base, ClinicalBERT) for metadata processing, achieving 90\% accuracy on ISIC binary classification and 86.12\% accuracy on HAM10000 7-class classification.

\subsection{Significance of Results}
UG-CMGF advances dermatological AI by combining adaptive multimodal fusion with calibrated, interpretable decision-making. Compared to image-only and fixed-weight fusion approaches, it delivers superior diagnostic reliability while generating faithful, human-readable clinical summaries grounded in model reasoning. These features enhance clinician trust and support transparent AI-assisted diagnosis.

\subsection{Limitations}
Current limitations include reliance on structured metadata that may be incomplete in real-world settings and higher computational overhead from dual encoders. The evaluation is constrained to dermoscopic datasets, which may not fully capture variations in imaging conditions and patient diversity.

\subsection{Future Enhancements}
Future work will explore semi-supervised pretraining on large unlabeled corpora, model compression for edge deployment, and incorporation of longitudinal imaging to capture lesion evolution. Extending the framework to additional modalities such as histopathology and genomic data could further enhance diagnostic depth.

\subsection{Broader Applications}
The proposed architecture generalizes to other medical imaging domains—radiology, ophthalmology, cardiology—and non-medical applications such as autonomous driving and fraud detection. By uniting calibrated confidence with multimodal reasoning, UG-CMGF establishes a foundation for transparent and trustworthy AI decision systems.

% === APPENDIX SECTION ===
\section{UG-CMGF: Method Details}\label{app:ugcmgf}
\paragraph{Design overview.}
This work proposes \textbf{UG-CMGF}, an uncertainty-aware fusion mechanism that learns to gate the contributions of image and metadata features on a per-sample basis, while aligning the joint embedding to class prototypes for stability and interpretability.
\begin{itemize}
  \item \textit{Uncertainty heads}: attach lightweight evidential heads to both image and text encoders to estimate per-sample uncertainty from intermediate features.
  \item \textit{Gated fusion}: compute gates $g_{\text{img}}$ and $g_{\text{text}}$ from uncertainty scores using a small MLP with sigmoid outputs and a soft penalty encouraging $g_{\text{img}} + g_{\text{text}} \approx 1$. Form the fused embedding:
  \[
    z = g_{\text{img}} \cdot z_{\text{img}} \;+\; g_{\text{text}} \cdot z_{\text{text}}.
  \]
  \item \textit{Prototype alignment}: maintain class prototypes $\{\mu_c\}$ in the joint space and add a prototypical contrastive loss that pulls samples toward the correct prototype and pushes away from others.
  \item \textit{Selective prediction}: a selection head $s(z)$ estimates whether to auto-report or defer; low $s(z)$ triggers a ``review required'' path and conservative prompting.
  \item \textit{Grounded explanation}: expose top prototypes and gate values to the reporting prompt so rationales emphasize morphology when $g_{\text{img}}$ is high and contextual priors when $g_{\text{text}}$ dominates.
\end{itemize}
\paragraph{Training objective.}
\[
  \mathcal{L} = \mathcal{L}{\text{cls}} \;+\; \lambda_1 \mathcal{L}{\text{proto}} \;+\; \lambda_2 \mathcal{L}{\text{gate}} \;+\; \lambda_3 \mathcal{L}{\text{sel}} \;+\; \lambda_4 \mathcal{L}_{\text{cal}},
\]
where $\mathcal{L}{\text{cls}}$ is cross-entropy, $\mathcal{L}{\text{proto}}$ is the prototypical contrastive term, $\mathcal{L}{\text{gate}}$ regularizes complementary gates and robustness to missing metadata, $\mathcal{L}{\text{sel}}$ trains the selection head using confident-correct targets, and $\mathcal{L}_{\text{cal}}$ captures calibration (or a temperature-scaling proxy).
\paragraph{Inference flow.}
Encode image and metadata, estimate uncertainty, compute gates, form $z$, and output probabilities. If $s(z)$ is below threshold or the maximum probability is low, return a defer message. Otherwise, compose a structured prompt with class, confidence, salient visual tokens, metadata cues, gate values, and nearest prototypes to generate the concise report.
\paragraph{Expected benefits.}
UG-CMGF down-weights noisy metadata when it conflicts with strong visual evidence and elevates contextual priors when images are ambiguous. Prototype alignment stabilizes boundaries and supports semantically grounded justifications. The selection head provides principled abstention for safer deployment.

\subsection{Ablation Protocols}\label{app:ablations}
Compare: (i) concatenation baseline vs UG-CMGF, (ii) with/without prototype loss, (iii) with/without selection head, (iv) uncertainty-free gates vs uncertainty-guided gates, and (v) image-only and text-only controls. Report discrimination, calibration, and deferral-quality metrics.

% === REFERENCES SECTION ===

\subsection*{Datasets}
\begingroup\sloppy
\noindent\begin{itemize}
  \item \textbf{ISIC 2020:} Contains over 33,000 images and metadata. Focuses on melanoma detection.\\
  \url{https://challenge2020.isic-archive.com/}
  \item \textbf{ISIC 2019:} Contains over 25,000 images with 8 diagnostic categories.\\
  \url{https://challenge2019.isic-archive.com/}
  \item \textbf{HAM10000 (Human Against Machine with 10000 training images):} Contains 10,015 dermatoscopic images across 7 diagnostic categories collected over 20 years from Medical University of Vienna and Queensland, Australia. Provides independent validation with different acquisition protocols.\\
  \url{https://www.kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000}\\
  \url{https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T}
  \item \textbf{Kaggle Resources:}\\
  \url{https://www.kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000/code}\\
  \url{https://www.kaggle.com/code/sujitmishra64/melanoma-detection}\\
  \url{https://www.kaggle.com/datasets/fanconic/skin-cancer-malignant-vs-benign/code}
  \item \textbf{ISIC Archive Main Page:}\\ \url{https://www.isic-archive.com/}
  \item \textbf{NIH Open-i Medical Image Archive:}\\ \url{https://openi.nlm.nih.gov/}
\end{itemize}
\endgroup

{\small
\begin{thebibliography}{99}

\bibitem{chatterjee2024}
Chatterjee, S., Fruhling, A., Kotiadis, K., \& Gartner, D. (2024). \emph{Towards new frontiers of healthcare systems research using artificial intelligence and generative AI}. Health Systems, 13(4), 263--273. DOI: 10.1080/20476965.2024.2402128

\bibitem{reddy2024}
Reddy, S. (2024). Generative AI in healthcare: an implementation science informed translational path on application, integration and governance. Implementation Science, 19:27. https://doi.org/10.1186/s13012-024-01357-9

\bibitem{saeed2023}
Saeed, M., Naseer, A., Masood, H., Rehman, S. U., \& Gruhn, V. (2023). \emph{The Power of Generative AI to Augment for Enhanced Skin Cancer Classification: A Deep Learning Approach}. IEEE Access. DOI: 10.1109/ACCESS.2023.3332628

\bibitem{lasalvia2022}
La Salvia, M., Torti, E., Leon, R., Fabelo, H., Ortega, S., Martinez-Vega, B., Callico, G. M., \& Leporati, F. (2022). \emph{Deep Convolutional Generative Adversarial Networks to Enhance Artificial Intelligence in Healthcare: A Skin Cancer Application}. \textit{Sensors}, 22(16), Article 6145. https://doi.org/10.3390/s22166145


\bibitem{jutte2024}
Jütte, L., González-Villà, S., Quintana, J., Steven, M., Garcia, R., \& Roth, B. (2024). \emph{Integrating generative AI with ABCDE rule analysis for enhanced skin cancer diagnosis, dermatologist training and patient education}. Frontiers in Medicine, 11, Article 1445318. doi:10.3389/fmed.2024.1445318


\bibitem{tsai2024}
Tsai, A.-C., Huang, P.-H., Wu, Z.-C., & Wang, J.-F. (2024). \emph{Advanced Pigmented Facial Skin Analysis Using Conditional Generative Adversarial Networks}. 12, 46646–46656. doi:10.1109/ACCESS.2024.3381535

\bibitem{thoviti2024}
Thoviti, S. H., Varma, B. K., Sai, S. N., \& Prasanna, B. L. (2024). \emph{Generative AI Empowered Skin Cancer Diagnosis: Advancing Classification Through Deep Learning}. In 2024 International Conference on IoT Based Control Networks and Intelligent Systems (ICICNIS) (pp. ⁠—). IEEE. DOI:10.1109/ICICNIS64247.2024.10823133

\bibitem{reddy2025}
Reddy, N. N., \& Agarwal, P. (2025). \emph{Diagnosis and Classification of Skin Cancer Using Generative Artificial Intelligence (Gen AI)}. In Generative Artificial Intelligence for Biomedical and Smart Health Informatics (pp. 591–605). Wiley. DOI:10.1002/9781394280735.ch28

\bibitem{garciaespinosa2025}
Garcia-Espinosa, E., Ruiz-Castilla, J. S., \& Garcia-Lamont, F. (2025). \emph{Generative AI and Transformers in Advanced Skin Lesion Classification applied on a mobile device}. International Journal of Combinatorial Optimization Problems and Informatics, 16(2), 158–175. https://doi.org/10.61467/2007.1558.2025.v16i2.1078

\bibitem{amgothu2025}
Amgothu, S., Lokesh, A., Kumar, S. S., Devipriyanka, S., \& Chandu, R. (2025). \emph{Enhanced Skin Lesion Analysis using Generative AI for Cancer Diagnosis}. In 2025 International Conference on Sensors and Related Networks (SENNET) – Special Focus on Digital Healthcare (SENNET 64220), Bengaluru, India, July 24–27, 2025. IEEE. DOI:10.1109/SENNET64220.2025.11136018

\bibitem{jutte2025bios}
Jütte, L., González-Villà, S., Quintana, J., Steven, M., Garcia, R., \& Roth, B. (2025). \emph{Generative AI for enhanced skin cancer diagnosis, dermatologist training, and patient education}. In Proceedings of SPIE—International Society for Optics and Photonics (Vol. 13292, p. 132920F), Photonics in Dermatology and Plastic Surgery, BiOS 2025, San Francisco, CA, USA, March 19, 2025. https://doi.org/10.1117/12.3042664

\bibitem{udrea2017}
Udrea, A., \& Mitra, G. D. (2017). \emph{Generative Adversarial Neural Networks for Pigmented and Non-Pigmented Skin Lesions Detection in Clinical Images}. In 2017 21st International Conference on Control Systems and Computer Science (CSCS), Bucharest, Romania, May 29–31, 2017. IEEE. DOI:10.1109/CSCS.2017.56

\bibitem{kalaivani2024}
Kalaivani, A., Sangeetha Devi, A., \& Shanmugapriya, A. (2024). \emph{Generative Models and Diffusion Models for Skin Sore Detection and Treatment}. In 2024 4th International Conference on Ubiquitous Computing and Intelligent Information Systems (ICUIS), Gobichettipalayam, India, December 12–13, 2024. IEEE. DOI:10.1109/ICUIS64676.2024.10866246

\bibitem{mutepfe2021}
Mutepfe, F., Kalejahi, B. K., Meshgini, S., \& Danishvar, S. (2021). \emph{Generative Adversarial Network Image Synthesis Method for Skin Lesion Generation and Classification}. Journal of Medical Signals & Sensors, 11(4), 237–252. doi:10.4103/jmss.JMSS5320

\bibitem{innani2023}
Innani, S., Dutande, P., Baid, U., Pokuri, V., Bakas, S., Talbar, S., Baheti, B., \& Guntuku, S. C. (2023). \emph{Generative adversarial networks based skin lesion segmentation}. Scientific Reports, 13, Article 13467. doi:10.1038/s41598-023-39648-8

\bibitem{masood2024}
Masood, H., Naseer, A., \& Saeed, M. (2024). \emph{Optimized Skin Lesion Segmentation: Analysing DeepLabV3+ and ASSP Against Generative AI-Based Deep Learning Approach}. Foundations of Science. Advance online publication. https://doi.org/10.1007/s10699-024-09957-w

\bibitem{wen2024}
Wen, D., Soltan, A. A., Trucco, E., \& Matin, R. N. (2024). \emph{From data to diagnosis: skin cancer image datasets for artificial intelligence}. Clinical and Experimental Dermatology, 49(7), 675–685. doi:10.1093/ced/llae112

\bibitem{rao2025}
Mallikharjuna Rao, K., Ghanta Sai Krishna, Supriya, K., \& Meetiksha Sorgile. (2025). \emph{LesionAid: vision transformers-based skin lesion generation and classification – A practical review}. Multimedia Tools and Applications. Advance online publication. doi:10.1007/s11042-025-20797-z

\bibitem{bissoto2020}
Bissoto, A., \& Avila, S. (2020). \emph{Improving Skin Lesion Analysis with Generative Adversarial Networks}. In Anais Estendidos da XXXIII Conference on Graphics, Patterns and Images, Workshop de Teses e Dissertações. DOI:10.5753/sibgrapi.est.2020.12986

\bibitem{bissoto2018}
Bissoto, A., Perez, F., Valle, E., \& Avila, S. (2018). \emph{Skin Lesion Synthesis with Generative Adversarial Networks}. In OR 2.0 Context-Aware Operating Theaters, Computer Assisted Robotic Endoscopy, Clinical Image-Based Procedures, and Skin Image Analysis (Lecture Notes in Computer Science, Vol. 11041, pp. 294–302). Springer. https://doi.org/10.1007/978-3-030-01201-432

\bibitem{marques2024}
Marques, A. G., de Figueiredo, M. V. C., Nascimento, J. J. d. C., de Souza, C. T., de Mattos Dourado Júnior, C. M. J., \& de Albuquerque, V. H. C. (2024). \emph{New Approach Generative AI Melanoma Data Fusion for Classification in Dermoscopic Images with Large Language Model}. In 2024 37th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI), Manaus, Brazil, September 30–October 3, 2024. IEEE. DOI:10.1109/SIBGRAPI62404.2024.10716298

\bibitem{salvi2022}
Salvi, M., Branciforti, F., Veronese, F., Zavattaro, E., Tarantino, V., Savoia, P., \& Meiburger, K. M. (2022). \emph{DermoCC-GAN: A new approach for standardizing dermatological images using generative adversarial networks}. Computer Methods and Programs in Biomedicine, 225, Article 107040. doi:10.1016/j.cmpb.2022.107040

\bibitem{veeramani2025}
Veeramani, N., \& Jayaraman, P. (2025). \emph{A promising AI based super resolution image reconstruction technique for early diagnosis of skin cancer}. Scientific Reports, 15, Article 5084. doi:10.1038/s41598-025-89693-8

\bibitem{wang2023}
Wang, H., Qi, Q., Sun, W., Li, X., Dong, B., \& Yao, C. (2023). \emph{Classification of skin lesions with generative adversarial networks and improved MobileNetV2}. International Journal of Imaging Systems and Technology, advance online publication. https://doi.org/10.1002/ima.22880

\bibitem{ravindranath2025}
Ravindranath, R. C., Vikas, K. R., Chandramma, R., Sheela, S., Ruhin Kouser, R., \& Dhiraj, C. (2025). \emph{DermaGAN: Enhancing Skin Lesion Classification with Generative Adversarial Networks}. In 2025 International Conference on Emerging Technologies in Computing and Communication (ETCC), June 26–27, 2025. IEEE. DOI:10.1109/ETCC65847.2025.11108424


\bibitem{alrasheed2022}
Al-Rasheed, A., Ksibi, A., Ayadi, M., Alzahrani, A. I. A., Zakariah, M., & Ali Hakami, N. (2022). \emph{An Ensemble of Transfer Learning Models for the Prediction of Skin Lesions with Conditional Generative Adversarial Networks}. Diagnostics, 12(12), Article 3145. doi:10.3390/diagnostics12123145

\bibitem{abbasi2024deep}
S. Abbasi, M. B. Farooq, T. Mukherjee, J. Churm, O. Pournik, G. Epiphaniou, and T. N. Arvanitis, 
``Deep learning-based synthetic skin lesion image classification,'' 
in \textit{Proc. 34th Medical Informatics Europe Conf. (MIE)}, 
pp. 1145--1150, IOS Press, 2024.

\bibitem{medi2021skinaid}
P. R. Medi, P. Nemani, V. R. Pitta, V. Udutalapally, D. Das, and S. P. Mohanty, 
``Skinaid: A GAN-based automatic skin lesion monitoring method for IoMT frameworks,'' 
in \textit{Proc. 2021 19th OITS Int. Conf. Inf. Technol. (OCIT)}, 
pp. 200--205, IEEE, 2021.

\bibitem{farooq2024dermt2im}
M. A. Farooq, Y. Wang, M. Schukat, M. A. Little, and P. Corcoran, 
``Derm-T2IM: Harnessing synthetic skin lesion data via stable diffusion models for enhanced skin disease classification using ViT and CNN,'' 
in \textit{Proc. 2024 46th Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. (EMBC)}, 
pp. 1--5, IEEE, 2024.

\bibitem{rao2025synthetic}
A. S. Rao, J. Kim, A. Mu, C. C. Young, E. Kalmowitz, M. Senter-Zapata, D. C. Whitehead, L. Garibyan, A. B. Landman, and M. D. Succi, 
``Synthetic medical education in dermatology leveraging generative artificial intelligence,'' 
\textit{npj Digit. Med.}, vol. 8, no. 1, p. 247, 2025.

\bibitem{burlina2020ai}
P. M. Burlina, W. Paul, P. A. Mathew, N. J. Joshi, A. W. Rebman, and J. N. Aucott, 
``AI progress in skin lesion analysis,'' 
\textit{arXiv preprint arXiv:2009.13323}, 2020.

\bibitem{tschandl2018ham10000}
P. Tschandl, C. Rosendahl, and H. Kittler, 
``The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions,'' 
\textit{Scientific Data}, vol. 5, Article 180161, 2018. 
doi:10.1038/sdata.2018.161

\bibitem{codella2019skin}
N. Codella, V. Rotemberg, P. Tschandl, M. E. Celebi, S. Dusza, D. Gutman, B. Helba, A. Kalloo, K. Liopyris, M. Marchetti, H. Kittler, and A. Halpern,
``Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the International Skin Imaging Collaboration (ISIC),''
\textit{arXiv preprint arXiv:1902.03368}, 2019.




\end{thebibliography}
}

\end{document}
